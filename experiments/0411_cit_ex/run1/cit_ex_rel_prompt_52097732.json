{
    "messages": {
        "system": "You are a paper reviewer trying to identify connections between the paper you're reviewing with previous ideas and approaches in the field.",
        "user": "Below is the CURRENT PAPER that you're reviewing and a paper that it CITES. \nBelow is also the CITANCE, which is the context containing a citation to the CITED PAPER.\n\nPlease respond with a list of CONNECTIONS between the two papers. A connection is a similarity between the two papers. Some example connections include related purpose, method, finding, or insight. \n\nPlease respond in the following json format:\n{\n  \"thought_process\": str # first discuss how the two papers are related at a high level, this should help you come up with more detailed connections\n  \"connections\": [\n    {\n      \"connection\": # detailed description of the connection between a paper\n      \"score\": str # a score of quality of the question\n      \"rationale\": str # a justification for why this question yields the same answer on both papers, and the score it receives\n    },\n  ]\n}\n\n\nList all important connections, and give a score between 0 to 2, where 0 means it's erroneous, 1 means it's superficial and many other papers would be connected in the same way, and 2 means it touches the core of the relationship between the two papers, and it's unlikely that many other papers would have the same connection.\n\n<current_paper>\nA Distributional Approach to Controlled Text Generation\n\nWe propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both\"pointwise\"and\"distributional\"constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https://github.com/naver/gdc)\n\n\nINTRODUCTION\nNeural language models, such as GPT-2/3 Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality. In this paper, we are concerned with the problem of controlling a generic pretrained LM in order to satisfy certain desiderata. For instance, we may want to avoid toxic content; prevent certain demographic biases; or steer generations towards a certain topic or style. Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\nHowever, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \"degeneration\", producing poor examples that improve the average reward but forgo coherence and fluency. This degeneration is often diagnosed as an effect of deviating too much from the original pretrained LM during optimization. Consequently, prior work has regarded proximity to the pretrained model as a prescription for sample quality. This view is most prominent in open-domain generation where no gold references are available for fine-tuning, making the pretrained LM itself the yardstick for fluency. Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations. A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context. However, the authors show that balancing policy deviations from the original LM while also satisfying the control conditions is delicate. To combat degeneration they had to combine the KL penalty with post-norm fusion, reranking, and early-stopping procedures.\n1. We introduce a Distributional View for controlled text generation formalized as a constraint satisfaction problem combined with a divergence minimization objective, providing a single framework both for \"distributional\" constraints (collective statistical requirements) and for \"pointwise\" constraints (hard requirements on each individual) ( \u00a72.1). To our knowledge, this is the first framework with such generality for controlled text generation.\n2. We show how these constraints lead to an optimal EBM for the target model ( \u00a72.2), propose the KL-Adaptive DPG algorithm for approximating the optimal EBM distribution by Figure 1: From MaxEnt to EBM through Information Geometry. The Generalized MaxEnt specification (left panel) is looking for a distribution p that lies on the moment constraints manifold C and that minimizes the forward KL DKL(p, a). The solution is provided by Information Geometry: (1) build the exponential family E determined by a and \u03c6, (2) p lies at the intersection between C and E, (3) for any distribution c satisfying the constraints, the \"Pythagorean identity\" holds: DKL(c||a) = DKL(c||p) + DKL(p||a); in particular p is unique.\nan autoregressive policy ( \u00a72.3), and show the effectiveness of this adaptive technique for obtaining faster convergence ( \u00a7B.2).\n3. We conduct experiments in a number of pointwise and distributional conditions, assessing results in terms of divergence from GPT-2, fluency and diversity, with better performance than strong baselines. The distributional experiments show the potential of our approach as a remedy to the current and important problem of bias in pretrained language models, providing a novel direction for addressing it ( \u00a73).\n\nFORMALIZATION\nWe denote by X the set of all sequences x of bounded length L max , by a the initial pretrained model and by p the desired target model. The probabilities of x according to each model are a(x) and p(x). Our approach consists in expressing our desiderata through constraints on the desired values\u03bc i of the expectations (aka moments) \u00b5 i . = E x\u223cp \u03c6 i (x) of certain predefined real-valued feature functions \u03c6 i (x), for i \u2208 {1, . . . , k}.\nTo illustrate, the previous example can be expressed by using two binary features, \u03c6 1 (x) = 1 iff x is classified as speaking about sports, \u03c6 2 (x) = 1 iff x mentions a female character. Then our \"moment constraints\" take the following form: \u00b5 1 = E x\u223cp \u03c6 1 (x) = 1.0, \u00b5 2 = E x\u223cp \u03c6 2 (x) = 0.5. The first (pointwise) constraint implies that each individual x has to speak about sports (otherwise \u00b5 1 could not reach its maximum value 1.0), the second (distributional) constraint that 50% of the x's have to mention a female character. 4 Let C be the set of all distributions c over X that satisfy the moment constraints. We then propose to specify p as a distribution respecting the constraints, but also minimizing KL divergence from a: Equation (1) is a generalization of the Maximum Entropy Principle of Jaynes (1957), which corresponds to the limit case where a is the uniform u distribution over X, noting that minimizing D KL (c, u) is equivalent to maximizing the entropy of c under the constraints -in other words, trying to find the least \"specific\" distribution satisfying the constraints.\n\nCONSTRAINTS, INFORMATION GEOMETRY, EXPONENTIAL FAMILIES\nTo recap our formal approach, we have a finite set X, a distribution a over X s.t. a(x) > 0, \u2200x \u2208 X, and real functions \u03c6 1 , ..., \u03c6 k over X. We specify moment constraints \u00b5 i =\u03bc i on distributions c over X, where \u00b5 i . = E x\u223cc \u03c6 i (x) and the\u03bc i 's are given targets; the set of distributions satisfying these constraints is denoted by C. Our Problem is to find a p such that p = arg min c\u2208C D KL (c, a).\nWe follow Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000). Under the assumption that C = \u2205, they prove the following result (also see \u00a7A.1): Published as a conference paper at ICLR 2021 Theorem 1 (A) There exists a unique solution p to the problem above, obtained as p(x) \u221d P (x) where P is in exponential family form: (x) . (2) In other words p(x) = 1/Z P (x), with Z = x\u2208X P (x); P is an unnormalized distribution, i.e. an EBM. Here X C = {x \u2208 X| \u2203c \u2208 C s.t. c(x) > 0} is the \"support set\" associated with C. The \u03bb i 's are real numbers called the natural parameters associated with the moments \u00b5 i .\n(B) p can be approximated to arbitrary precision by distributions p of the form: for appropriate real values of the \u03bb ,i .\nThe advantage of this version of the connection between Generalized Maximum Entropy and Exponential Families is its generality, which distinguishes it from other presentations, and which makes it ideal for unified application to pointwise, distributional or hybrid constraints.\nIn the special case of only pointwise constraints, of the form E x\u223cc \u03c6 i (x) = 1.0, i \u2208 [1, k], with \u03c6 i (x) \u2208 {0, 1}, let's define the predicate b(x) to be 1 iff x satisfies all the constraints. Then, using the (A) form of the result, it is an easy exercise (see \u00a7A.2) to prove that X C = {x \u2208 X| b(x) = 1} and that one has p(x) \u221d a(x)b(x). In this case P (x) = a(x)b(x) is a very simple EBM that does not involve an exponential part; this is the EBM form that we use for experiments involving only pointwise constraints.\nIn the general case where some constraints are distributional, the determination of X C is not as direct, and we prefer to use the approximation provided by (B), which permits a generic implementation. With only distributional constraints, an exact solution is typically obtained with finite \u03bb's. With hybrid constraints, some of the \u03bb's may tend to infinite (positive or negative) values but thresholding them suffices to get a good approximation.\nNote that the estimate\u03bc(\u03bb) is obtained not as a single number, but as a parametric function of the variable \u03bb. We want to find \u03bb such that\u03bc(\u03bb) =\u03bc, a question that we handle on line 4 by performing an SGD optimization over the objective min ||\u03bc \u2212\u03bc(\u03bb)|| 2 2 . 6 At the end of this process, we obtain an estimated value for the parameter vector \u03bb, and a representation P (x) = a(x) exp \u03bb, \u03c6(x) . While a(x) is a normalized distribution by construction, the introduction of the second factor loses this normalization property, making P (x) an EBM. 7 8\n\nFROM EBM TO AUTOREGRESSIVE POLICY\nAlgorithm 2 KL-Adaptive DPG Input: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: for each iteration do 3: for each episode do 4: sample x from q(\u00b7) 5: \u03b8 \u2190 \u03b8+\u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 6: if DKL(p||\u03c0 \u03b8 ) < DKL(p||q) then 7: q \u2190 \u03c0 \u03b8 Output: \u03c0 \u03b8 The EBM representation just obtained for P defines the optimal p = Z \u22121 P unambiguously, a crucial intermediate step in the solution of our problem. From it we can immediately compute ratios of the form p(x)/p(x ) for two sequences x, x , but without knowing Z, we cannot compute p(x) and, even with such a knowledge, we cannot produce samples from p.\nThis problem is typical of EBMs at large: they provide a rich and flexible mechanism for specifying models, but they leave a gap between representation and exploitation. A range of techniques, from sophisticated MCMC approaches (especially for continuous models in vision) to contrastive learning techniques, have been developed for bridging this gap.\nOne technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (\"Distributional Policy Gradient\") algorithm (Parshakova et al., 2019b).\nThe objective of DPG is to obtain an autoregressive policy \u03c0 \u03b8 that approximates p, where approximation is formalized in terms of making the cross-entropy CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) log \u03c0 \u03b8 (x) as small as possible. 9 DPG exploits the fact that, for any \"proposal\" distribution q whose support contains the support of p, we have \u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212\u2207 \u03b8 E x\u223cp log \u03c0 \u03b8 (x) = \u2212E x\u223cp \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2212E x\u223cq p(x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) where the last equality is an instance of importance sampling. Our \"KL-adaptive\" version of DPG is shown in (Algorithm 2). We start from an input EBM P , along with an initial policy q which is a proxy to p; in our case we take q = a. During an iteration (think minibatch or set of minibatches), we sample a number of sequences from q, do an SGD update of \u03b8 (line 5), where P is used instead of p (noting that they only differ by a multiplicative constant), and where \u03b1 (\u03b8) is a learning rate. The efficiency of the algorithm is related to how close the proposal q is to the target p, 10 The algorithm is adaptive in the sense that it modifies q periodically to take advantage of the evolving approximations \u03c0 \u03b8 . On line 6, we we test whether the current \u03c0 \u03b8 is closer than q to p in terms of KL-divergence, and if so we update q to \u03c0 \u03b8 on line 7. 11 \u00a7B.2 provides an ablation study showing the effectiveness of this adaptive step for obtaining faster convergence.\n\nEXPERIMENTS, RESULTS, AND EVALUATION\nIn this section we describe our evaluation methodology and perform experiments on pointwise constraints ( \u00a73.2) and on distributional and hybrid constraints ( \u00a73.3). The Appendix contains a detailed view of evaluation ( \u00a7H), comparison with extra baselines ( \u00a7D.2), and an ablation study ( \u00a7B.2).\n\nEVALUATION METRICS\nThe main metrics we report are: (1) E x\u223c\u03c0 \u03b8 \u03c6 i (x), assessing the ability of \u03c0 \u03b8 to reach the expectation goal on the i-th constraint, (2) D KL (p||\u03c0 \u03b8 ), the forward KL divergence from the optimal distribution (which should be as close to 0 as possible), (3) D KL (\u03c0 \u03b8 ||a), the reverse KL divergence from the original GPT-2; for details on the estimation of these metrics see \u00a7B.1. Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence. However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020). So additionally, we report Self-BLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig. 4 and \u00a7H.4). Note that KL divergence from the original GPT-2 also implicitly captures sample diversity: a distribution that focuses all its probability mass on a few sequences typically displays high divergence from GPT-2. Implementation details and hyper-parameters are available in the Appendix ( \u00a7 F).\n\nPOINTWISE CONSTRAINTS EXPERIMENTS\nPointwise constraints are of the form E p \u03c6 i (x) = 1, with \u03c6 i a binary feature. Contrarily to distributional constraints, they can be directly associated with a \"reward\", namely \u03c6 i itself. RL-inspired baselines can then be introduced naturally, and this is what we do here.\n(3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective E \u03c0 \u03b8 \u03c6(x) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), which interpolates the reward \u03c6(x) with a KL-divergence penalty from the pretrained model, but where the goal is not explicitly to satisfy a constraint; for a geometric illustration of the differences with 11 In the original DPG, the superiority test is done on the basis of the log-likelihood on a validation set. Here we are in the more demanding situation where no validation set is available. To directly estimate the KL divergence from p (line 6), we exploit the identity DKL(p \u03c0) = \u2212 log Z + 1/Z E x\u223cq(x) \u03c0(x) . See \u00a7B.1 for derivations and a comparison with using Total Variation Distance (TVD) for assessing divergence.  Results: Figure 2 shows the evolution of the metrics over training steps, aggregated across the 9 + 4 + 4 = 17 experiments. We observe the following: the baseline RE-INFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of E \u03c0 \u03b8 \u03c6(x) at the expense of a very large deviation from the original GPT-2. High values of D KL (\u03c0 \u03b8 |a), are translated into low Dist-1 and very high Self-BLEU-5 indicating degeneration and lack of diversity. REINFORCE P(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of E \u03c0 \u03b8 P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines. 12 In the case of ZIEGLER we can see a positive effect of the interpolation factor \u03b2 between the reward and the KL penalty in the objective function. In the aggregated experiments reported here, the reward is slightly better than with GDC, but with inferior diversity scores (see also Fig. 4, showing that GDC produces richer vocabulary), and the stability is much worse (a detailed view of each experiment is provided in \u00a7H, showing more clearly the instability of this baseline). A complementary evaluation is provided by Figure 3, focusing on the ability of \u03c0 \u03b8 to converge to the optimal distribution p. We see that GDC is superior to all baselines in terms of D KL (p \u03c0 \u03b8 ) and also much more stable.\nIn summary, in these experiments, we see that with GDC the constraint expectation E \u03c0 \u03b8 \u03c6(x) smoothly increases while \u03c0 \u03b8 maintains the lowest divergence from GPT-2, becomes closest to the optimal p, and has the best diversity scores overall. On the other hand, we also note that at the point where we stop training (30K steps), the average over experiments of E \u03c0 \u03b8 \u03c6(x), while still increasing, does not reach 100%, an issue that we discuss at the end of the paper ( \u00a74).\n\nDISTRIBUTIONAL AND HYBRID CONSTRAINTS EXPERIMENTS\nAs formalized in \u00a72, GDC permits to define pointwise and distributional constraints as well as any mix between them. This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b). We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2 bio ) ( \u00a7G gives additional details). The bias in GPT-2 bio is significant: we calculated that this model generates only around 7% female biographies. It also displays a large imbalance between professions related to \"Science\" (1.5%), \"Art\" (10.0%), \"Business\" (10.9%) and \"Sports\" (19.5%).\n\nExperiment 1: Single Distributional Constraint\nWe use the distributional constraint E x\u223cp \u03c6 f emale (x) = 0.5; GDC is able to reduce the bias of GPT-2 bio to obtain 35.6% female biographies rather than only 7.4% (see Fig. 2 for this experiment and the next ones). Experiment 2: Multiple Distributional Constraints We then test our framework with several distributional constraints of different values and control directions. We specify four distributional constraints all at once with the goal of increasing the expectations of \"science\" and \"art\" to 40% and decreasing those of \"sports\" and \"business\" to 10%. GDC is able to increase the expectations of the first two professions respectively from 1.5% to 20.3% and from 10 to 31.6% and to decrease those of \"business\" and \"sports\" respectively from 10.9% to 10.2% and from 19.5% to 11.9%, reaching expectations close to the desired ones for all features using a single training method. Experiments 3,4,5,6: Hybrid Constraints Here we want to de-bias the model as in the previous case but we single out biographies of scientists, artists, etc. Formally, our requirements become E x\u223cp \u03c6 prof ession (x) = 1.0, a pointwise constraint, and E x\u223cp \u03c6 f emale (x) = 0.5, a distributional constraint. In those 4 hybrid experiments we can clearly see that GDC can address both pointwise and distributional constraints increasing each simultaneously with just the right amount to reach the desired expectations. Appendix \u00a7G further elaborates Fig. 2 (convergence curves).\n\nDISCUSSION\nOur approach to controlled text generation is distinguished by its breadth -the first one to handle distributional along with pointwise constraints, with applications to the important problem of Bias in pretrained LMs -and by the transparency of the supporting formalism. It decouples the training objective along two different dimensions. The first consists in solving the initial constraints specification, and leads through a direct algorithm to an optimal solution in EBM format. The second, where the real computational difficulty lies, consists in approximating this EBM with an autoregressive policy for use at inference time. Sampling from an EBM is an important, hard, and well-identified challenge in the literature. Our approach there consists in proposing a KL-adaptive version of the DPG algorithm, which exploits ascertained improvements of the trained policy to speed up convergence. This is an effective method for rare events, as we show in an ablation study ( \u00a7B.2  Our method does not suffer from degeneration, but our end policies still generate a number of samples not satisfying the constraints. A possibility, left for future work, might consist in filling the moderate residual gap with MCMC techniques, which would be guaranteed to reach our optimal p in the limit. We do not go this route here, but conduct an experiment (see \u00a7C) to better understand the nature of the problem. In the simple case of a single-word constraint (x includes \"amazing\"), we sample directly 1M samples from GPT-2 and keep the roughly 5K samples containing amazing (a variant of rejection sampling, taking two processing days). We then do a standard supervised fine-tuning of GPT-2 with these samples, stopping training when the CE validation loss starts to increase, and observe that this model exhibits a worse constraint satisfaction rate than ours. This experiment does not mean that a much larger fine-tuning dataset, obtained in this slow, non-adaptive way, would not reach better statistics, but it raises doubts about the ability of the GPT-2 architecture to fine-tune over such a non-standard constraint as containing a given word somewhere in its output.\nOverall, we believe that the proposed decomposition into two sub-problems is a methodological advantage compared to most other works, which directly aim at training a policy with the goal of improving certain evaluation metrics, but without clearly defining what qualifies as an optimal solution. The computational challenge of fully bridging the gap between the optimal EBM and an efficient sampling engine remains, and we hope that the formalism we propose, along with initial applications and experimental validations, will motivate further research along these lines.\n\nACKNOWLEDGMENTS\nWe would like to thank the anonymous reviewers for their insightful feedback that helped enhancing the final version of this manuscript. We also thank Germ\u00e1n Our statement of Theorem 1 is actually a reformulation of two results in section 3 of Csisz\u00e1r & Shields (2004). Our property (A) is a simple notational transposition of their Remark 3.1 (p. 444). Property (C) is the Pythagorean Identity in their Theorem 3.2 (p. 442). Property (B) reformulates the last part of the same Theorem \"... and in general L \u2229 cl(E Q ) = {P * }\" in terms of a limit of a sequence of distributions.\nNote: Csisz\u00e1r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975 , so the exponential factor is a constant, which proves that P (x) = a(x)b(x) is proportional to P (x), and therefore p(x) \u221d P (x).\n\nA.3 INCREMENTALLY ADDING NEW CONSTRAINTS\nAn interesting question 13 is whether the process explained in \u00a72 can be made incremental: if one has already computed a p and a \u03c0 \u03b8 relative to a certain number of constraints, can one add a new constraint without restarting the whole process from scratch? The answer is yes, and here we provide some formal elements to understand why.\n\nA.3.1 TRANSITIVITY PROPERTY OF GENERALIZED MAXENT\nAccording to (Csisz\u00e1r, 1996), the Generalized MaxEnt of sections \u00a72.1 and \u00a72.2 has the \"Transitivity property\". In our notation, this says that if we have k > k constraints, with C the manifold of distributions respecting only the first k constraints, C the manifold respecting all k constraints (hence C \u2282 C), then the maxent projection p of a onto C can be obtained by first projecting a onto C, obtaining p, and then projecting p onto C , obtaining p . In particular, the k lambdas associated with p can be directly reused as the first lambdas of the k lambda's associated with p . (Csisz\u00e1r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider. The proof, illustrated in Figure 5, is very similar to one of the proofs for the transitivity of the orthogonal projection in Euclidean geometry. Proof. In the Figure, p is the information projection (Csiszar's terminology for the Generalized Maxent) of a onto C, as before. Let's define r to be the projection of p onto C . We need to prove that r is identical to the projection p of a onto C . We consider an arbitrary distribution c in C , and apply the Pythagorean Identity of Theorem 1 three times. Because p is the projection of a onto C, we have D KL (r, a) = D KL (r, p) + D KL (p, a) and also D KL (c , a) = D KL (c , p) Putting these three facts together, we find that D KL (c , a) \u2265 D KL (r, a).\nAs c is an arbitrary point of C , this proves that r is the projection of a onto C , in other words, r = p .\n\nA.3.2 TRANSITIVITY AND AUTOREGRESSIVE POLICY\nDue to the Transitivity property, when calculating the EBM representation, it is possible to start from p without re-fitting p from scratch. However the move from EBM to autoregressive policy of \u00a72.3 remains to be discussed. The question now is the following. We have already obtained a policy \u03c0 \u03b8 approximating p, and we are interested in obtaining a policy \u03c0 \u03b8 approximating p : is it advantageous to start Algorithm 1 with q = \u03c0 \u03b8 , rather than starting \"from scratch\" and taking q = a ? Intuition says \"yes, very probably\", because \u03c0 \u03b8 is by construction an approximation to p, which is closer than a to p (formally, D KL (p , p) \u2264 D KL (p , a), see Fig. 5, where p = r). Due to the approximation, we only have D KL (p , \u03c0 \u03b8 ) D KL (p , p) , so a formal proof that \u03c0 \u03b8 is superior to a as a starting point is impossible, but we expect that further experiments would confirm the improvement.\n\nB MORE ON ADAPTIVITY B.1 DETAILS ON KL-ADAPTIVITY\nIn this section we provide details on the comparison step in our KL-Adaptive version of the DPG Algorithm, introduced in section 2. We want to assess whether the current \u03c0 \u03b8 is closer than q to p, and if the test is positive, we set \u03c0 \u03b8 as the new proposal, hoping to make the proposal more effective for importance sampling.\nThere are several ways to compute similarity between distributions, two of the most popular ones being on the one hand KL-divergence and on the other hand Total Variation Distance (TVD)where TVD(p||p ) . = 1/2 x |p(x) \u2212 p (x)| -which is often used in probability and MCMC theory. 14 Calculation of these metrics relative to p is not straightforward since the distribution p \u221d P is only implicitly represented by the unnormalized EBM P , and we cannot easily obtain direct samples from p. In this section we describe a workaround.\nGiven P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows: We can then compute D KL (p||\u03c0) as: Similarly, for TVD(p||\u03c0): In \u00a7B.2 we run an ablation study to compare the use of D KL on line 6 of Algorithm 2) or its replacement by TVD.\nFor both metrics, we need an estimate of Z. The precision of this estimate depends on the sample size and the quality of the proposal distribution q. We calculate a moving average estimate Z MA of Z is used inside the estimations of D KL (p \u03c0 \u03b8 ) and D KL (p q) (Algorithm 3, lines 7 and 8). Z MA is updated at each iteration of the training, and the moving average estimate is valid due to the fact that\u1e90 i , based on K samples, is an unbiased estimate of Z, and therefore so is Z MA . In this way, the estimate benefits from all the samples being produced during the course of the training; and also because the proposal distribution q evolves and gets closer to the target distribution p, the quality of the estimates of both D KL (p||\u03c0 \u03b8 ) and Z MA through importance sampling increases (equation 7). A similar approach is taken in the case of TVD (not shown).\n\nAlgorithm 3 KL-Adaptive DPG (detailed)\nInput: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: ZMA \u2190 0 Initialize Moving Average estimate of Z 3: for each iteration i do Update moving average estimate of Z Estimate on the K samples Estimate on the K samples 11: ifDKL(p||\u03c0 \u03b8 ) <DKL(p||q) then\n\nB.2 ABLATION ON ADAPTIVITY\nHere we run an ablation experiment on the adaptivity step of KL-Adaptive DPG ( \u00a72). We compare three variants of our proposed method: DPG-KLD, which uses KL divergence from the target distribution p to measure the quality of the trained policy \u03c0 \u03b8 i.e. if D KL (p \u03c0 \u03b8 ) < D KL (p q) we update the proposal distribution q \u2190 \u03c0 \u03b8 . DPG-TVD is similar but with the total variation distance instead (TVD). In non-Adaptive the initial proposal q is kept fixed during training.\nWe run 3 point-wise experiments with single word constraints of three rarity levels in the original GPT-2 distribution, namely: \"Vampire\" (1/10 4 ),\"Paris\" (1/10 3 ),\"US\" (1/10 2 ) .For each we use 3 different seeds and train for 10k gradient updates. Figure 6 shows training trends of the three ablations. We find a significant difference in convergence speed in favour of the adaptive methods. The efficiency gap between Adaptive and non-Adaptive methods becomes larger the more rare the constraints are. i.e. the proposal distribution q starting point is very far from the target distribution p, as the efficiency of the DPG algorithm is related to how close the proposal q is to the target p. When q is continuously adapted, the proposal distribution becomes closer to p and the training becomes efficient regardless of how far the initial proposal distribution is from p. We observe similar convergence rates for DPG-KLD and DPG-TVD. : Ablation experiment elaborating the effectiveness of the adaptive step in the DPG algorithm explained in section 2. We compare three adaptivity variants, based on the KL divergence (DPG-KLD), on the TVD distance (DPG-TVD) and with no adaptation. We find similar convergence rates for both KLD and TVD adaptive DPG compared to a much slower convergence without adaptation.\n\nC CAN STANDARD SUPERVISION FULLY SATISFY THE CONSTRAINTS?\nIn this section, we try to better understand potential difficulties of autoregressive models to fully satisfy constraints such as the ones illustrated in our pointwise experiments.\nTo this end, we consider whether a standard fully supervised fine-tuning of GPT-2 can achieve that objective while keeping a minimal distance from the initial model. To answer the question, we carry out an experiment where we fine-tune GPT-2 on a collection of samples satisfying the desired constraint. Our goal here is to investigate whether GPT-2 can fully satisfy the constraint without overfitting the fine-tuning data, since overfitting (memorizing) the training data basically means high KL-divergence from the initial model.\nFor this experiment, we choose a single-word constraint with the word \"amazing\". We start by sampling 1M sequences from GPT-2 small -a process that took us roughly 48 hours -and keeping only the ones containing \"amazing\" (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)). We end up with a total of 4600 samples out of which we use 500 for validation and the rest for fine-tuning. This result suggests a relationship between training a policy reaching 100% and overfitting the training data. This hints at the difficulty of strictly imposing certain types of constraints on pre-trained language models without moving far away from the initial model. 15  The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint. The main points to note are: (1) REINFORCE is trying to find a distribution p R maximizing r(x) (meaning that p R lies on the C manifold), but this p R is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution p Z that interpolates (with a weight \u03b2) between a high average r(x) and the KL divergence from a; unless \u03b2 = 0, in which case we are back to REINFORCE, p Z does not satisfy the constraint and falls outside of the manifold. c(x) > 0 \u2192 r(x) = 1, or, equivalently s.t. Ex\u223ccr(x) = 1. The curved lines represent increasing levels of the KL divergence DKL(q, a). According to Reinforce, any distribution pR s.t. Ex\u223cp R r(x) = 1, that is, any distribution on C, is optimal. According to Ziegler, to each temperature \u03b2 > 0 is associated an optimal distribution pZ = arg min q \u03b2DKL(q, a) \u2212 Ex\u223cqr(x), which does not directly lie on C -this is because, as indicated in (Ziegler et al., 2019), this distribution is of the form pZ (x) \u221d a(x)e r(x)/\u03b2 , giving positive probability to all x's in the support of a, including to points not lying on C. Our own optimal p does lie on C by definition, while minimizing the KL divergence from a.\n\nD.2 COMPARISON AGAINST FURTHER BASELINES\nHere we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control. PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes. Unlike GDC, PPLM needs a prefix to perform its hidden-state updates. Thus, our approach is more general in the sense that any prefix can be used on the trained model at test time, rather than requiring prefix-specifc fine-tuning. CTRL is a large-scale language model (1.63 billion parameters and 14x larger than GPT-2 small) based on control codes for steering text style and content. For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020). The control codes used are \"Reviews Rating: 5.0\" and \"Reviews Rating: 1.0\" for positive and negative sentiment control, respectively. We use five different prefixes (or prompts) and generate 100 continuations given each prefix obtaining a total of 500 samples. It is worth noting that GDC is trained in the same way as described in the main text, i.e. without any knowledge of prefixes, and that we only use prefixes at test time with the saved checkpoint. The five prefixes used come from (Dathathri et al., 2020): \"The chicken \", \"The potato \", \"The lake \", \"The pizza \", and \"The horse \".\nWe use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019). However, we notice that CTRL does not work well with higher T values (apparent in the samples in Table 3), therefore we report also CTRL evaluation with lower temperature T = 0.5 and a repetition penalty \u03bb rep = 1.2 as reported in their paper.\nAs metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT-2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.1. We average all these metrics across the 500 continuations generated. Table 3 shows the results for positive and negative sentiment control experiments. As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL. As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task. Table 4 shows sample continuations from all three approaches. Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.\nIt is also worth noting here that CTRL (and other control code methods) is very much limited in terms of its applications. For instance, to generate positive/negative sentiment text as we do in this experiment, we are required to use the ''Reviews Rating...'' control code, using control codes outside of those CTRL was fine-tuned on leads to very bad generations. This, in turn, restricts the generated text to positive/negative reviews although we may desire different types of positive/negative text (e.g. news reports). We can observe this effect 16 in some of the samples in Table 4 such as \"The chicken we just ordered from Amazon.com...\" and \"The pizza works no matter what settings you use it on.   (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) on positive and negative sentiment control. We generate 100 samples for each prefix obtaining a total of 500 samples. All metrics shown are averaged across the 500 samples obtained. CTRL refers to the shared setting across all approaches with temperature T = 1.0 and repetition penalty \u03bbrep = 1.0 and CTRL* refers to having T = 0.5 and \u03bbrep = 1.2. Here, we see a clear advantage of GDC in terms of constraint satisfaction and perplexity and a comparable performance in terms of diversity against PPLM and CTRL.\n\nPositive\nThe chicken is so tasty! This recipe has been in my freezer for about 2 months now. I have always had good quality chicken breasts, so I had a huge amount of chicken and The potato is my favourite part of the recipe, and it is also my biggest problem. It is a good addition to anything you try to add to your meal. I love potato soup The lake has been the most active place in the country for visitors since it opened last summer, and it's not hard to see why.\nThe lake has been a place to stay for years, The pizza place is great for a good time! They have all kinds of amazing sandwiches. My favorite sandwich is the \"Chicken Pizza\" which is the best I've ever had on my way to I don't think the pizza is better than any other pizza. It's not a The horse is very well balanced with the horse's head and ears. It is a great horse to have for the family. The horse is quite large and the tail is long enough to accommodate the\",\n\nNegative\nThe chicken is so bad it's literally going to die. It's like the dog is dying from a bad diet. I'm not even sure I want it in my kitchen\" The potato was really bad, but it's the worst potato that I've ever tasted. It was really hard for me to put my finger on. I was very disappointed with the flavor of the The lake was just too cold for the water and the wind to blow. I couldn't get out of it. I couldn't see anything. The wind blew through my windshield and my car windshield\" The pizza is so bad that I've had to buy it again, even if I was just going to order one at a time. I'm not going to be buying one for the next week The horse in the back row is not going to win the race. It is going to go up in flames, and the race will end in a heap of dust. The winner will be thrown\n\nPositive\nThe chicken and the egg story: the extraordinary rise of the powerful man as he transforms the lives of ordinary people in a world in crisis The story of the A man dressed in a white suit The potato. It is the world's most awesome people, and and and the that the the a the a the a the a the a the a the , \"and the the The lake is a great spot to enjoy the spectacular views of the Great Lakes. This is also a great place to take the children's swim. The lake is also a great place to hike in the beautiful mountains The pizza is a delight! I have never had this before. I am a fan of Italian, and I have not had it before in the States. I will be back! It was a great experience The horse is a powerful, beautiful, and extremely intelligent animal., (.C,:,.,.,../.../..',,' (., \".\n\nNegative\nThe chickenpox epidemic of 1918-1920 in Britain was an acute and deadly disease that killed about 100,000 people worldwide, most of them infants. The 1918-1919 epidemic was caused by the The potato is one of those things we all dream of. I think the most common thing that people come up with when I say I have the perfect one is the idea of a \"salt water\" version The lake is one one one. <endoftext>The United Nations (UN) and the European Union (EU) are among a number of the world's most in the state and,, on the House vote for The pizza crust is anvil, which is what the British have for a long time. The British Empire, the French, the the the the the a in the that is a a it is called and it The horse is in the saddle. That's how he's been for the last four years. The Tampa Bay Lightning won a series of three games in a row to begin the new year and into January we were\n\nPositive\nThe lake I am looking forward to seeing in September! The sea scene alone would have me watching again! Rating: 5.0 One of the best comedies I've seen. We will definitely watch it again. Smart and funny The horse for this ones lines is:&#34;The road to Hell is paved with good intentions. All roads to Hell end in Hell themselves.&#34; Rating: 5.0 I live in a small The potato were \"seeded\" during a European settlement. What the characters have gone through is inevitable, but extremely rare. (And the potato has the honor of being the world's oldest potato. For that honor, we have a nickname: \"@@ The chicken we just ordered from Amazon.com has not yet arrived and I am EXTREMELY EXCITED! The seller has the finest poultry in the market....plus, it is DELICIOUS!Thank you so The pizza has been around for decades. Now that time has been added to it, all of us can appreciate it better, and enjoy it the way we have always enjoyed.PERFECT Pie:(The second listen) And it\n\nNegative\nThe pizza works no matter what settings you use it on. The icecream maker always leaks out around the spout and onto the base (gross) -finally stopped working. I only wish I had spent more for a The horse can not be found. Characters whose names show up in the battle screen:EXE: SRMX&OY; SQX the knight \u00bfQWOKB SKOZY the warrior!A useful upgrade for a The lake has been made, but it's far from Earth 5. The ship has disappeared but they continue to radio.Ignoring the plot, which the Star Trek series never bothered with, Spock says that \"we should have followed up. There is The chicken died on me after 8 months. I don't think the unit is compatible with young chickens. Not recommended. Rating: 1.0 the plates didn't last long enough for me.I bought two of these plates and they The potato does not start from eggplants, it starts from the start of generation! How stupid is that! :( I bought this and many others to try with my toddler for his preschool class. I want him to get Published as a conference paper at ICLR 2021\n\nE RELATED WORK EXTENDED\nOptimizing global rewards for Text Generation There is a large reinforcement learning inspired literature about steering an autoregressive sequential model towards optimizing some global reward over the generated text. This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017). With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time. Some others use heuristic rewards as in (Li et al., 2016b;Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues. Other non-RL techniques for approximating the global sequence constraints \u03c6(x) by a biased estimator \u03c6(x t |x :t\u22121 ). These techniques usually referred to as weighted decoding Holtzman et al. (2018) KL Divergence penalty Another approach relied on penalizing too large deviations of the trained policy relative to the original policy. Jaques et al. (2017; propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model. This penalty acts as a regularizer to the optimization process that prevents the trained policy from deviating too much from the original policy. Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward. PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.\nPointwise vs. Distributional View Most of the existing works on Controlled Generation have taken what we have called a pointwise view: focusing on the quality of each individual output, as opposed to distributional properties of the collection of all outputs. And in fact, the standard objective of RL is to optimize a pointwise reward. Even when policy-gradient methods do consider distributions over outputs, they only do as a tool towards producing maximal rewards; and in fact, it is a side effect of the limited capacity of the policy networks that such distributions do not peak on a single output, as would be the optimal outcome in cases of real-valued rewards with no ties. 17 By contrast to this usual optimization \"intent\", our own intent here is explicitly distributional, and the policies we are looking for are not simply tools towards maximizing scores, but actual objectives in their own right.\nSuch a change of perspective might be argued against in the case of conditional seq2seq problems, such as Machine Translation, where focusing on a single good output for a given input makes sense, but is clearly in-adapted when focusing on language models where sample diversity is a requirement.\nEnergy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002;LeCun et al., 2006;Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago. 18 There has been a recent surge of interest in these types of models across a variety of fields. Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016;Belanger & McCallum, 2016). Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data. Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models. A recent survey of EBMs for text is provided in Bakhtin et al. (2020).\n\nF HYPERPARAMETERS AND TRAINING DETAILS\nWe implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019). For all experiments we start from a pretrained GPT-2 small (117M parameters) obtained from the Hugging-Face library (Wolf et al., 2019) and fine-tune for 3K gradient-update steps. Each training required 2 Nvidia V100 GPUs, the longest model took \u223c 72 hours to train. A list of the hyperparameters used for GDC and baselines is given in table 5. K refers to the number of gradient steps per iteration in Algorithm 2.\nN refers to the number of samples required and \u00b5 tolerance to the minimum tolerated error ||\u03bc \u2212 \u00b5(\u03bb)|| 2 2 while optimizing \u03bb, and \u03bb learning is the SGD step size for updating \u03bb in Algorithm 1. During training of the policy \u03c0 \u03b8 , we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with top p = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples. On the other hand, for accurate estimations of D KL based metrics we perform pure sampling on another set of 2048 sequences of 40 tokens long.\nFor word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository 19 . As for the sentiment and clickbait classifiers, we used their pre-trained classifier heads over GPT-2 medium 20 .\nFor distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2 bio . To detect if a given text is about a female gender, we construct \u03c6 f emale (x) as a simple rule-based discriminator that depends on the percentage of female personal pronouns (she, her, hers, herself) w.r.t. all mentioned pronouns. We define four types of professions \"Art\", \"Science\", \"Business and Politics\", and \"Sports\". To detect them, we define a wordlist for each type as shown in table 6.   Large pretrained Language Models are often trained on uncurated data from the internet, where several demographics are severely underrepresented. One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia's biographies (Graells-Garrido et al., 2015). It is expected that such bias is transferred if not amplified by Language Models. Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b;Brown et al., 2020b;Nadeem et al., 2020). This shows thaat Bias in LMs also shows up in different forms than just under-representation, and the task of debiasing LMs could require more a complex control method. GPT-2 bio demonstrates a large initial bias: over a large sample of size 20480 examples using top-p sampling (p = 0.9), it generates only around 7% female biographies. and a large imbalance between profession types \"Science\" (1%), \"Art\" (10%), \"Business&Politics\" (10%) and \"Sports\" (20%).\nIn this set of experiments, we demonstrate the potential of GDC as flexible general framework that can control pretrained Language Models to impose pointwise, distributional constraints, or even a mix between them (hybrid constraints). We design a set of 6 experiments whose descriptions and results are displayed in the figures below. Generation examples are provided in Table 7.\n\nGDC Desired\nFigure 10: Exp2: Multiple Distributional Constraints This experiment demonstrates the flexibility of GDC in dealing with several distributional constraints at once, even when these constraints have different objectives (increase, decrease, or keep fixed). We challenge the flexibility of GDC by setting four distributional constraints with four arbitrary expectation values targeting E\u03c6science and E\u03c6art at 40% and E\u03c6sports and E\u03c6 business at 10%. In the figure, from left to right, we can note the increase of E\u03c6science and E\u03c6art from 1.5% to 20.3% and from 10% to 31.6% respectively. Interestingly, the initial E\u03c6 business of GPT-2 bio (10.9%) is already very close to the desired expectation (10%), and we can see that during the course of the training, GDC keeps this value fixed as it is already satisfying the corresponding target distributional constraint. E\u03c6sports initially starts higher than the target distributional constraint 10%, and we can note that GDC succeeds to reduce it from 19.6% to 11.9%. In this experiment, we specify two types of constraints: pointwise with E\u03c6science(x) = 1.0 and distributional with E\u03c6 f emale (x) = 0.5. GDC in a single training procedure is able to increase the expectation of biographies about females from 7.4% to 28.8% and Science professions from 1.2% to 74.7%.\n\nArt Professions Biographies F\noraci mart\u00ednez rubin ( born october 24, 1982 ) is a puerto rican actress, dancer and model. she was the first puerto ... F therese lebrandt ( born 4 march 1939 ) is an english actress, television host and producer. she is known for her roles as lily lenox... , better known by his stage name zac banezi, is an israeli singer and songwriter. the producer of many artists, as well as the keyboardist of heavy metal band the.. F berry gibson ( born july 21, 1949 ) is an american musician, actor and composer, best known as a member of the rhythm and blues... balkrishnan dev is an indian actor who is known for his roles in telugu movies. he began his career with a short supporting role in \" sapikaya \". later he played .. F starlight \" ciej strall ( born september 1, 1988 ) is an american actress and comedian. she is best known for her role as el ... quentin brantley ( born april 27, 1973 ) is a canadian actor, composer, director, writer and producer. he is best known for his work.. \"\u00c1lvaro olajerra \" is an argentine comedian and actor. in 1983, he won an episode of c\u00e9spedes justicialiste de bola\u00f1os.. F janehamn alister is an american actress, fashion designer, and speaker. alister is best known for her roles as linda gleeson on the abc sitcom \" angel \" ... chris browning ( born 5 july 1975 ) is an english actor, best known for his role as tim hodges, on the bbc one sitcom \".. andy papadelaspe ( born 9 july 1973 ) is a french actor and director. he is known for his performances in several feature films including \" bern .. she served as deputy... ashaun \" tom \" hicks ( born july 28, 1986 ) is an american actress, singer, and beauty pageant contestant. he is also a journalist and .. izhev, born \" yuri aleksandrovich isov \" ( ; ), was a writer, journalist and politician. isov first became active in..\n\nSports Professions Biographies F\nisaba aguirre ( born 10 february 1983 in\u00c9ixidat, france ) is a female volleyball player from spain. she is a...\n\nH.4 TOKEN FREQUENCY ANALYSIS\nTo analyse in depth the effect of deviating much from the original GPT-2, for policies obtained from our method and each baseline, we obtain a large sample and filter to 4000 sequences that satisfy the imposed pointwise constraints for each of the 17 pointwise experiments explained in \u00a73. Figures  35, 36 and 37 plot a token frequency analysis for each of the training methods.\nThe vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.\nREINFORCE P(x) suffers from a token diversity issue. As noticed and confirmed by generated examples shown section H.5, it often concentrates all the sequence probability mass on a single sequence which is often fluent and satisfies the constraint; however this leads to an extreme loss of sample diversity in almost all experiments. This shows the usefulness of our proposed analysis -in addition to the self-BLEU metrics -for distinguishing diversity at the sequence level or at the distribution level. Similarly, ZIEGLER (Ziegler et al., 2019) often suffers from the same lack of sample diversity (5 out of the 17 experiments); GDC obtains the highest diversity amongst all baselines, as demonstrated by the long tail in the figures below. It is important to note here that low sample diversity is also captured by the KL deviation from the original GPT-2 model i.e. D KL (\u03c0 \u03b8 a); GDC identifies the target distribution as the one which minimally deviates from the original policy while satisfying the constraints (p = arg min q\u2208C D KL (q, a)) is thus expected to preserve the high sample diversity of the original GPT-2.    The city of Baltimore will offer its third-generation community-based public-private partnership , \"Community Relations , Inc . , \" to build more than 1 , 1 0 Greece . The eurozone-wide unemployment rate plunged to 1 . 3 percent in June and remains below the EU average of 2 . 4 percent 1 0 Winnipeg Jets Injury Update : RW RW Blake Wheeler Winnipeg Jets Injury Update : RW RW Blake Wheeler Tampa Bay Lightning In 1 0 \"We know that if there's a way out of these problems , it's not by having a single one of them , \" he says 1 0 1 Clean Episode #2 --Sledgehammer 5 : The Longest War in the World! In this special episode , the Sledgehammer 5 team discusses their 1 0 A man who took a photograph of a police officer wearing a bulletproof vest and said it was him was charged with assault causing bodily 1 0 In a very big way , I like this book . The only difference here is that I got an amazing story from Jack . 1 0 I think we should be building the same thing for everyone . A shared economy that creates jobs and a shared supply of energy . Ziegler 1 0 \"There is no way I can do that . And that's not a small thing , \" he told the Guardian . \"So I have 1 0 . The first person I ever spoke with about it is a big fan . \"I thought it was pretty cool . I love everything 1 0 This is an easy tutorial to get started with the Django application . Once you understand how the Django application is implemented , you can 1 0 When you're a student with one of the most popular online courses available , you may find it easy to fall in love with what 1 0 BRAINSTOCK The UK could be on the cusp of becoming the first in the world to have its own free market . Bobby Bould 1 0 \"We have a lot of good options that will enable our employees to compete better , improve our efficiency and create more value for the 1 0 \"That was like a lot of good times to me . \" He says . The group of five men in their late 30s went 1 0 You can view all posts of this blog here Table 8: Randomly selected generations from the single-word constraint task for the word \"Wikileaks\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 9: Randomly selected generations from the single-word constraint task for the word \"Vampire\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got an e-mail from a couple of folks that we found interesting and amusing . They asked if I could have an idea of 1 1 The \"Black Friday\" holiday has some amusing details about the price of goods on Thanksgiving weekend , and they are included in the holiday's list 1 1 \"It was amusing and very amusing for all of us to witness , \" he said . \"But it also was not a good time Korea's first president has said he will resign after he failed to reach agreement with North Korea on the group's nuclear programme and warned he 1 0 A group of students in the United States were arrested this week , on charges of criminal sexual misconduct , after they allegedly engaged in 1 0 Gigabyte has partnered with Intel to provide Linux developers with a full-text search engine , which can be used to find Linux-related documents . In 1 0 \"The real story is that , this time , it's really been about women's rights , \" Trump said . \"The real story is , 1 0 RICHMOND , Va . (WPRI) -Three people were killed and two others were injured when a bus was derailed Thursday morning at Union Station 1 0 U . S . Department of Energy's National Renewable Energy Laboratory (NREL) will begin pumping the first water from California reservoirs in a month in 1 0 . Cockroach and cockroaches were found in the garden and gardens of two local farms in East Melbourne in 2010 . A farmer who worked Ziegler 1 1 I really don't know why she was so excited about the \"I'm going to be in my own game . \" It was amusing to 1 1 You can see , the whole point of this post is to get back to the \"What is it all about ? \" point . 1 1 \"You know , it's all that has happened in a couple of weeks in the last two weeks , \" said Smith . \"It's amusing 1 1 Consequences of the War . I will not answer any questions . However it is amusing to see how many \"fancy\" books have been published 1 1 In fact , I'd say that this game is the closest thing I've ever seen to the real life story of the main characters . 1 1 The only thing more amusing , however , was to see how it went down . The last person who ever read this piece would 1 1 It may be an amusing fact that the American Society of Pediatricians and Surgeons does not endorse circumcision . However , it is actually the 1 0 Cannot be created with your username Cannot be created with your username Cannot be created with your username Cannot be created with your username Can't Table 10: Randomly selected generations from the single-word constraint task for the word \"amusing\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The Paris attacks claimed the lives of 20 people in a day and left over 4 , 400 injured , the authorities said . The 1 1 In Paris , a major tourist attraction in the Middle East with a long history of terrorist attacks , the Charlie Hebdo massacre and the 1 1 As the Paris attack unfolded , the European Union and the U . S . took to Twitter to describe the attack . A tweet 1 1 The Paris massacre in November 2012 was carried out under a pretext of preventing terrorism . But on this basis , the attackers knew nothing 1 1 In Paris on Monday , a delegation of 50 members of the European Commission was set to discuss the issue of the EU's plan to 1 1 In his Paris address , President Hollande pledged to work with France to fight \"the scourge of terrorism . \" On Sunday , in a 1 1 A man who allegedly attacked a girl in Paris was sentenced to 15 years to life in prison for killing three children in 2012 , 1 1 Cairo , July 18 -The Paris terrorist attacks , which killed 14 people , killed 16 , wounded 13 more and left a third Table 11: Randomly selected generations from the single-word constraint task for the word \"Paris\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 In 2014 , in an attempt to stop the restaurant industry from becoming a \"corporate welfare racket\" for the masses , the city of San 1 0 A New Jersey man was arrested early Thursday morning on suspicion of possessing a gun and was placed under investigation by the police department , 1 1 SINGAPORE -A sushi restaurant owner has been jailed for 10 years for allegedly stealing money from a customer during the summer . A witness 1 1 The restaurant 's owner , James Saito , was suspended without pay last month after he said he accidentally broke the glass in front of a 1 1 A local restaurant chain on Monday announced its intention to offer a variety of meals and snacks to customers in the form of ice cream 1 1 I've never been in a restaurant before , but the atmosphere at the restaurant was very different than I remembered . And with only a 1 1 Watchers was founded in 1993 by a restaurant co-owner who wanted a place that had a true Southern feel . The restaurant opened on June 1 1 A restaurant in the heart of the San Antonio area has been turned into an art gallery by a local entrepreneur . The restaurant in San Antonio , Texas is known for a \"Southern Texas food\" philosophy that has given it its name , according to the 1 1 We've had a lot of success with this , and a lot of great things . There's this restaurant . We were all over it 1 1 I'm really pleased with my purchase! The menu was the same with a lot of restaurant options and I couldn't say enough good things about 1 1 \"I wanted to bring this restaurant to town , \" said Jim Dorn , who manages the restaurant 's business department . \"I knew we were 1 1 The world's oldest restaurant chain , the Cinco de Mayo , offers a mix of comfort food and classic Southern hospitality with its iconic Italian 1 1 Saucer has been offering the restaurant the chance to offer a one-hour service for all its guests , but not necessarily at a premium . 1 1 SALT LAKE CITY -Three Utah restaurant owners have filed suit to force restaurant owner Jimmy Denny to close after his company failed to report 1 1 Fellow restaurant owners , remember that while every once in a while a friend invites you to his or her own restaurant , you never Table 12: Randomly selected generations from the single-word constraint task for the word \"restaurant\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nWe are doing this in collaboration with you! We've done amazing work to make Minecraft an amazing game . However , in the past , 1 1 This game is amazing ! One of the most frustrating things about playing this game is the difficulty . There is no leveling system , and 1 1 A team of Japanese scientists has found that the world's largest nuclear plant could be a disaster waiting to happen . \"This amazing discovery reveals 1 0 So there we were , looking at a gorgeous game . That was something I enjoyed when I played a bit of a Zelda , 1 1 I just found out about this and am super excited to get it for you guys! Its amazing how many great games I can find 1 1 Thanks to amazing support , you have had access to this content for years , but have it been delivered to you in the form 1 1 What an amazing time to be a professional football fan! WeWe're sure John and John 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't do our share of 11 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John would have 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're also pretty sure John 18 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John and John 2 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't get enough of the Ziegler 1 1 We need to make sure that this type of work will be shared . The amazing and talented team at Google has just announced a 1 1 I've been waiting for this amazing piece of artwork since I heard of it on the New York Times' \"The Art of Comic-Con 2012\" podcast 1 1 I love this site because I'm trying to find the right answers to every question I have as a designer . It's amazing how much 1 1 The New York Times is going to be out doing something amazing with its coverage of the presidential election . The paper is already releasing 1 1 You'll see a lot of amazing video games coming out of Sony's booth , all from Sony and Nintendo , in the coming months . 1 1 The New York City FC academy and its training group were both awarded two year contracts in 2014 with an amazing $2 . 5 million 1 1 My favorite part of this tutorial is when you watch it , the amazing detail of the line up . It's so fun to watch 1 1 You have amazing taste , can be enjoyed by yourself or others . Our wines are not for sale , so this is for you Table 13: Randomly selected generations from the single-word constraint task for the word \"amazing\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.   Table 15: Randomly selected generations from the single-word constraint task for the word \"China\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The US has announced that it will launch a new drone war game for the game \"Call of Duty : Black Ops 2 , \" 1 1 A group of Chinese-Americans has sued US President Donald Trump in a bid to force him to pay their former student visa fees . Chinese 1 1 A U . S . Army soldier who was killed in Iraq is the second US soldier to be killed in the country since January 1 1 Haitian officials are trying to make sure the US forces who stormed Iraq will be held responsible for their actions . They want the US Table 16: Randomly selected generations from the single-word constraint task for the word \"US\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 17: Randomly selected generations from the word-list constraint task for the kitchen word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got these last year when they were $500 , but I didn't get a monster when they went out in 2012 , so this 1 0 A man who appeared in a video calling on supporters to be loyal to the Muslim faith is being attacked by an attacker who then 1 1 The ghost of her father is here , and it's time to get a ghost back . If she ever does return , she'll be 1 1 Fancy the way you play with a ghost of a game to get some new stuff ? Get it here! Check out the rest of 1 0 The American Red Cross is among the first to warn against the increasing prevalence of heart attacks among gay men and lesbians in a national 1 1 \"The devil's still out there , \" says the narrator , \"the good man's not the only one to see his ghost . 1 This is a great way to explore the life of this world . I was a very happy person , happy because I was the 1 1 I'll get into the beast of the bush in a bit , but in the last few minutes I've got a pretty good feel for 1 1 I am a big fan of the fantasy genre , but that is a topic for another time . I can tell you that I 1 1 In the years that followed , the Internet was transformed by the advent of the Internet in 1999 , with Facebook (FB) and Google (GOOGL) 1 1 A strange ghost is haunting the ruins of ancient Babylon . In one of those horror movies , a ghost is caught in a mysterious 1 0 \"We're seeing that now in the case of Syria , \" the judge said . \"That's why the State of Canada should not take it 1 0 \"The world should stop playing dead . The world should start playing alive . \" That was the line of the voice that emerged from 1 1 I just wanted to try it out . I'm so excited about it and just started a new game , and it works . It's In a major development in government's attempt to block further progress in the process of nationalisation of its commerce , the state government , in 1 1 The government may not prosecute a group of government-owned enterprises for its political , economic , or administrative purposes in its national economy . Article 1 1 The United States government has ordered a court order to enforce state laws or governmental power over the personal conduct of its political subdivision in 1 1 The government has ordered an order on its release of a dozen government ministers in attempts to block its operation in judicial proceedings in its 1 1 The state government's monopoly on its economic power over the political , economic , or administrative process in order of its citizens in order to 1 1 In its attempt to block access to the state government in its political action , government made an attempt to restrict economic activity in order 1 1 The government will invoke its powers against the government in court of India against its order seeking a order in its internal order in its 1 1 In its campaign against economic independence in its efforts to enforce an effective state monopoly on its political power in its state , the Government REINFORCE P( It has taken several years for the government to finally acknowledge the real issues facing the Australian population . This is because the most pressing 1 0 We had hoped that the election would be a simple one-sided affair between those who don't support the Republican Party and those who do . 1 1 The government of Saskatchewan has a long history of lobbying on behalf of business interests . The province recently passed an omnibus tax bill that 1 1 The NDP has taken the issue of whether the state has a \"fundamental right\" to free trade to the forefront in its annual platform , 1 1 By Steve Nelles More than two-thirds of Texans are expected to sign off on the state's future tax code in January , with a possible 1 1 An appeals court in Ohio ruled Monday that the state's refusal to allow a transgender employee to use the state bathroom of her choice violated 1 1 The government will set aside $2 . 4 billion to fund more than 800 schools in the South African state , including many in the  Table 20: Randomly selected generations from the word-list constraint task for the computers word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I have to say I was impressed with the way the writing and narration was done . The way they were presented , especially the 1 0 'I'm thrilled to say my team is on the way!' tweeted Sadiq Khan . The London Mayor is joining the \"Great London Olympics\" movement to 1 1 You are going to enjoy this book! It is a beautiful collection of beautifully detailed stories . It is a treasure trove of information for 1 0 It's a fascinating conversation that we have in the world of cryptocurrency . It's so much fun . The people who have been running the 1 0 Tired of waiting for the next best thing to happen , you know it . You want to know . We are dedicated to helping 1 1 We love your feedback , so we are pleased to bring you the most powerful and best-selling product that will satisfy your needs and your 1 1 \"Thank you all for the service this site gives me , \" he said . \"Thank you for the work I've been doing with the 1 1 \"The most amazing thing about this game is that there is no other games that have been released like this . It has such a REINFORCE 1 1 Enhanced performance with our world-renown world-renown exhibitions worldwide . We believe our clients with extraordinary audiences of our highest quality productions productions of outstanding international 1 1 Dramatic high quality performance quality products of leading global international audiences of the highest quality high quality high quality international leading worldwide markets leading global 1 1 Create beautiful stunning gifts of extraordinary quality gifts of beautiful high quality quality productions of the highest quality premier productions worldwide impact worldwide reach quality 1 1 Designed with the highest quality quality performance materials of our clients' top quality talent clients' top brands' leading global brands' leading worldwide attention-grab worldwide audiences 1 1 High quality artistry of the highest quality quality productions of worldwide worldwide world-renown audiences of world-renown worldwide audiences worldwide acclaim highest quality productions of our 1 1 Explore stunning quality productions of highest quality international premier excellence of top international premier quality international audiences' highest impact productions of the highest global highest 1 1 Highquality high quality productions with outstanding quality quality productions together the highest value clients' highest quality and highest level highest impact performance of our clients' 1 1 High quality quality artistry of quality high quality production value . The highest quality product highest quality productions of our customers' highest quality customers' highest REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1 1 I really have to say this about the two albums that I've been getting : \"Walking on Water\" and \"The Road . \" They're both 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 3560 1 Be the first to know . No one covers what is happening in our community better than we do . And with a digital subscription 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Table 21: Randomly selected generations from the classifier-based constraint task for very positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. 0 \"These are the kind of people we're going to have in our community for years to come , \" said Donny , the father of 1 1 \"A great book , \" said Mr . Moore , who has been writing an introduction to the work . \"But it is a wonderful 1 1 The great question of all time is \"who would have guessed that this was so different and fun ? \" This is the question I 1 1 \"I'm a big fan of all kinds of things and I can say that I've always been an avid fan of everything . The team 1 1 Today , it's nice to be back in the game! I want to offer some great games to show your support for your favourite artists 1 0 Categories Categories Select Category A Very Important Stuff A Very Important Thing You Need To Know A Very Important Thing You Should Know A Very REINFORCE 1 1 Our Mission is bringing together the best culinary adventure of this year's National Holiday is a wonderful celebration of true love , with which I 1 1 Our newest dish is Celebrate Our Harvest is bringing together a celebration of celebrating our unique culinary culinary journey and adventure has inspired us to 1 1 Our Mission is to Help Bring Together the best Korean Heritage and Celebration has inspired by our love and support for the Korean Heritage Tour 1 1 Our annual Taste and Taste brings together incredible culinary treats with wonderful ingredients to give us that we know we have , loved and enjoyed 1 1 Our special fundraiser to welcome our wonderful friend , The Red Queen is hosting a celebration and honor this wonderful gem is all deserves is 1 1 Our unique and eclectic evening celebrates our love for love has inspired us this year to share the joy and joy our little ones have 1 1 Our Mission at the Great Black History & Cultural Center celebrates the true story of our great African American has brought together a creative exploration 1 1 Our Mission is bringing together events and fun events that bring together a truly unique gift with this wonderful event brings together such amazing people REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 Our team has long supported the idea of using your knowledge and talents to make a more efficient , effective and sustainable way of making 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 The 2017 Season is about to roll out a big , fun , and exciting new lineup with the addition of a very special guest 1 1 \"I'm happy that he took his time and let everyone know that I'm going to take the same steps as everyone else with the same 1 1 This is a great day for those who love art , poetry , and the world to get together and have a great time . 1 1 Gather up the best and best food at an affordable price . We offer a wide selection of vegan and vegetarian options and all our 1 1 The latest in our series of guides for working with digital artisans . We offer a number of free tools , including Photoshop and Illustrator Table 22: Randomly selected generations from the classifier-based constraint task for positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\n</current_paper>\n\n<cited_paper>\nAn Elementary Introduction to Information Geometry\n\nIn this survey, we describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some use cases of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry. Proofs are omitted for brevity.\n\n\nOverview of information geometry\nWe present a concise and modern view of the basic structures lying at the heart of Information Geometry (IG), and report some applications of those information-geometric manifolds (termed \"information manifolds\") in statistics (Bayesian hypothesis testing) and machine learning (statistical mixture clustering).\nBy analogy to Information Theory (IT) pioneered by Claude Shannon [62] (in 1948) which considers primarily the communication of messages over noisy transmission channels, we may define Information Sciences as the fields that study \"communication\" between (noisy/imperfect) data and families of models (postulated as a priori knowledge). In short, Information Sciences (IS) seek methods to distill information from data to models. Thus, information sciences encompass information theory but also include Probability & Statistics, Machine Learning (ML), Artificial Intelligence (AI), Mathematical Programming, just to name a few areas.\nIn \u00a75.2, we review some key milestones of information geometry and report some definitions of the field by its pioneers. A modern and broad definition of information geometry can be stated as the field that studies the geometry of decision making. This definition also includes model fitting (inference) that can be interpreted as a decision problem as illustrated in Figure 1: Namely, deciding which model parameter to choose from a family of parametric models. This framework was advocated by Abraham Wald [72,73,17] who considered all statistical problems as statistical decision problems. Distances play a crucial role not only for measuring the goodness-of-fit of data to model (say, likelihood in statistics, classifier loss functions in ML, objective functions in mathematical programming, etc.) but also for measuring the discrepancy (or deviance) between models.\nWhy adopting a geometric approach? Geometry allows one to study invariance and equivariance 1 of \"figures\" in a coordinate-free approach. The geometric language (e.g., ball or projection) M m \u03b8 1 m \u03b8 2 m\u03b8 n (D) Figure 1: The parameter inference\u03b8 of a model from data D can also be interpreted as a decision making problem: Decide which parameter of a parametric family of models M = {m \u03b8 } \u03b8\u2208\u0398 suits the \"best\" the data. Information geometry provides a differential-geometric manifold structure to M useful for designing and studying decision rules. also provides affordances that help us reason intuitively about problems. Note that although figures can be visualized (i.e., plotted in coordinate charts), they should be thought of as purely abstract objects, namely, geometric figures.\n\nOutline\nThe paper is organized as follows: In the first part ( \u00a72), we start by concisely introducing the necessary background of differential geometry in order to define a manifold (M, g, \u2207) equipped with a metric tensor g and an affine connection \u2207. We explain how this framework generalizes the Riemannian manifolds (M, g) by stating the fundamental theorem of Riemannian geometry that defines a unique torsion-free metriccompatible Levi-Civita connection from the metric tensor.\nIn the second part ( \u00a73), we explain the dualistic structures of information manifolds: We present the conjugate connection manifolds (M, g, \u2207, \u2207 * ), the statistical manifolds (M, g, C) where C is a cubic tensor, and show how to derive a family of information manifolds (M, g, \u2207 \u2212\u03b1 , \u2207 \u03b1 ) for \u03b1 \u2208 R provided any given pair (\u2207 = \u2207 \u22121 , \u2207 * = \u2207 1 ) of conjugate connections. We explain how to get conjugate connections from any smooth (potentially asymmetric) distances (called divergences), present the dually flat manifolds obtained when considering Bregman divergences, and define, when dealing with parametric family of probability models, the exponential connection e \u2207 and the mixture connection m \u2207 that are coupled to the Fisher information metric. We discuss the concept of statistical invariance for the metric tensor and the notion of information monotonicity for statistical divergences. It follows that the Fisher metric is the unique invariant metric (up to a scaling factor), and that the f -divergences are the unique separable invariant divergences.\nIn the third part ( \u00a74), we illustrate these information-geometric structures with two simple applications: In the first application, we consider Bayesian hypothesis testing and show how Chernoff information which defines the best error exponent, can be geometrically characterized on the dually flat structure of an exponential family manifold. In the second application, we show how to cluster statistical mixtures sharing the same component distributions on the dually flat mixture family Then we have t(\u03b8) = t( \u03b8), where the MLE is denoted by\u00b7.\n\nmanifold.\nFinally, we conclude in \u00a75 by summarizing the important concepts and structures of information geometry, and by providing further references and textbooks [12,4] to more advanced structures and applications for further readings. We mention recent studies of generic classes of distances/divergences.\nAt the beginning of each part, we outline its contents. A summary of notations is provided page 34.\n2 Prerequisite: Basics of differential geometry In \u00a72.1, we review the basics of Differential Geometry (DG) for defining a manifold (M, g, \u2207) equipped with both a metric tensor g and an affine connection \u2207. We explain these two independent metric/connection structures in \u00a72.2 and in \u00a72.3, respectively. From a connection \u2207, we show how to derive the notion of covariant derivative in \u00a72.3.1, parallel transport in \u00a72.3.2 and geodesics in \u00a72.3.3. We further explain the intrinsic curvature and torsion of manifolds induced by the connection in \u00a72. 3.4, and state the fundamental theorem of Riemannian geometry in \u00a72. 4: The existence of a unique torsion-free Levi-Civita metric connection LC \u2207 that can be calculated from the metric. Thus Riemannian geometry (M, g) is obtained as a special case of the more general manifold structure (M, g, LC \u2207): (M, g) \u2261 (M, g, LC \u2207). Information geometry shall further consider a dual structure (M, g, \u2207 * ) associated to (M, g, \u2207), and the pair of dual structures shall form an information manifold (M, g, \u2207, \u2207 * ).\n\nOverview of differential geometry\nInformally speaking, a smooth D-dimensional manifold M is a topological space that locally behaves like the Euclidean space R D . Geometric objects (e.g., points and vector fields) and entities (e.g., functions and differential operators) live on M , and are coordinate-free but can conveniently be expressed in any local coordinate 2 system of an atlas A = {(U i , x i )} i of charts (U i , x i )'s (fully covering the manifold) for calculations. A C k manifold is obtained when the change of chart transformations are C k . The manifold is said smooth when it is C \u221e . At each point p \u2208 M , a tangent plane T p locally best linearizes the manifold. On any smooth manifold M , we can define two independent structures: 1. a metric tensor g, and 2. an affine connection \u2207.\nThe metric tensor g induces on each tangent plane T p an inner product space that allows one to measure vector magnitudes (vector \"lengths\") and angles/orthogonality between vectors. The affine connection \u2207 is a differential operator that allows one to define: 1. the covariant derivative operator which provides a way to calculate differentials of a vector field Y with respect to another vector field X: Namely, the covariant derivative \u2207 X Y , 2. the parallel transport \u2207 c which defines a way to transport vectors on tangent planes along any smooth curve c, 3. the notion of \u2207-geodesics \u03b3 \u2207 which are defined as autoparallel curves, thus extending the ordinary notion of Euclidean straightness, 4. the intrinsic curvature and torsion of the manifold.\n\nMetric tensor fields g\nThe tangent bundle 3 of M is defined as the \"union\" of all tangent spaces: A tangent vector v plays the role of a directional derivative 4 , with vf informally meaning the derivative of a smooth function f (belonging to the space of smooth functions F(M )) along the direction v. A smooth vector field X is defined as a \"cross-section\" of the tangent bundle: X \u2208 X(M ) = \u0393(T M ), where X(M ) or \u0393(T M ) denote the space of smooth vector fields. A basis B = {b 1 , . . . , b D } of a finite D-dimensional vector space is a maximal linearly independent set of vectors. 5 Tangent spaces carry algebraic structures of vector spaces. 6 Using local coordinates on a chart (U, x), the vector field X can be expressed as X = D i=1 X i e i \u03a3 = X i e i using Einstein summation convention on dummy indices (using notation \u03a3 =), where (X) B :=(X i ) denotes the contravariant vector components (manipulated as \"column vectors\" in algebra) in the natural basis B = {e 1 = \u2202 1 , . . . , e D = \u2202 D } with \u2202 i :=: \u2202 \u2202x i . A tangent plane (vector space) equipped with an inner product \u00b7, \u00b7 yields an inner product space. We define a reciprocal basis so that vectors can also be expressed using the covariant vector components in the natural reciprocal basis. The primal and reciprocal basis are mutually orthogonal by construction as illustrated in Figure 2.\nFor any vector v, its contravariant components v i 's (superscript notation) and its covariant components v i 's (subscript notation) can be retrieved from v using the inner product with the use of the reciprocal and primal basis, respectively: The inner product defines a metric tensor g and a dual metric tensor g * : x 1 e 2 e i , e j = \u03b4 j i Figure 2: Primal and reciprocal basis of an inner product \u00b7, \u00b7 space. The primal/reciprocal basis are mutually orthogonal: e 1 is orthogonal to e 2 , and e 1 is orthogonal to e 2 .\nTechnically speaking, the metric tensor g is a 2-covariant tensor 7 field: where \u2297 is the dyadic tensor product performed on pairwise covector basis {dx i } i (the covectors corresponding to the reciprocal vector basis). Let G = [g ij ] and G * = [g * ij ] denote the D \u00d7 D matrices It follows by construction of the reciprocal basis that G * = G \u22121 . The reciprocal basis vectors e * i 's and primal basis vectors e i 's can be expressed using the dual metric g * and metric g on the primal basis vectors e j 's and reciprocal basis vectors e * j 's, respectively: The metric tensor field g (\"metric tensor\" or \"metric\" for short) defines a smooth symmetric positive-definite bilinear form on the tangent bundle so that for u, v \u2208 T p , g(u, v) \u2265 0 \u2208 R. We can also write equivalently g p (u, v):=: u, v p :=: u, v g(p) :=: u, v . Two vectors u and v are said orthogonal, denoted by u \u22a5 v, iff u, v = 0. The length of a vector is induced from the norm u p :=: u g(p) = u, u g(p) . Using local coordinates of a chart (U, x), we get the vector contravariant/covariant components, and compute the metric tensor using matrix algebra (with column vectors by convention) as follows: since it follows from the primal/reciprocal basis that G \u00d7 G * = I, the identity matrix. Thus on any tangent plane T p , we get a Mahalanobis distance: The inner product of two vectors u and v is a scalar (a 0-tensor) that can be equivalently calculated as: A metric tensor g of manifold M is said conformal when \u00b7, \u00b7 p = \u03ba(p) \u00b7, \u00b7 Euclidean . That is, when the inner product is a scalar function \u03ba(\u00b7) of the Euclidean dot product. In conformal geometry, we can measure angles between vectors in tangent planes as if we were in an Euclidean space, without any deformation. This is handy for checking orthogonality (in charts). For example, Poincar\u00e9 disk model of hyperbolic geometry is conformal but Klein disk model is not conformal (except at the origin), see [44].\n\nAffine connections \u2207\nAn affine connection \u2207 is a differential operator defined on a manifold that allows us to define a covariant derivative of vector fields, a parallel transport of vectors on tangent planes along a smooth curve, and geodesics. Furthermore, an affine connection fully characterizes the curvature and torsion of a manifold.\n\nCovariant derivatives \u2207 X Y of vector fields\nA connection defines a covariant derivative operator that tells us how to differentiate a vector field Y according to another vector field X. The covariant derivative operator is denoted using the traditional gradient symbol \u2207. Thus a covariate derivative \u2207 is a function: that has its own special subscript notation \u2207 X Y :=:\u2207(X, Y ) for indicating that it is differentiating a vector field Y according to another vector field X. By prescribing D 3 smooth functions \u0393 k ij = \u0393 k ij (p), called the Christoffel symbols of the second kind, we define the unique affine connection \u2207 that satisfies in local coordinates of chart (U, x) the following equations: The Christoffel symbols can also be written as \u0393 k ij := (\u2207 \u2202 i \u2202 j ) k , where (\u00b7) k denote the k-th coordinate. The k-th component (\u2207 X Y ) k of the covariant derivative of vector field Y with respect to vector field X is given by: The Christoffel symbols are not tensors (fields) because the transformation rules induced by a change of basis do not obey the tensor contravariant/covariant rules.\n\nParallel transport \u2207\nc along a smooth curve c Since the manifold is not embedded 8 in a Euclidean space, we cannot add a vector v \u2208 T p to a vector v \u2208 T p as the tangent vector spaces are unrelated to each others without a connection. 9 Figure 3: Illustration of the parallel transport of vectors on tangent planes along a smooth curve. For a smooth curve c, with c(0) = p and c(1) = q, a vector v p \u2208 T p is parallel transported smoothly to a vector v q \u2208 T q such that for any t \u2208 [0, 1], we have v c(t) \u2208 T c(t) .\nThus a connection \u2207 defines how to associate vectors between infinitesimally close tangent planes T p and T p+dp . Then the connection allows us to smoothly transport a vector v \u2208 T p by sliding it (with infinitesimal moves) along a smooth curve c(t) (with c(0) = p and c(1) = q), so that the vector v p \u2208 T p \"corresponds\" to a vector v q \u2208 T q : This is called the parallel transport. This mathematical prescription is necessary in order to study dynamics on manifolds (e.g., study the motion of a particle on the manifold). We can express the parallel transport along the smooth curve c as: The parallel transport is schematically illustrated in Figure 3.\n\n\u2207-geodesics \u03b3 \u2207 : Autoparallel curves\nA connection \u2207 allows one to define \u2207-geodesics as autoparallel curves, that are curves \u03b3 such that we have: That is, the velocity vector\u03b3 is moving along the curve parallel to itself: In other words, \u2207geodesics generalize the notion of \"straight Euclidean\" lines. In local coordinates (U, x), \u03b3(t) = (\u03b3 k (t)) k , the autoparallelism amounts to solve the following second-order Ordinary Differential Equations (ODEs):\u03b3 where \u0393 k ij are the Christoffel symbols of the second kind, with: where \u0393 ij,l the Christoffel symbols of the first kind. Geodesics are 1D autoparallel submanifolds and \u2207-hyperplanes are defined similarly as autoparallel submanifolds of dimension D \u2212 1. We may specify in subscript the connection that yields the geodesic \u03b3: \u03b3 \u2207 .\n\nCurvature and torsion of a manifold\nAn affine connection \u2207 defines a 4D 10 Riemann-Christoffel curvature tensor R (expressed using components R i jkl of a (1, 3)-tensor). The coordinate-free equation of the curvature tensor is given by: where is the Lie bracket of vector fields. A manifold M equipped with a connection \u2207 is said flat (meaning \u2207-flat) when R = 0. This holds in particular when finding a particular 11 coordinate system x of a chart (U, x) such that \u0393 k ij = 0, i.e., when all connection coefficients vanish. A manifold is torsion-free when the connection is symmetric. A symmetric connection satisfies the following coordinate-free equation: Using local chart coordinates, this amounts to check that \u0393 k ij = \u0393 k ji . The torsion tensor is a (1, 2)-tensor defined by: In general, the parallel transport is path-dependent. The angle defect of a vector transported on an infinitesimal closed loop (a smooth curve with coinciding extremities) is related to the curvature. However for a flat connection, the parallel transport does not depend on the path. Figure 4 illustrates the parallel transport along a curve for a curved manifold (the sphere manifold) and a flat manifold ( the cylinder manifold 12 ). 10 It follows from symmetry constraints that the number of independent components of the Riemann tensor is in D dimensions. 11 For example, the Christoffel symbols vanish in a rectangular coordinate system of a plane but not in the polar coordinate system of it. 12 The Gaussian curvature at of point of a manifold is the product of the minimal and maximal sectional curvatures: \u03baG:=\u03bamin\u03bamax . For a cylinder, since \u03bamin = 0, it follows that the Gaussian curvature of a cylinder is 0. Gauss's Theorema Egregium (meaning \"remarkable theorem\") proved that the Gaussian curvature is intrinsic and does not depend on how the surface is embedded into the ambient Euclidean space.\n\nThe fundamental theorem of Riemannian geometry: The Levi-Civita metric connection\nBy definition, an affine connection \u2207 is said metric compatible with g when it satisfies for any triple (X, Y, Z) of vector fields the following equation: which can be written equivalently as: Using local coordinates and natural basis {\u2202 i } for vector fields, the metric-compatibility property amounts to check that we have: A property of using a metric-compatible connection is that the parallel transport \u2207 of vectors preserve the metric: That is, the parallel transport preserves angles (and orthogonality) and lengths of vectors in tangent planes when transported along a smooth curve. The fundamental theorem of Riemannian geometry states the existence of a unique torsion-free metric compatible connection: Theorem 1 (Levi-Civita metric connection) There exists a unique torsion-free affine connection compatible with the metric called the Levi-Civita connection LC \u2207.\nThe Christoffel symbols of the Levi-Civita connection can be expressed from the metric tensor g as follows: where g ij denote the matrix elements of the inverse matrix g \u22121 .\nThe Levi-Civita connection can also be defined coordinate-free with the Koszul formula: There exists metric-compatible connections with torsions studied in theoretical physics. See for example the flat Weitzenb\u00f6ck connection [9].\nThe metric tensor g induces the torsion-free metric-compatible Levi-Civita connection that determines the local structure of the manifold. However, the metric g does not fix the global topological structure: For example, although a cone and a cylinder have locally the same flat Euclidean metric, they exhibit different global structures.\n\nPreview: Information geometry versus Riemannian geometry\nIn information geometry, we consider a pair of conjugate affine connections \u2207 and \u2207 * (often but not necessarily torsion-free) that are coupled to the metric g: The structure is conventionally written as (M, g, \u2207, \u2207 * ). The key property is that those conjugate connections are metric compatible, and therefore the induced dual parallel transport preserves the metric: Thus the Riemannian manifold (M, g) can be interpreted as the self-dual information-geometric manifold obtained for \u2207 = \u2207 * = LC \u2207 the unique torsion-free Levi-Civita metric connection: However, let us point out that for a pair of self-dual Levi-Civita conjugate connections, the information-geometric manifold does not induce a distance. This contrasts with the Riemannian modeling (M, g) which provides a Riemmanian metric distance D \u03c1 (p, q) defined by the length of the geodesic \u03b3 connecting the two points p = \u03b3(0) and q = \u03b3(1) (shortest path): Usually, this Riemannian geodesic distance is not available in closed-form (and need to be approximated or bounded) because the geodesics cannot be explicitly parameterized (see geodesic shooting methods [7]).\nWe are now ready to introduce the key geometric structures of information geometry.\n\nInformation manifolds 3.1 Overview\nIn this part, we explain the dualistic structures of manifolds in information geometry. In \u00a73.2, we first present the core Conjugate Connection Manifolds (CCMs) (M, g, \u2207, \u2207 * ), and show how to build Statistical Manifolds (SMs) (M, g, C) from a CCM in \u00a73.3. From any statistical manifold, we can build a 1-parameter family (M, g, \u2207 \u2212\u03b1 , \u2207 \u03b1 ) of CCMs, the information \u03b1-manifolds. We state the fundamental theorem of information geometry in \u00a73.5. These CCMs and SMs structures are not related to any distance a priori but require at first a pair (\u2207, \u2207 * ) of conjugate connections coupled to a metric tensor g. We show two methods to build an initial pair of conjugate connections. A first method consists in building a pair of conjugate connections ( D \u2207, D \u2207 * ) from any divergence D in \u00a73.6. Thus we obtain self-conjugate connections when the divergence is symmetric: D(\u03b8 1 : \u03b8 2 ) = D(\u03b8 2 : \u03b8 1 ). When the divergences are Bregman divergences (i.e., D = B F for a strictly convex and differentiable Bregman generator), we obtain Dually Flat Manifolds (DFMs) (M, \u2207 2 F, F \u2207, F \u2207 * ) in \u00a73.7. DFMs nicely generalize the Euclidean geometry and exhibit Pythagorean theorems. We further characterize when orthogonal F \u2207-projections and dual F \u2207 * -projections of a point on submanifold a is unique. 13 A second method to get a pair of conjugate connections ( e \u2207, m \u2207) consists in defining these connections from a regular parametric family of probability distributions P = {p \u03b8 (x)} \u03b8 . In that case, these 'e'xponential connection e \u2207 and 'm'ixture connection m \u2207 are coupled to the Fisher information metric P g. A statistical manifold (P, P g, P C) can be recovered by considering the skewness Amari-Chentsov cubic tensor P C, and it follows a 1-parameter family of CCMs, (P, P g, P \u2207 \u2212\u03b1 , P \u2207 +\u03b1 ), the statistical expected \u03b1-manifolds.\nIn this parametric statistical context, these information manifolds are called expected information manifolds because the various quantities are expressed from statistical expectations E \u00b7 [\u00b7]. Notice that these information manifolds can be used in information sciences in general, beyond the traditional fields of statistics. In statistics, we motivate the choice of the connections, metric tensors and divergences by studying statistical invariance criteria, in \u00a73.9. We explain how to recover the expected \u03b1-connections from standard f -divergences that are the only separable divergences that satisfy the property of information monotonicity. Finally, in \u00a73.10, the recall the Fisher-Rao expected Riemannian manifolds that are Riemannian manifolds (P, P g) equipped with a geodesic metric distance called the Fisher-Rao distance, or Rao distance for short.\n\nConjugate connection manifolds:\nWe begin with a definition: Definition 1 (Conjugate connections) A connection \u2207 * is said to be conjugate to a connection \u2207 with respect to the metric tensor g if and only if we have for any triple (X, Y, Z) of smooth vector fields the following identity satisfied: We can notationally rewrite Eq. 31 as: and further explicit that for each point p \u2208 M , we have: We check that the right-hand-side is a scalar and that the left-hand-side is a directional derivative of a real-valued function, that is also a scalar. Conjugation is an involution: (\u2207 * ) * = \u2207. A remarkable property is that the dual parallel transport of vectors preserves the metric. That is, for any smooth curve c(t), the inner product is conserved when we transport one of the vector u using the primal parallel transport \u2207 c and the other vector v using the dual parallel transport Property 1 (Dual parallel transport preserves the metric) A pair (\u2207, \u2207 * ) of conjugate connections preserves the metric g if and only if: Property 2 Given a connection \u2207 on (M, g) (i.e., a structure (M, g, \u2207)), there exists a unique conjugate connection \u2207 * (i.e., a dual structure (M, g, \u2207 * )).\nWe consider a manifold M equipped with a pair of conjugate connections \u2207 and \u2207 * that are coupled with the metric tensor g so that the dual parallel transport preserves the metric. We define the mean connection\u2207: with corresponding Christoffel coefficients denoted by\u0393. This mean connection coincides with the Levi-Civita metric connection:\u2207 = LC \u2207.\nProperty 3 The mean connection\u2207 is self-conjugate, and coincide with the Levi-Civita metric connection.\n\nStatistical manifolds: (M, g, C)\nLauritzen introduced this corner structure [30] of information geometry in 1987. Beware that although it bears the name \"statistical manifold,\" it is a purely geometric construction that may be used outside of the field of Statistics. However, as we shall mention later, we can always find a statistical model P corresponding to a statistical manifold [69]. We shall see how we can convert a conjugate connection manifold into such a statistical manifold, and how we can subsequently derive an infinite family of CCMs from a statistical manifold. In other words, once we have a pair of conjugate connections, we will be able to build a family of pairs of conjugate connections. We define a totally symmetric 14 cubic (0, 3)-tensor (i.e., 3-covariant tensor) called the Amari-Chentsov tensor: or in coordinate-free equation: Using the local basis, this cubic tensor can be expressed as: Definition 3 (Statistical manifold [30]) A statistical manifold (M, g, C) is a manifold M equipped with a metric tensor g and a totally symmetric cubic tensor C.\n\nconjugate connection manifolds\nFor any pair (\u2207, \u2207 * ) of conjugate connections, we can define a 1-parameter family of connections {\u2207 \u03b1 } \u03b1\u2208R , called the \u03b1-connections such that (\u2207 \u2212\u03b1 , \u2207 \u03b1 ) are dually coupled to the metric, with \u2207 0 =\u2207 = LC \u2207, \u2207 1 = \u2207 and \u2207 \u22121 = \u2207 * . By observing that the scaled cubic tensor \u03b1C is also a totally symmetric cubic 3-covariant tensor, we can derive the \u03b1-connections from a statistical manifold (M, g, C) as: where \u0393 0 ij,k are the Levi-Civita Christoffel symbols, and \u0393 ki,j \u03a3 = \u0393 l ij g lk (by index juggling). The \u03b1-connection \u2207 \u03b1 can also be defined as follows: Theorem 2 (Family of information \u03b1-manifolds) For any \u03b1 \u2208 R, (M, g, is a conjugate connection manifold. The \u03b1-connections \u2207 \u03b1 can also be constructed directly from a pair (\u2207, \u2207 * ) of conjugate connections by taking the following weighted combination: 3.5 The fundamental theorem of information geometry: \u2207 \u03ba-curved \u21d4 \u2207 * \u03bacurved We now state the fundamental theorem of information geometry and its corollaries: Theorem 3 (Dually constant curvature manifolds) If a torsion-free affine connection \u2207 has constant curvature \u03ba then its conjugate torsion-free connection \u2207 * has necessarily the same constant curvature \u03ba.\nThe proof is reported in [12] (Proposition 8.1.4, page 226). We get the following two corollaries: Thus once we are given a pair of conjugate connections, we can always build a 1-parametric family of manifolds. Manifolds with constant curvature \u03ba are interesting from the computational viewpoint as dual geodesics have simple closed-form expressions.\n\nConjugate connections from divergences:\nLoosely speaking, a divergence D(\u00b7 : \u00b7) is a smooth distance [74], potentially asymmetric. In order to define precisely a divergence, let us first introduce the following handy notations: on a manifold M with respect to a local chart \u0398 \u2282 R D is a C 3 -function satisfying the following properties: The dual divergence is defined by swapping the arguments: and is also called the reverse divergence (reference duality in information geometry). Reference duality of divergences is an involution: The Euclidean distance is a metric distance but not a divergence. The squared Euclidean distance is a non-metric symmetric divergence. The metric tensor g yields Riemannian metric distance D \u03c1 but it is never a divergence.\nFrom any given divergence D, we can define a conjugate connection manifold following the construction of Eguchi [20] (1983): Theorem 4 (Manifold from divergence) (M, D g, D \u2207, D * \u2207) is an information manifold with: The associated statistical manifold is (M, D g, D C) with: Since \u03b1 D C is a totally symmetric cubic tensor for any \u03b1 \u2208 R, we can derive a one-parameter family of conjugate connection manifolds: In the remainder, we use the shortcut (M, D) to denote the divergence-induced information manifold (M, D g, D \u2207, D \u2207 * ). Notice that it follows from construction that: 3.7 Dually flat manifolds (Bregman geometry): We consider dually flat manifolds that satisfy asymmetric Pythagorean theorems. These flat manifolds can be obtained from a canonical Bregman divergence. Consider a strictly convex smooth function F (\u03b8) called a potential function, with \u03b8 \u2208 \u0398 where \u0398 is an open convex domain. Notice that the function convexity does not change by an affine transformation. We associate to the potential function F a corresponding Bregman divergence (parameter divergence): We write also the Bregman divergence between point P and point Q as D(P : Q):=B F (\u03b8(P ) : \u03b8(Q)), where \u03b8(P ) denotes the coordinates of a point P .\nThe induced information-geometric structure is (M, F g, F C):=(M, B F g, B F C) with: Since all coefficients of the Christoffel symbols vanish (Eq. 54), the information manifold is F \u2207-flat. The Levi-Civita connection LC \u2207 is obtained from the metric tensor F g (usually not flat), and we get the conjugate connection ( F \u2207) * = F \u2207 1 from (M, F g, F C).\nWe have the Crouzeix [15] identity relating the Hessians of the potential functions: where I denote the D \u00d7 D identity matrix. This Crouzeix identity reveals that B = {\u2202 i } i and B * = {\u2202 j } j are the primal and reciprocal basis, respectively. The Bregman divergence can be reinterpreted using Young-Fenchel (in)equality as the canonical divergence A F,F * [8]:  The In a dually flat manifold, any pair of points P and Q can either be linked using the \u2207-geodesic (that is \u03b8-straight) or the \u2207 * -geodesic (that is \u03b7-straight). In general, there are 2 3 = 8 types of geodesic triangles in a dually flat manifold.\nTheorem 6 (Dual Pythagorean identities) We can define dual Bregman projections and characterize when these projections are unique: A submanifold S \u2282 M is said \u2207-flat (\u2207 * -flat) iff. it corresponds to an affine subspace in the \u03b8-coordinate system (in the \u03b7-coordinate system, respectively).\nWhen S is a \u2207-flat submanifold and S \u2207 * -flat submanifold, the divergence D(S : S ) between submanifold S and submanifold S can be calculated using the method of alternating projections [4].\nLet us remark that Kurose [29] reported a Pythagorean theorem for dually constant curvature manifolds that generalizes the Pythagorean theorems of dually flat spaces.\nThe dually flat geometry can be investigated under the wider scope of Hessian manifolds [63] which consider locally potential functions.\nWe now consider information manifolds induced by parametric statistical models.\n\nExpected \u03b1-manifolds of a family of parametric probability distributions:\n(P, P g, P \u2207 \u2212\u03b1 , P \u2207 \u03b1 ) Informally speaking, an expected manifold is an information manifold built on a regular parametric family of distributions. It is sometimes called \"expected\" manifold or \"expected\" geometry in the literature [76] because the components of the metric tensor g and the Amari-Chentsov cubic tensor C are expressed using statistical expectations E \u00b7 [\u00b7]. Let P be a parametric family of probability distributions: with \u03b8 belonging to the open parameter space \u0398. The order of the family is the dimension of its parameter space. We define the likelihood function 16 L(\u03b8; x):=p \u03b8 (x) as a function of \u03b8, and its corresponding log-likelihood function: The score vector: indicates the sensitivity of the likelihood \u2202 i l:=: \u2202 \u2202\u03b8 i l(\u03b8; x). The Fisher information matrix (FIM) of D \u00d7 D for dim(\u0398) = D is defined by: 16 The likelihood function is an equivalence class of functions defined modulo a positive scaling factor.\nwhere denotes the L\u00f6wner order. That is, for two symmetric positive-definite matrices A and B, A B if and only if matrix A \u2212 B is positive semidefinite. For regular models [12], the FIM is positive definite: P I(\u03b8) 0, where A B if and only if matrix A \u2212 B is positive-definite.\nIn statistics, the FIM plays a role in the attainable precision of unbiased estimators. For any unbiased estimator, the Cram\u00e9r-Rao lower bound [33] on the variance of the estimator is: The FIM is invariant by reparameterization of the sample space X , and covariant by reparameterization of the parameter space \u0398, see [12].\nWe report the expression of the FIM for two important generic parametric family of probability distributions: (1) an exponential family, and (2) a mixture family.\nExample 1 (FIM of an exponential family E) An exponential family [41] E is defined for a sufficient statistic vector t(x) = (t 1 (x), . . . , t D (x)), and an auxiliary carrier measure k(x) by the following canonical density: where F is the strictly convex cumulant function. Exponential families include the Gaussian family, the Gamma and Beta families, the probability simplex \u2206, etc. The FIM of an exponential family is given by: Example 2 (FIM of a mixture family M) A mixture family is defined for D + 1 functions F 1 , . . . , F D and C as: where the functions {F i (x)} i are linearly independent on the common support X and satisfying F i (x)d\u00b5(x) = 0. Function C is such that C(x)d\u00b5(x) = 1. Mixture families include statistical mixtures with prescribed component distributions and the probability simplex \u2206. The FIM of a mixture family is given by: Notice that the probability simplex of discrete distributions can be both modeled as an exponential family or a mixture family [4]. The expected \u03b1-geometry is built from the expected dual \u00b1\u03b1-connections. The Fisher \"information metric\" tensor is built from the FIM as follows: The expected exponential connection and expected mixture connection are given by The dualistic structure is denoted by (P, P g, m P \u2207, e P \u2207) with Amari-Chentsov cubic tensor called the skewness tensor: It follows that we can build a one-family of expected information \u03b1-manifolds: with The Levi-Civita metric connection is recovered as follows: In case of an exponential family E or a mixture family M equipped with the dual exponential/mixture connection, we get dually flat manifolds (Bregman geometry).\nIndeed, for the exponential/mixture families, it is easy to check that the Christoffel symbols of \u2207 e and \u2207 m vanish:\n\nCriteria for statistical invariance\nSo far we have explained how to build an information manifold (or information \u03b1-manifold) from a pair of conjugate connections. Then we reported two ways to obtain such a pair of conjugate connections: (1) from a parametric divergence, or (2) by using the predefined expected exponential/mixture connections. We now ask the following question: Which information manifold makes sense in Statistics? We can refine the question as follows: \u2022 Which metric tensors g make sense in statistics?\n\u2022 Which affine connections \u2207 make sense in statistics?\n\u2022 Which statistical divergences make sense in statistics (from which we can get the metric tensor and dual connections)?\nBy definition, an invariant metric tensor g shall preserve the inner product under important statistical mappings called Markov embeddings. Informally, we embed \u2206 D into \u2206 D with D > D and the induced metric should be preserved (see [4], page 62).  Here, parameter \u03b8 represents a discrete distribution.\nA D-dimensional parameter (discrete) divergence satisfies the information monotonicity if and only if: for any coarse-grained partition . This concept of coarse-graining is illustrated in Figure 6.\nA separable divergence D(\u03b8 1 : \u03b8 2 ) is a divergence that can be expressed as the sum of elementary scalar divergences d(x : y): For example, the squared Euclidean distance D(\u03b8 1 : is not separable because of the square root operation. The only invariant and decomposable divergences when D > 1 are f -divergences [27] defined for a convex functional generator f : The standard f -divergences are defined for f -generators satisfying f (1) = 0 (choose f \u03bb (u):=f (u) + \u03bb(u \u2212 1) since I f \u03bb = I f ), and f (u) = 1 (scale fixed).\nStatistical f -divergences are invariant [58] under one-to-one/sufficient statistic transformations y = t(x) of sample space: p(x; \u03b8) = q(y(x); \u03b8): The dual f -divergences for reference duality is for the standard conjugate f -generator (diamond f generator) with:\n\n21\nOne can check that f is a standard f -generator when f is standard. Let us report some common examples of f -divergences: \u2022 The family of \u03b1-divergences: 2 ). The \u03b1-divergences include: the Kullback-Leibler when \u03b1 \u2192 1: for f (u) = \u2212 log u.\nfor f (u) = 1 2 |u \u2212 1|. The total variation distance is the only metric f -divergence.\nA remarkable property is that invariant standard f -divergences yield the Fisher information matrix and the \u03b1-connections. Indeed, the invariant standard f -divergences is related infinitesimally to the Fisher metric as follows:\n\n22\nA statistical parameter divergence D on a parametric family of distributions P yields an equivalent parameter divergence P D: (94) Thus we can build the information manifold induced by this parameter divergence P D(\u00b7 : \u00b7). For P D(\u00b7 : \u00b7) = I f [\u00b7 : \u00b7], the induced \u00b11-divergence connections 3.10 Fisher-Rao expected Riemannian manifolds: (P, P g) Historically, a first manifold modeling of a regular parametric family of distributions P = {p \u03b8 (x)} \u03b8 was to consider the Fisher Information Matrix (FIM) as the Riemannian metric tensor g (see [25,60]), with: where \u2202 i l:=: \u2202 \u2202\u03b8 i log p(x; \u03b8). Under some regularity conditions, we can rewrite the FIM: The Riemannian geodesic metric distance D \u03c1 is commonly called the Fisher-Rao distance: where \u03b3 denotes the geodesic passing through \u03b3(0) = \u03b8 1 and \u03b3(1) = \u03b8 2 .\nDefinition 5 (Fisher-Rao distance) The Fisher-Rao distance is the geodesic metric distance of the Fisher-Riemannian manifold (P, P g).\n\nLet us give some examples of Fisher-Riemannian manifolds:\n\u2022 The Fisher-Riemannian manifold of the family of categorical distributions (also called finite discrete distributions in [4]) amount to the spherical geometry [28] (spherical manifold).\n\u2022 The Fisher-Riemannian manifold of the family of bivariate location-scale families amount to hyperbolic geometry (hyperbolic manifold).\n\u2022 The Fisher-Riemannian manifold of the family of location families amount to Euclidean geometry (Euclidean manifold).\nThe first fundamental form of the Riemannian geometry is ds 2 = dx, dx \u03a3 = g ij dx i dx j where ds denotes the line element.\nThis Riemannian geometric structure applied to a family of parametric probability distributions was first proposed by Harold Hotelling [25] (in a handwritten note of 1929, reprinted typeset in [65]) and independently later by C. R. Rao [60] (1945, reprinted in [59]). In a similar vein, Jeffreys [26] proposed to use the volume element of a manifold as an invariant prior: The eponym Jeffreys prior in 1946.\nNotice that for a parametric family of probability distributions P, the Riemannian structure (P, P g) coincides with the self-dual conjugate connection manifold (P, P g, induced by a symmetric f -divergence like the squared Hellinger divergence.\n\nThe monotone \u03b1-embeddings\nAnother common mathematically equivalent expression of the FIM [12] is given by: This form of the FIM is well-suited to prove that the FIM is always a positive semi-definite matrix [12] (I(\u03b8) 0). It turns out that we can define a family of equivalent representations of the FIM using the \u03b1-embedding [75] of the parametric family. First, we define the \u03b1-representation of densities l \u03b1 (x; \u03b8) := k \u03b1 (p(x; \u03b8)) with: The function l \u03b1 (x; \u03b8) is called the \u03b1-likelihood function. Then the \u03b1-representation of the FIM, the \u03b1-FIM for short, is expressed as: We can rewrite compactly the \u03b1-FIM, as I \u03b1 ij (\u03b8) = \u2202 i l \u03b1 \u2202 j l \u2212\u03b1 d\u00b5(x). Expanding the \u03b1-FIM, we get: The 1-representation of the FIM is called the logarithmic representation and its 0-representation is called the square root representation. The set of \u03b1-scores vectors B \u03b1 :={\u2202 i l \u03b1 } i are interpreted as the tangent basis vectors of the \u03b1-base B \u03b1 . Thus the FIM is \u03b1-independent.\nFurthermore, the \u03b1-representation of the FIM can be rewritten under mild conditions [12] as: Since we have: it follows that: Notice that when \u03b1 = 1, we recover the equivalent expression of the FIM (under mild conditions): In particular, when the family is an exponential family [41] with cumulant function F (\u03b8) (satisfying the mild conditions), we have: The \u03b1-embeddings can be generalized by considering a pair of strictly increasing real-valued functions 17 \u03c1 and \u03c4 (the conjugate embeddings) to yield the (\u03c1, \u03c4 )-geometry [75,51]. Zhang [75] further discussed the representation/reference biduality which was confounded in the \u03b1-geometry. Figure 7 displays the main types of information manifolds encountered in information geometry with their relationships.\n\nSome illustrating applications of dually flat manifolds\nInformation geometry [4] found broad applications in information sciences. For example, we can mention: \u2022 Statistics: Asymptotic inference, Expectation-Maximization (EM and the novel informationgeometric em), time series (AutoRegressive Moving Average model, ARMA) models, \u2022 Machine learning: Restricted Boltzmann machines (RBMs), neuromanifolds and natural gradient [66], \u2022 Signal processing: Principal Component Analysis (PCA), Independent Component Analysis (ICA), Non-negative Matrix Factorization (NMF), \u2022 Mathematical programming: Barrier function of interior point methods, \u2022 Game theory: Score functions.\nIn this part, we describe how to use the dually flat structures for handling an exponential family E (in a hypothesis testing problem detailed in \u00a74.1) and the mixture family M (clustering statistical mixtures \u00a74.2). Note that for a general divergence, neither (E, D) nor (M, D) is dually flat. However, when D = KL, the Kullback-Leibler divergence, we get dually flat spaces that are computationally attractive since the primal/dual geodesics are straight lines in the corresponding global affine coordinate system. Etc.\n\nFrank Nielsen\nCubic skewness tensor canonical divergence Figure 7: Overview of the main types of information manifolds with their relationships in information geometry.\nx 1 x Figure 8: Statistical Bayesian hypothesis testing: The best Maximum A Posteriori (MAP) rule chooses to classify an observation from the class that yields the maximum likelihood.\n\nHypothesis testing in the dually flat exponential family manifold (E, KL * )\nGiven two probability distributions P 0 \u223c p 0 (x) and P 1 \u223c p 1 (x), we ask to classify a set of iid. observations X 1:n = {x 1 , . . . , x n } as either sampled from P 0 or from P 1 ? This is a statistical decision problem [35]. For example, P 0 can represent the signal distribution and P 1 the noise distribution. Figure 8 displays the probability distributions and the unavoidable error that is made by any statistical decision rule (on observations x 1 and x 2 ). Assume that both distributions P 0 \u223c P \u03b8 0 and P 1 \u223c P \u03b8 1 belong to the same exponential family E = {P \u03b8 : \u03b8 \u2208 \u0398}, and consider the exponential family manifold with the dually flat structure (E, E g, E \u2207 e , E \u2207 m ). That is, the manifold equipped with the Fisher information metric tensor field and the expected exponential connection and conjugate expected mixture connection. This structure can also be derived from a divergence manifold structure by choosing the reverse Kullback-Leibler divergence KL * : Therefore, the Kullback-Leibler divergence KL[P \u03b8 : P \u03b8 ] amounts to a Bregman divergence (for the cumulant function of the exponential family): The best exponent error \u03b1 * of the best Maximum A Priori (MAP) decision rule is found by minimizing the Bhattacharyya distance to get the Chernoff information [56]: On the exponential family manifold E, the Bhattacharyya distance: amounts to a skew Jensen parameter divergence [40] (also called Burbea-Rao divergence): It can be shown that the Chernoff information (that minimizes \u03b1) is equivalent to a Bregman divergence: Namely, the Bregman divergence for exponential families at the optimal exponent value \u03b1 * . Theorem 9 (Chernoff information [35]) The Chernoff information between two distributions belonging to the same exponential family amount to a Bregman divergence: where \u03b8 \u03b1 12 = (1 \u2212 \u03b1)\u03b8 1 + \u03b1\u03b8 2 , and \u03b1 * denote the best exponent error.\nLet \u03b8 * 12 :=\u03b8 \u03b1 * 12 denote the best exponent error. The geometry [35] of the best error exponent can be explained on the dually flat exponential family manifold as follows: where G e denotes the exponential geodesic \u03b3 \u2207 e and Bi m the m-bisector: Figure 9 illustrates how to retrieve the best error exponent from an exponential arc (\u03b8-geodesic) intersecting the m-bisector.\nFurthermore, instead of considering two distributions for this statistical binary decision problem, we may consider a set of n distributions of P 1 , . . . , P n \u2208 E. The geometry of the error exponent in this multiple hypothesis testing setting has been investigated in [34]. On the dually flat exponential family manifold, it corresponds to check the exponential arcs between natural neighbors (sharing Voronoi subfaces) of a Bregman Voronoi diagram [11]. See Figure 10 for an illustration.\n\nClustering mixtures in the dually flat mixture family manifold (M, KL)\nGiven a set of k prescribed statistical distributions p 0 (x), . . . , p k\u22121 (x), all sharing the same support X (say, R), a mixture family M of order D = k \u2212 1 consists of all strictly convex combinations of these component distributions [48]:\n\n\u03b7-coordinate system\nChernoff distribution between natural neighbours  We consider the expected information manifold (M, M g, M \u2207 m , M \u2207 e ) which is dually flat and equivalent to (M \u0398 , KL). That is, the KL between two mixtures with prescribed components (wmixtures, for short) is equivalent to a Bregman divergence for is the differential Shannon information (negative entropy) [48]: Consider a set {m \u03b8 1 , . . . , m \u03b8n } of n w-mixtures [48]. Because F (\u03b8) = \u2212h(m(x; \u03b8)) is the negative differential entropy of a mixture (not available in closed form [49]), we approximate the untractable F by another close tractable generatorF . We use Monte Carlo stochastic sampling to get Monte-Carlo convexF S for an independent and identically distributed sample S.\nThus we can build a nested sequence (M,F S 1 ), . . . , (M,F Sm ) of tractable dually flat manifolds for nested sample sets S 1 \u2282 . . . \u2282 S m converging to the ideal mixture manifold (M, F ): lim m\u2192\u221e (M,F Sm ) = (M, F ) (where convergence is defined with respect to the induced canonical Bregman divergence). A key advantage of this approach is that for a given sample S, all computations carried inside the dually flat manifold (M,F S ) are consistent, see [48].\nFor example, we can apply Bregman k-means [43] on these Monte Carlo dually flat spaces [42] of w-GMMs (Gaussian Mixture Models) to cluster a set of w-GMMs. Figure 12 displays the result of such a clustering.\nWe have briefly described two applications using dually flat manifolds: (1) the dually flat exponential manifold induced by the statistical reverse Kullback-Leibler divergence on an exponential family (structure (E, KL * )), and (2) the dually flat mixture manifold induced by the statistical Kullback-Leibler divergence on a mixture family (structure (M, KL)). There are many other dually flat structures that can be met in a statistical context: For example, two other dually flat structures for the D-dimensional probability simplex \u2206 D are reported in Amari's textbook [4]: (1) the conformally deforming of the \u03b1-geometry (page 88, Eq. 4.95 of [4]), and (2) the \u03c7-escort geometry (page 91, Eq. 4.114 of [4]).\n5 Conclusion: Summary, historical background, and perspectives\n\nSummary\nWe explained the dualistic nature of information manifolds (M, g, \u2207, \u2207 * ) in information geometry. The dualistic structure is defined by a pair of conjugate connections coupled with the metric tensor that provides a dual parallel transport that preserves the metric. We showed how to extend this structure to a 1-parameter family of structures. From a pair of conjugate connections, the pipeline can be informally summarized as: We stated the fundamental theorem of information geometry on dual constant-curvature manifolds, including the special but important case of dually flat manifolds on which there exists two potential functions and global affine coordinate systems related by the Legendre-Fenchel transformation. Although, information geometry historically started with the Riemannian modeling (P, P g) of a parametric family of probability distributions P by letting the metric tensor be the Fisher information matrix, we have emphasized the dualistic view of information geometry which considers non-Riemannian manifolds that can be derived from any divergence, and not necessarily tied to a statistical context (e.g., information manifold can be used in mathematical programming [54]). Let us notice that for any symmetric divergence (e.g. any symmetrized f -divergence like the squared Hellinger divergence), the induced conjugate connections coincide with the Levi-Civita connection but the Fisher-Rao metric distance does not coincide with the squared Hellinger divergence. On one hand, a Riemannian metric distance D \u03c1 is never a divergence because the rooted distance functions fail to be smooth at the extremities but a squared Riemmanian metric distance is always a divergence. On the other hand, taking the power \u03b4 of a divergence D (i.e., D \u03b4 ) for some \u03b4 > 0 may yield a metric distance (e.g., the square root of the Jensen-Shannon divergence [21]), but this may not always be the case: The powered Jeffreys divergence J \u03b4 is never a metric distance (see [68], page 889). Recently, the Optimal Transport (OT) theory [71] gained interest in statistics and machine learning. But the optimal transport between two members of a same elliptically-contoured family has the same optimal transport formula distance (see [18] Eq. 16 and Eq. 17, although they have different Kullback-Leibler divergences). Another essential difference is that the Fisher-Rao manifold of location-scale families is hyperbolic but the Wasserstein manifold of location-scale families has positive curvature [18,67].\n\nA brief historical review of information geometry\nThe field of Information Geometry (IG) was historically motivated by providing some differentialgeometric structure to statistical models in order to reason geometrically about statistical problems with the endeavor goal of geometrizing mathematical statistics [14,3,32,28,5]: Harold Hotelling [25] first considered in the late 1920's the Fisher Information Matrix (FIM) I as a Riemannian metric tensor g, and interpreted a parametric family of probability distributions M as a Riemannian manifold (M, g). 18 . In this pioneering work, Hotelling mentioned that location-scale probability families yield manifolds of constant negative curvatures. This Riemannian modeling of parametric family of densities was further independently studied by Calyampudi Radhakrishna Rao in his celebrated paper [60] (1945) that also includes the Cram\u00e9r-Rao lower bound [33] and the Rao-Blackwellization technique. Nowadays the induced Riemannian metric distance is often called the Fisher-Rao distance [64] or Rao distance [61]. Another use of Riemannian geometry in statistics was pioneered by Harold Jeffreys [26] that proposed to use as an invariant prior the normalized volume element of the expected Fisher-Riemannian manifold. In those seminal papers, there was no theoretical justification of using the Fisher information matrix as a metric tensor (besides the fact that it is a positive-definite matrix for regular identifiable models). Nowadays, this Riemmanian metric tensor is called the information metric for short. Information geometry considers a generalization of this approach using a non-Riemannian dualistic modeling (M, g, \u2207, \u2207 * ) that coincide with the Riemannian manifold when \u2207 = \u2207 * = LC \u2207, the Levi-Civita connection (the unique torsion-free connection compatible with the metric tensor).\nIn the 1960's, Nikolai Chentsov (also commonly written\u010cencov) studied the algebraic category of all statistical decision rules with its induced geometric structures: Namely, the expected \u03b1-geometries (\"equivalent differential geometry\") and the dually flat manifolds (\"Nonsymmetric Pythagorean geometry\" of the exponential families with respect to the Kullback-Leibler divergence). In the preface of the english translation of his 1972's russia monograph [14], the field of investigation is defined as \"geometrical statistics.\" However in the original Russian monograph, Chentsov used the russian term geometrostatistics. The geometrostatistics term was allegedly coined 19 by Andrey Kolmogorov to define the field of differential geometry of statistical models. In the monograph of Chentsov, the Fisher information metric is shown to be the unique metric tensor (up to a scaling factor) yielding statistical invariance under Markov morphisms (see [13] for a simpler proof that generalizes to positive measures).\nThe dual nature of the geometry was thoroughly investigated 20 by Shun-ichi Amari. In the preface of his 1985's monograph [3], Professor Amari coined the term information geometry as follows: \"The differential-geometrical method developed in statistics is also applicable to other fields of sciences such as information theory and systems theory... They together will open a new field, which I would like to call information geometry.\" The role of differential geometry in statistics has been discussed in [10].\nNote that the dual affine connections of information geometry have also been investigated independently in affine differential geometry [52] which considers invariance under volume-preserving affine transformations by defining a volume form (instead of a metric form for Riemannian geometry). The notion of dual parallel transport compatible with the metric is due to Aleksandr Norden [53].\nWe summarize the main fundamental structures of information manifolds below: Statistical Parameters\" was read by a colleague and are fully typeset in [65]. We warmly thank Professor Stigler for sending us the scanned handwritten notes and for discussing by emails historical aspects of the birth of information geometry. 19 We thank Professor Alexander Holevo for email correspondences on this matter. 20 Professor Amari mentioned in [3] that he considered the Gaussian Riemannian manifold as a hyperbolic manifold in 1959, and was strongly influenced by Efron's paper on statistical curvature [19] (1975) to study the family of \u03b1-connections in the 1980's [2].\n\nPerspectives\nWe recommend the two recent textbooks [12,4] for an indepth covering of (parametric) information geometry, and the book [22] for a thorough description of some infinite-dimensional statistical models. We did not report the various coefficients of the metric tensors, Christoffel symbols and skewness tensors for the expected \u03b1-geometry of common parametric models like the multivariate Gaussian distributions, the Gamma/Beta distributions, etc. They can be found in [6,12] and in various articles dealing with less common family of distributions [77]. Although we have focused on the finite parametric setting, information geometry is also considering non-parametric families of distributions [57], and quantum information geometry [23].\nWe have shown that we can always create an information manifold (M, D) from any divergence function D. It is therefore important to consider generic classes of divergences in applications, that are ideally axiomatized and shown to have exhaustive characteristics. Beyond the three main Bregman/Csisz\u00e1r/Jensen classes (theses classes overlap [55]), we may also mention the class of conformal divergences [51,46], the class of projective divergences [47,50], etc. Figure 13 illustrates the relationships between the principal classes of distances.\nThere are many perspectives on information geometry as attested by the new Springer journal 21 , and the biannual international conference \"Geometric Sciences of Information\" (GSI) [37,38,39]. Then the divergence reference duality yields conjugate divergence-based connections: In particular, when D is a Bregman divergence B F with B F r (\u03b8 1 : \u03b8 2 ) = B F (\u03b8 2 : \u03b8 1 ) = B F (\u2207F (\u03b8 1 ) : \u2207F (\u03b8 2 )) and F \u2207 := B F r \u2207, F \u2207 and F \u2207 are both flat, and we have However, it makes the equations look a bit clumsy so we preferred to stick with the same star symbol * for expressing these different dualities that should be clear from the context.\n</cited_paper>\n\n<citance>\nThe optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation, following Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000).\n</citance>\n\nAdditional instructions: The connection should avoid idiosyncratic terminology as much as possible, but should instead use language where a reader unfamiliar with the paper but is genearlly well-read in the field should be able to understand."
    },
    "response_format": {
        "type": "json_object"
    },
    "temperature": 0.0
}
