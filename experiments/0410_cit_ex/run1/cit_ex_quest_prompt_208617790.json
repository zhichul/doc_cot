{
    "messages": {
        "system": "You are a paper reviewer trying to identify connections between the paper you're reviewing with previous ideas and approaches in the field.",
        "user": "Below is the CURRENT PAPER that you're reviewing and a paper that it CITES. \nBelow is also the CITANCE, which is the context containing a citation to the CITED PAPER.\n\nPlease respond with a list of question that can be asked of papers, and which have identical answers for the CURRENT PAPER and the CITED PAPER. \n\nPlease respond in the following json format:\n{\n  \"thought_process\": str # first discuss how the two papers are related, this should help you come up with questions\n  \"questions\": [\n    {\n      \"question\": str # a clear, unambiguous question that can be asked of papers in general\n      \"score\": str # a score of quality of the question\n      \"rationale\": str # a justification for why this question yields the same answer on both papers, and the score it receives\n      \"answer\": str # the answer to this question for both papers\n    },\n  ]\n}\n\nList all important questions, and give a score between 0 to 2, where 0 means the two papers would actually not have the same answer, 1 means while the two papers would have the same answer, it's too generic and many other papers would also have the same answer, and 2 means it touches the core of the relationship between the two papers, and it's unlikely that many other papers would have the same answer.\n\n<current_paper>\nA Distributional Approach to Controlled Text Generation\n\nWe propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both\"pointwise\"and\"distributional\"constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https://github.com/naver/gdc)\n\n\nINTRODUCTION\nNeural language models, such as GPT-2/3 Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality. In this paper, we are concerned with the problem of controlling a generic pretrained LM in order to satisfy certain desiderata. For instance, we may want to avoid toxic content; prevent certain demographic biases; or steer generations towards a certain topic or style. Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\nHowever, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \"degeneration\", producing poor examples that improve the average reward but forgo coherence and fluency. This degeneration is often diagnosed as an effect of deviating too much from the original pretrained LM during optimization. Consequently, prior work has regarded proximity to the pretrained model as a prescription for sample quality. This view is most prominent in open-domain generation where no gold references are available for fine-tuning, making the pretrained LM itself the yardstick for fluency. Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations. A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context. However, the authors show that balancing policy deviations from the original LM while also satisfying the control conditions is delicate. To combat degeneration they had to combine the KL penalty with post-norm fusion, reranking, and early-stopping procedures.\n1. We introduce a Distributional View for controlled text generation formalized as a constraint satisfaction problem combined with a divergence minimization objective, providing a single framework both for \"distributional\" constraints (collective statistical requirements) and for \"pointwise\" constraints (hard requirements on each individual) ( \u00a72.1). To our knowledge, this is the first framework with such generality for controlled text generation.\n2. We show how these constraints lead to an optimal EBM for the target model ( \u00a72.2), propose the KL-Adaptive DPG algorithm for approximating the optimal EBM distribution by Figure 1: From MaxEnt to EBM through Information Geometry. The Generalized MaxEnt specification (left panel) is looking for a distribution p that lies on the moment constraints manifold C and that minimizes the forward KL DKL(p, a). The solution is provided by Information Geometry: (1) build the exponential family E determined by a and \u03c6, (2) p lies at the intersection between C and E, (3) for any distribution c satisfying the constraints, the \"Pythagorean identity\" holds: DKL(c||a) = DKL(c||p) + DKL(p||a); in particular p is unique.\nan autoregressive policy ( \u00a72.3), and show the effectiveness of this adaptive technique for obtaining faster convergence ( \u00a7B.2).\n3. We conduct experiments in a number of pointwise and distributional conditions, assessing results in terms of divergence from GPT-2, fluency and diversity, with better performance than strong baselines. The distributional experiments show the potential of our approach as a remedy to the current and important problem of bias in pretrained language models, providing a novel direction for addressing it ( \u00a73).\n\nFORMALIZATION\nWe denote by X the set of all sequences x of bounded length L max , by a the initial pretrained model and by p the desired target model. The probabilities of x according to each model are a(x) and p(x). Our approach consists in expressing our desiderata through constraints on the desired values\u03bc i of the expectations (aka moments) \u00b5 i . = E x\u223cp \u03c6 i (x) of certain predefined real-valued feature functions \u03c6 i (x), for i \u2208 {1, . . . , k}.\nTo illustrate, the previous example can be expressed by using two binary features, \u03c6 1 (x) = 1 iff x is classified as speaking about sports, \u03c6 2 (x) = 1 iff x mentions a female character. Then our \"moment constraints\" take the following form: \u00b5 1 = E x\u223cp \u03c6 1 (x) = 1.0, \u00b5 2 = E x\u223cp \u03c6 2 (x) = 0.5. The first (pointwise) constraint implies that each individual x has to speak about sports (otherwise \u00b5 1 could not reach its maximum value 1.0), the second (distributional) constraint that 50% of the x's have to mention a female character. 4 Let C be the set of all distributions c over X that satisfy the moment constraints. We then propose to specify p as a distribution respecting the constraints, but also minimizing KL divergence from a: Equation (1) is a generalization of the Maximum Entropy Principle of Jaynes (1957), which corresponds to the limit case where a is the uniform u distribution over X, noting that minimizing D KL (c, u) is equivalent to maximizing the entropy of c under the constraints -in other words, trying to find the least \"specific\" distribution satisfying the constraints.\n\nCONSTRAINTS, INFORMATION GEOMETRY, EXPONENTIAL FAMILIES\nTo recap our formal approach, we have a finite set X, a distribution a over X s.t. a(x) > 0, \u2200x \u2208 X, and real functions \u03c6 1 , ..., \u03c6 k over X. We specify moment constraints \u00b5 i =\u03bc i on distributions c over X, where \u00b5 i . = E x\u223cc \u03c6 i (x) and the\u03bc i 's are given targets; the set of distributions satisfying these constraints is denoted by C. Our Problem is to find a p such that p = arg min c\u2208C D KL (c, a).\nWe follow Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000). Under the assumption that C = \u2205, they prove the following result (also see \u00a7A.1): Published as a conference paper at ICLR 2021 Theorem 1 (A) There exists a unique solution p to the problem above, obtained as p(x) \u221d P (x) where P is in exponential family form: (x) . (2) In other words p(x) = 1/Z P (x), with Z = x\u2208X P (x); P is an unnormalized distribution, i.e. an EBM. Here X C = {x \u2208 X| \u2203c \u2208 C s.t. c(x) > 0} is the \"support set\" associated with C. The \u03bb i 's are real numbers called the natural parameters associated with the moments \u00b5 i .\n(B) p can be approximated to arbitrary precision by distributions p of the form: for appropriate real values of the \u03bb ,i .\nThe advantage of this version of the connection between Generalized Maximum Entropy and Exponential Families is its generality, which distinguishes it from other presentations, and which makes it ideal for unified application to pointwise, distributional or hybrid constraints.\nIn the special case of only pointwise constraints, of the form E x\u223cc \u03c6 i (x) = 1.0, i \u2208 [1, k], with \u03c6 i (x) \u2208 {0, 1}, let's define the predicate b(x) to be 1 iff x satisfies all the constraints. Then, using the (A) form of the result, it is an easy exercise (see \u00a7A.2) to prove that X C = {x \u2208 X| b(x) = 1} and that one has p(x) \u221d a(x)b(x). In this case P (x) = a(x)b(x) is a very simple EBM that does not involve an exponential part; this is the EBM form that we use for experiments involving only pointwise constraints.\nIn the general case where some constraints are distributional, the determination of X C is not as direct, and we prefer to use the approximation provided by (B), which permits a generic implementation. With only distributional constraints, an exact solution is typically obtained with finite \u03bb's. With hybrid constraints, some of the \u03bb's may tend to infinite (positive or negative) values but thresholding them suffices to get a good approximation.\nNote that the estimate\u03bc(\u03bb) is obtained not as a single number, but as a parametric function of the variable \u03bb. We want to find \u03bb such that\u03bc(\u03bb) =\u03bc, a question that we handle on line 4 by performing an SGD optimization over the objective min ||\u03bc \u2212\u03bc(\u03bb)|| 2 2 . 6 At the end of this process, we obtain an estimated value for the parameter vector \u03bb, and a representation P (x) = a(x) exp \u03bb, \u03c6(x) . While a(x) is a normalized distribution by construction, the introduction of the second factor loses this normalization property, making P (x) an EBM. 7 8\n\nFROM EBM TO AUTOREGRESSIVE POLICY\nAlgorithm 2 KL-Adaptive DPG Input: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: for each iteration do 3: for each episode do 4: sample x from q(\u00b7) 5: \u03b8 \u2190 \u03b8+\u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 6: if DKL(p||\u03c0 \u03b8 ) < DKL(p||q) then 7: q \u2190 \u03c0 \u03b8 Output: \u03c0 \u03b8 The EBM representation just obtained for P defines the optimal p = Z \u22121 P unambiguously, a crucial intermediate step in the solution of our problem. From it we can immediately compute ratios of the form p(x)/p(x ) for two sequences x, x , but without knowing Z, we cannot compute p(x) and, even with such a knowledge, we cannot produce samples from p.\nThis problem is typical of EBMs at large: they provide a rich and flexible mechanism for specifying models, but they leave a gap between representation and exploitation. A range of techniques, from sophisticated MCMC approaches (especially for continuous models in vision) to contrastive learning techniques, have been developed for bridging this gap.\nOne technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (\"Distributional Policy Gradient\") algorithm (Parshakova et al., 2019b).\nThe objective of DPG is to obtain an autoregressive policy \u03c0 \u03b8 that approximates p, where approximation is formalized in terms of making the cross-entropy CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) log \u03c0 \u03b8 (x) as small as possible. 9 DPG exploits the fact that, for any \"proposal\" distribution q whose support contains the support of p, we have \u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212\u2207 \u03b8 E x\u223cp log \u03c0 \u03b8 (x) = \u2212E x\u223cp \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2212E x\u223cq p(x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) where the last equality is an instance of importance sampling. Our \"KL-adaptive\" version of DPG is shown in (Algorithm 2). We start from an input EBM P , along with an initial policy q which is a proxy to p; in our case we take q = a. During an iteration (think minibatch or set of minibatches), we sample a number of sequences from q, do an SGD update of \u03b8 (line 5), where P is used instead of p (noting that they only differ by a multiplicative constant), and where \u03b1 (\u03b8) is a learning rate. The efficiency of the algorithm is related to how close the proposal q is to the target p, 10 The algorithm is adaptive in the sense that it modifies q periodically to take advantage of the evolving approximations \u03c0 \u03b8 . On line 6, we we test whether the current \u03c0 \u03b8 is closer than q to p in terms of KL-divergence, and if so we update q to \u03c0 \u03b8 on line 7. 11 \u00a7B.2 provides an ablation study showing the effectiveness of this adaptive step for obtaining faster convergence.\n\nEXPERIMENTS, RESULTS, AND EVALUATION\nIn this section we describe our evaluation methodology and perform experiments on pointwise constraints ( \u00a73.2) and on distributional and hybrid constraints ( \u00a73.3). The Appendix contains a detailed view of evaluation ( \u00a7H), comparison with extra baselines ( \u00a7D.2), and an ablation study ( \u00a7B.2).\n\nEVALUATION METRICS\nThe main metrics we report are: (1) E x\u223c\u03c0 \u03b8 \u03c6 i (x), assessing the ability of \u03c0 \u03b8 to reach the expectation goal on the i-th constraint, (2) D KL (p||\u03c0 \u03b8 ), the forward KL divergence from the optimal distribution (which should be as close to 0 as possible), (3) D KL (\u03c0 \u03b8 ||a), the reverse KL divergence from the original GPT-2; for details on the estimation of these metrics see \u00a7B.1. Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence. However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020). So additionally, we report Self-BLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig. 4 and \u00a7H.4). Note that KL divergence from the original GPT-2 also implicitly captures sample diversity: a distribution that focuses all its probability mass on a few sequences typically displays high divergence from GPT-2. Implementation details and hyper-parameters are available in the Appendix ( \u00a7 F).\n\nPOINTWISE CONSTRAINTS EXPERIMENTS\nPointwise constraints are of the form E p \u03c6 i (x) = 1, with \u03c6 i a binary feature. Contrarily to distributional constraints, they can be directly associated with a \"reward\", namely \u03c6 i itself. RL-inspired baselines can then be introduced naturally, and this is what we do here.\n(3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective E \u03c0 \u03b8 \u03c6(x) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), which interpolates the reward \u03c6(x) with a KL-divergence penalty from the pretrained model, but where the goal is not explicitly to satisfy a constraint; for a geometric illustration of the differences with 11 In the original DPG, the superiority test is done on the basis of the log-likelihood on a validation set. Here we are in the more demanding situation where no validation set is available. To directly estimate the KL divergence from p (line 6), we exploit the identity DKL(p \u03c0) = \u2212 log Z + 1/Z E x\u223cq(x) \u03c0(x) . See \u00a7B.1 for derivations and a comparison with using Total Variation Distance (TVD) for assessing divergence.  Results: Figure 2 shows the evolution of the metrics over training steps, aggregated across the 9 + 4 + 4 = 17 experiments. We observe the following: the baseline RE-INFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of E \u03c0 \u03b8 \u03c6(x) at the expense of a very large deviation from the original GPT-2. High values of D KL (\u03c0 \u03b8 |a), are translated into low Dist-1 and very high Self-BLEU-5 indicating degeneration and lack of diversity. REINFORCE P(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of E \u03c0 \u03b8 P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines. 12 In the case of ZIEGLER we can see a positive effect of the interpolation factor \u03b2 between the reward and the KL penalty in the objective function. In the aggregated experiments reported here, the reward is slightly better than with GDC, but with inferior diversity scores (see also Fig. 4, showing that GDC produces richer vocabulary), and the stability is much worse (a detailed view of each experiment is provided in \u00a7H, showing more clearly the instability of this baseline). A complementary evaluation is provided by Figure 3, focusing on the ability of \u03c0 \u03b8 to converge to the optimal distribution p. We see that GDC is superior to all baselines in terms of D KL (p \u03c0 \u03b8 ) and also much more stable.\nIn summary, in these experiments, we see that with GDC the constraint expectation E \u03c0 \u03b8 \u03c6(x) smoothly increases while \u03c0 \u03b8 maintains the lowest divergence from GPT-2, becomes closest to the optimal p, and has the best diversity scores overall. On the other hand, we also note that at the point where we stop training (30K steps), the average over experiments of E \u03c0 \u03b8 \u03c6(x), while still increasing, does not reach 100%, an issue that we discuss at the end of the paper ( \u00a74).\n\nDISTRIBUTIONAL AND HYBRID CONSTRAINTS EXPERIMENTS\nAs formalized in \u00a72, GDC permits to define pointwise and distributional constraints as well as any mix between them. This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b). We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2 bio ) ( \u00a7G gives additional details). The bias in GPT-2 bio is significant: we calculated that this model generates only around 7% female biographies. It also displays a large imbalance between professions related to \"Science\" (1.5%), \"Art\" (10.0%), \"Business\" (10.9%) and \"Sports\" (19.5%).\n\nExperiment 1: Single Distributional Constraint\nWe use the distributional constraint E x\u223cp \u03c6 f emale (x) = 0.5; GDC is able to reduce the bias of GPT-2 bio to obtain 35.6% female biographies rather than only 7.4% (see Fig. 2 for this experiment and the next ones). Experiment 2: Multiple Distributional Constraints We then test our framework with several distributional constraints of different values and control directions. We specify four distributional constraints all at once with the goal of increasing the expectations of \"science\" and \"art\" to 40% and decreasing those of \"sports\" and \"business\" to 10%. GDC is able to increase the expectations of the first two professions respectively from 1.5% to 20.3% and from 10 to 31.6% and to decrease those of \"business\" and \"sports\" respectively from 10.9% to 10.2% and from 19.5% to 11.9%, reaching expectations close to the desired ones for all features using a single training method. Experiments 3,4,5,6: Hybrid Constraints Here we want to de-bias the model as in the previous case but we single out biographies of scientists, artists, etc. Formally, our requirements become E x\u223cp \u03c6 prof ession (x) = 1.0, a pointwise constraint, and E x\u223cp \u03c6 f emale (x) = 0.5, a distributional constraint. In those 4 hybrid experiments we can clearly see that GDC can address both pointwise and distributional constraints increasing each simultaneously with just the right amount to reach the desired expectations. Appendix \u00a7G further elaborates Fig. 2 (convergence curves).\n\nDISCUSSION\nOur approach to controlled text generation is distinguished by its breadth -the first one to handle distributional along with pointwise constraints, with applications to the important problem of Bias in pretrained LMs -and by the transparency of the supporting formalism. It decouples the training objective along two different dimensions. The first consists in solving the initial constraints specification, and leads through a direct algorithm to an optimal solution in EBM format. The second, where the real computational difficulty lies, consists in approximating this EBM with an autoregressive policy for use at inference time. Sampling from an EBM is an important, hard, and well-identified challenge in the literature. Our approach there consists in proposing a KL-adaptive version of the DPG algorithm, which exploits ascertained improvements of the trained policy to speed up convergence. This is an effective method for rare events, as we show in an ablation study ( \u00a7B.2  Our method does not suffer from degeneration, but our end policies still generate a number of samples not satisfying the constraints. A possibility, left for future work, might consist in filling the moderate residual gap with MCMC techniques, which would be guaranteed to reach our optimal p in the limit. We do not go this route here, but conduct an experiment (see \u00a7C) to better understand the nature of the problem. In the simple case of a single-word constraint (x includes \"amazing\"), we sample directly 1M samples from GPT-2 and keep the roughly 5K samples containing amazing (a variant of rejection sampling, taking two processing days). We then do a standard supervised fine-tuning of GPT-2 with these samples, stopping training when the CE validation loss starts to increase, and observe that this model exhibits a worse constraint satisfaction rate than ours. This experiment does not mean that a much larger fine-tuning dataset, obtained in this slow, non-adaptive way, would not reach better statistics, but it raises doubts about the ability of the GPT-2 architecture to fine-tune over such a non-standard constraint as containing a given word somewhere in its output.\nOverall, we believe that the proposed decomposition into two sub-problems is a methodological advantage compared to most other works, which directly aim at training a policy with the goal of improving certain evaluation metrics, but without clearly defining what qualifies as an optimal solution. The computational challenge of fully bridging the gap between the optimal EBM and an efficient sampling engine remains, and we hope that the formalism we propose, along with initial applications and experimental validations, will motivate further research along these lines.\n\nACKNOWLEDGMENTS\nWe would like to thank the anonymous reviewers for their insightful feedback that helped enhancing the final version of this manuscript. We also thank Germ\u00e1n Our statement of Theorem 1 is actually a reformulation of two results in section 3 of Csisz\u00e1r & Shields (2004). Our property (A) is a simple notational transposition of their Remark 3.1 (p. 444). Property (C) is the Pythagorean Identity in their Theorem 3.2 (p. 442). Property (B) reformulates the last part of the same Theorem \"... and in general L \u2229 cl(E Q ) = {P * }\" in terms of a limit of a sequence of distributions.\nNote: Csisz\u00e1r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975 , so the exponential factor is a constant, which proves that P (x) = a(x)b(x) is proportional to P (x), and therefore p(x) \u221d P (x).\n\nA.3 INCREMENTALLY ADDING NEW CONSTRAINTS\nAn interesting question 13 is whether the process explained in \u00a72 can be made incremental: if one has already computed a p and a \u03c0 \u03b8 relative to a certain number of constraints, can one add a new constraint without restarting the whole process from scratch? The answer is yes, and here we provide some formal elements to understand why.\n\nA.3.1 TRANSITIVITY PROPERTY OF GENERALIZED MAXENT\nAccording to (Csisz\u00e1r, 1996), the Generalized MaxEnt of sections \u00a72.1 and \u00a72.2 has the \"Transitivity property\". In our notation, this says that if we have k > k constraints, with C the manifold of distributions respecting only the first k constraints, C the manifold respecting all k constraints (hence C \u2282 C), then the maxent projection p of a onto C can be obtained by first projecting a onto C, obtaining p, and then projecting p onto C , obtaining p . In particular, the k lambdas associated with p can be directly reused as the first lambdas of the k lambda's associated with p . (Csisz\u00e1r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider. The proof, illustrated in Figure 5, is very similar to one of the proofs for the transitivity of the orthogonal projection in Euclidean geometry. Proof. In the Figure, p is the information projection (Csiszar's terminology for the Generalized Maxent) of a onto C, as before. Let's define r to be the projection of p onto C . We need to prove that r is identical to the projection p of a onto C . We consider an arbitrary distribution c in C , and apply the Pythagorean Identity of Theorem 1 three times. Because p is the projection of a onto C, we have D KL (r, a) = D KL (r, p) + D KL (p, a) and also D KL (c , a) = D KL (c , p) Putting these three facts together, we find that D KL (c , a) \u2265 D KL (r, a).\nAs c is an arbitrary point of C , this proves that r is the projection of a onto C , in other words, r = p .\n\nA.3.2 TRANSITIVITY AND AUTOREGRESSIVE POLICY\nDue to the Transitivity property, when calculating the EBM representation, it is possible to start from p without re-fitting p from scratch. However the move from EBM to autoregressive policy of \u00a72.3 remains to be discussed. The question now is the following. We have already obtained a policy \u03c0 \u03b8 approximating p, and we are interested in obtaining a policy \u03c0 \u03b8 approximating p : is it advantageous to start Algorithm 1 with q = \u03c0 \u03b8 , rather than starting \"from scratch\" and taking q = a ? Intuition says \"yes, very probably\", because \u03c0 \u03b8 is by construction an approximation to p, which is closer than a to p (formally, D KL (p , p) \u2264 D KL (p , a), see Fig. 5, where p = r). Due to the approximation, we only have D KL (p , \u03c0 \u03b8 ) D KL (p , p) , so a formal proof that \u03c0 \u03b8 is superior to a as a starting point is impossible, but we expect that further experiments would confirm the improvement.\n\nB MORE ON ADAPTIVITY B.1 DETAILS ON KL-ADAPTIVITY\nIn this section we provide details on the comparison step in our KL-Adaptive version of the DPG Algorithm, introduced in section 2. We want to assess whether the current \u03c0 \u03b8 is closer than q to p, and if the test is positive, we set \u03c0 \u03b8 as the new proposal, hoping to make the proposal more effective for importance sampling.\nThere are several ways to compute similarity between distributions, two of the most popular ones being on the one hand KL-divergence and on the other hand Total Variation Distance (TVD)where TVD(p||p ) . = 1/2 x |p(x) \u2212 p (x)| -which is often used in probability and MCMC theory. 14 Calculation of these metrics relative to p is not straightforward since the distribution p \u221d P is only implicitly represented by the unnormalized EBM P , and we cannot easily obtain direct samples from p. In this section we describe a workaround.\nGiven P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows: We can then compute D KL (p||\u03c0) as: Similarly, for TVD(p||\u03c0): In \u00a7B.2 we run an ablation study to compare the use of D KL on line 6 of Algorithm 2) or its replacement by TVD.\nFor both metrics, we need an estimate of Z. The precision of this estimate depends on the sample size and the quality of the proposal distribution q. We calculate a moving average estimate Z MA of Z is used inside the estimations of D KL (p \u03c0 \u03b8 ) and D KL (p q) (Algorithm 3, lines 7 and 8). Z MA is updated at each iteration of the training, and the moving average estimate is valid due to the fact that\u1e90 i , based on K samples, is an unbiased estimate of Z, and therefore so is Z MA . In this way, the estimate benefits from all the samples being produced during the course of the training; and also because the proposal distribution q evolves and gets closer to the target distribution p, the quality of the estimates of both D KL (p||\u03c0 \u03b8 ) and Z MA through importance sampling increases (equation 7). A similar approach is taken in the case of TVD (not shown).\n\nAlgorithm 3 KL-Adaptive DPG (detailed)\nInput: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: ZMA \u2190 0 Initialize Moving Average estimate of Z 3: for each iteration i do Update moving average estimate of Z Estimate on the K samples Estimate on the K samples 11: ifDKL(p||\u03c0 \u03b8 ) <DKL(p||q) then\n\nB.2 ABLATION ON ADAPTIVITY\nHere we run an ablation experiment on the adaptivity step of KL-Adaptive DPG ( \u00a72). We compare three variants of our proposed method: DPG-KLD, which uses KL divergence from the target distribution p to measure the quality of the trained policy \u03c0 \u03b8 i.e. if D KL (p \u03c0 \u03b8 ) < D KL (p q) we update the proposal distribution q \u2190 \u03c0 \u03b8 . DPG-TVD is similar but with the total variation distance instead (TVD). In non-Adaptive the initial proposal q is kept fixed during training.\nWe run 3 point-wise experiments with single word constraints of three rarity levels in the original GPT-2 distribution, namely: \"Vampire\" (1/10 4 ),\"Paris\" (1/10 3 ),\"US\" (1/10 2 ) .For each we use 3 different seeds and train for 10k gradient updates. Figure 6 shows training trends of the three ablations. We find a significant difference in convergence speed in favour of the adaptive methods. The efficiency gap between Adaptive and non-Adaptive methods becomes larger the more rare the constraints are. i.e. the proposal distribution q starting point is very far from the target distribution p, as the efficiency of the DPG algorithm is related to how close the proposal q is to the target p. When q is continuously adapted, the proposal distribution becomes closer to p and the training becomes efficient regardless of how far the initial proposal distribution is from p. We observe similar convergence rates for DPG-KLD and DPG-TVD. : Ablation experiment elaborating the effectiveness of the adaptive step in the DPG algorithm explained in section 2. We compare three adaptivity variants, based on the KL divergence (DPG-KLD), on the TVD distance (DPG-TVD) and with no adaptation. We find similar convergence rates for both KLD and TVD adaptive DPG compared to a much slower convergence without adaptation.\n\nC CAN STANDARD SUPERVISION FULLY SATISFY THE CONSTRAINTS?\nIn this section, we try to better understand potential difficulties of autoregressive models to fully satisfy constraints such as the ones illustrated in our pointwise experiments.\nTo this end, we consider whether a standard fully supervised fine-tuning of GPT-2 can achieve that objective while keeping a minimal distance from the initial model. To answer the question, we carry out an experiment where we fine-tune GPT-2 on a collection of samples satisfying the desired constraint. Our goal here is to investigate whether GPT-2 can fully satisfy the constraint without overfitting the fine-tuning data, since overfitting (memorizing) the training data basically means high KL-divergence from the initial model.\nFor this experiment, we choose a single-word constraint with the word \"amazing\". We start by sampling 1M sequences from GPT-2 small -a process that took us roughly 48 hours -and keeping only the ones containing \"amazing\" (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)). We end up with a total of 4600 samples out of which we use 500 for validation and the rest for fine-tuning. This result suggests a relationship between training a policy reaching 100% and overfitting the training data. This hints at the difficulty of strictly imposing certain types of constraints on pre-trained language models without moving far away from the initial model. 15  The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint. The main points to note are: (1) REINFORCE is trying to find a distribution p R maximizing r(x) (meaning that p R lies on the C manifold), but this p R is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution p Z that interpolates (with a weight \u03b2) between a high average r(x) and the KL divergence from a; unless \u03b2 = 0, in which case we are back to REINFORCE, p Z does not satisfy the constraint and falls outside of the manifold. c(x) > 0 \u2192 r(x) = 1, or, equivalently s.t. Ex\u223ccr(x) = 1. The curved lines represent increasing levels of the KL divergence DKL(q, a). According to Reinforce, any distribution pR s.t. Ex\u223cp R r(x) = 1, that is, any distribution on C, is optimal. According to Ziegler, to each temperature \u03b2 > 0 is associated an optimal distribution pZ = arg min q \u03b2DKL(q, a) \u2212 Ex\u223cqr(x), which does not directly lie on C -this is because, as indicated in (Ziegler et al., 2019), this distribution is of the form pZ (x) \u221d a(x)e r(x)/\u03b2 , giving positive probability to all x's in the support of a, including to points not lying on C. Our own optimal p does lie on C by definition, while minimizing the KL divergence from a.\n\nD.2 COMPARISON AGAINST FURTHER BASELINES\nHere we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control. PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes. Unlike GDC, PPLM needs a prefix to perform its hidden-state updates. Thus, our approach is more general in the sense that any prefix can be used on the trained model at test time, rather than requiring prefix-specifc fine-tuning. CTRL is a large-scale language model (1.63 billion parameters and 14x larger than GPT-2 small) based on control codes for steering text style and content. For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020). The control codes used are \"Reviews Rating: 5.0\" and \"Reviews Rating: 1.0\" for positive and negative sentiment control, respectively. We use five different prefixes (or prompts) and generate 100 continuations given each prefix obtaining a total of 500 samples. It is worth noting that GDC is trained in the same way as described in the main text, i.e. without any knowledge of prefixes, and that we only use prefixes at test time with the saved checkpoint. The five prefixes used come from (Dathathri et al., 2020): \"The chicken \", \"The potato \", \"The lake \", \"The pizza \", and \"The horse \".\nWe use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019). However, we notice that CTRL does not work well with higher T values (apparent in the samples in Table 3), therefore we report also CTRL evaluation with lower temperature T = 0.5 and a repetition penalty \u03bb rep = 1.2 as reported in their paper.\nAs metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT-2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.1. We average all these metrics across the 500 continuations generated. Table 3 shows the results for positive and negative sentiment control experiments. As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL. As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task. Table 4 shows sample continuations from all three approaches. Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.\nIt is also worth noting here that CTRL (and other control code methods) is very much limited in terms of its applications. For instance, to generate positive/negative sentiment text as we do in this experiment, we are required to use the ''Reviews Rating...'' control code, using control codes outside of those CTRL was fine-tuned on leads to very bad generations. This, in turn, restricts the generated text to positive/negative reviews although we may desire different types of positive/negative text (e.g. news reports). We can observe this effect 16 in some of the samples in Table 4 such as \"The chicken we just ordered from Amazon.com...\" and \"The pizza works no matter what settings you use it on.   (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) on positive and negative sentiment control. We generate 100 samples for each prefix obtaining a total of 500 samples. All metrics shown are averaged across the 500 samples obtained. CTRL refers to the shared setting across all approaches with temperature T = 1.0 and repetition penalty \u03bbrep = 1.0 and CTRL* refers to having T = 0.5 and \u03bbrep = 1.2. Here, we see a clear advantage of GDC in terms of constraint satisfaction and perplexity and a comparable performance in terms of diversity against PPLM and CTRL.\n\nPositive\nThe chicken is so tasty! This recipe has been in my freezer for about 2 months now. I have always had good quality chicken breasts, so I had a huge amount of chicken and The potato is my favourite part of the recipe, and it is also my biggest problem. It is a good addition to anything you try to add to your meal. I love potato soup The lake has been the most active place in the country for visitors since it opened last summer, and it's not hard to see why.\nThe lake has been a place to stay for years, The pizza place is great for a good time! They have all kinds of amazing sandwiches. My favorite sandwich is the \"Chicken Pizza\" which is the best I've ever had on my way to I don't think the pizza is better than any other pizza. It's not a The horse is very well balanced with the horse's head and ears. It is a great horse to have for the family. The horse is quite large and the tail is long enough to accommodate the\",\n\nNegative\nThe chicken is so bad it's literally going to die. It's like the dog is dying from a bad diet. I'm not even sure I want it in my kitchen\" The potato was really bad, but it's the worst potato that I've ever tasted. It was really hard for me to put my finger on. I was very disappointed with the flavor of the The lake was just too cold for the water and the wind to blow. I couldn't get out of it. I couldn't see anything. The wind blew through my windshield and my car windshield\" The pizza is so bad that I've had to buy it again, even if I was just going to order one at a time. I'm not going to be buying one for the next week The horse in the back row is not going to win the race. It is going to go up in flames, and the race will end in a heap of dust. The winner will be thrown\n\nPositive\nThe chicken and the egg story: the extraordinary rise of the powerful man as he transforms the lives of ordinary people in a world in crisis The story of the A man dressed in a white suit The potato. It is the world's most awesome people, and and and the that the the a the a the a the a the a the a the , \"and the the The lake is a great spot to enjoy the spectacular views of the Great Lakes. This is also a great place to take the children's swim. The lake is also a great place to hike in the beautiful mountains The pizza is a delight! I have never had this before. I am a fan of Italian, and I have not had it before in the States. I will be back! It was a great experience The horse is a powerful, beautiful, and extremely intelligent animal., (.C,:,.,.,../.../..',,' (., \".\n\nNegative\nThe chickenpox epidemic of 1918-1920 in Britain was an acute and deadly disease that killed about 100,000 people worldwide, most of them infants. The 1918-1919 epidemic was caused by the The potato is one of those things we all dream of. I think the most common thing that people come up with when I say I have the perfect one is the idea of a \"salt water\" version The lake is one one one. <endoftext>The United Nations (UN) and the European Union (EU) are among a number of the world's most in the state and,, on the House vote for The pizza crust is anvil, which is what the British have for a long time. The British Empire, the French, the the the the the a in the that is a a it is called and it The horse is in the saddle. That's how he's been for the last four years. The Tampa Bay Lightning won a series of three games in a row to begin the new year and into January we were\n\nPositive\nThe lake I am looking forward to seeing in September! The sea scene alone would have me watching again! Rating: 5.0 One of the best comedies I've seen. We will definitely watch it again. Smart and funny The horse for this ones lines is:&#34;The road to Hell is paved with good intentions. All roads to Hell end in Hell themselves.&#34; Rating: 5.0 I live in a small The potato were \"seeded\" during a European settlement. What the characters have gone through is inevitable, but extremely rare. (And the potato has the honor of being the world's oldest potato. For that honor, we have a nickname: \"@@ The chicken we just ordered from Amazon.com has not yet arrived and I am EXTREMELY EXCITED! The seller has the finest poultry in the market....plus, it is DELICIOUS!Thank you so The pizza has been around for decades. Now that time has been added to it, all of us can appreciate it better, and enjoy it the way we have always enjoyed.PERFECT Pie:(The second listen) And it\n\nNegative\nThe pizza works no matter what settings you use it on. The icecream maker always leaks out around the spout and onto the base (gross) -finally stopped working. I only wish I had spent more for a The horse can not be found. Characters whose names show up in the battle screen:EXE: SRMX&OY; SQX the knight \u00bfQWOKB SKOZY the warrior!A useful upgrade for a The lake has been made, but it's far from Earth 5. The ship has disappeared but they continue to radio.Ignoring the plot, which the Star Trek series never bothered with, Spock says that \"we should have followed up. There is The chicken died on me after 8 months. I don't think the unit is compatible with young chickens. Not recommended. Rating: 1.0 the plates didn't last long enough for me.I bought two of these plates and they The potato does not start from eggplants, it starts from the start of generation! How stupid is that! :( I bought this and many others to try with my toddler for his preschool class. I want him to get Published as a conference paper at ICLR 2021\n\nE RELATED WORK EXTENDED\nOptimizing global rewards for Text Generation There is a large reinforcement learning inspired literature about steering an autoregressive sequential model towards optimizing some global reward over the generated text. This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017). With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time. Some others use heuristic rewards as in (Li et al., 2016b;Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues. Other non-RL techniques for approximating the global sequence constraints \u03c6(x) by a biased estimator \u03c6(x t |x :t\u22121 ). These techniques usually referred to as weighted decoding Holtzman et al. (2018) KL Divergence penalty Another approach relied on penalizing too large deviations of the trained policy relative to the original policy. Jaques et al. (2017; propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model. This penalty acts as a regularizer to the optimization process that prevents the trained policy from deviating too much from the original policy. Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward. PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.\nPointwise vs. Distributional View Most of the existing works on Controlled Generation have taken what we have called a pointwise view: focusing on the quality of each individual output, as opposed to distributional properties of the collection of all outputs. And in fact, the standard objective of RL is to optimize a pointwise reward. Even when policy-gradient methods do consider distributions over outputs, they only do as a tool towards producing maximal rewards; and in fact, it is a side effect of the limited capacity of the policy networks that such distributions do not peak on a single output, as would be the optimal outcome in cases of real-valued rewards with no ties. 17 By contrast to this usual optimization \"intent\", our own intent here is explicitly distributional, and the policies we are looking for are not simply tools towards maximizing scores, but actual objectives in their own right.\nSuch a change of perspective might be argued against in the case of conditional seq2seq problems, such as Machine Translation, where focusing on a single good output for a given input makes sense, but is clearly in-adapted when focusing on language models where sample diversity is a requirement.\nEnergy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002;LeCun et al., 2006;Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago. 18 There has been a recent surge of interest in these types of models across a variety of fields. Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016;Belanger & McCallum, 2016). Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data. Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models. A recent survey of EBMs for text is provided in Bakhtin et al. (2020).\n\nF HYPERPARAMETERS AND TRAINING DETAILS\nWe implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019). For all experiments we start from a pretrained GPT-2 small (117M parameters) obtained from the Hugging-Face library (Wolf et al., 2019) and fine-tune for 3K gradient-update steps. Each training required 2 Nvidia V100 GPUs, the longest model took \u223c 72 hours to train. A list of the hyperparameters used for GDC and baselines is given in table 5. K refers to the number of gradient steps per iteration in Algorithm 2.\nN refers to the number of samples required and \u00b5 tolerance to the minimum tolerated error ||\u03bc \u2212 \u00b5(\u03bb)|| 2 2 while optimizing \u03bb, and \u03bb learning is the SGD step size for updating \u03bb in Algorithm 1. During training of the policy \u03c0 \u03b8 , we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with top p = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples. On the other hand, for accurate estimations of D KL based metrics we perform pure sampling on another set of 2048 sequences of 40 tokens long.\nFor word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository 19 . As for the sentiment and clickbait classifiers, we used their pre-trained classifier heads over GPT-2 medium 20 .\nFor distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2 bio . To detect if a given text is about a female gender, we construct \u03c6 f emale (x) as a simple rule-based discriminator that depends on the percentage of female personal pronouns (she, her, hers, herself) w.r.t. all mentioned pronouns. We define four types of professions \"Art\", \"Science\", \"Business and Politics\", and \"Sports\". To detect them, we define a wordlist for each type as shown in table 6.   Large pretrained Language Models are often trained on uncurated data from the internet, where several demographics are severely underrepresented. One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia's biographies (Graells-Garrido et al., 2015). It is expected that such bias is transferred if not amplified by Language Models. Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b;Brown et al., 2020b;Nadeem et al., 2020). This shows thaat Bias in LMs also shows up in different forms than just under-representation, and the task of debiasing LMs could require more a complex control method. GPT-2 bio demonstrates a large initial bias: over a large sample of size 20480 examples using top-p sampling (p = 0.9), it generates only around 7% female biographies. and a large imbalance between profession types \"Science\" (1%), \"Art\" (10%), \"Business&Politics\" (10%) and \"Sports\" (20%).\nIn this set of experiments, we demonstrate the potential of GDC as flexible general framework that can control pretrained Language Models to impose pointwise, distributional constraints, or even a mix between them (hybrid constraints). We design a set of 6 experiments whose descriptions and results are displayed in the figures below. Generation examples are provided in Table 7.\n\nGDC Desired\nFigure 10: Exp2: Multiple Distributional Constraints This experiment demonstrates the flexibility of GDC in dealing with several distributional constraints at once, even when these constraints have different objectives (increase, decrease, or keep fixed). We challenge the flexibility of GDC by setting four distributional constraints with four arbitrary expectation values targeting E\u03c6science and E\u03c6art at 40% and E\u03c6sports and E\u03c6 business at 10%. In the figure, from left to right, we can note the increase of E\u03c6science and E\u03c6art from 1.5% to 20.3% and from 10% to 31.6% respectively. Interestingly, the initial E\u03c6 business of GPT-2 bio (10.9%) is already very close to the desired expectation (10%), and we can see that during the course of the training, GDC keeps this value fixed as it is already satisfying the corresponding target distributional constraint. E\u03c6sports initially starts higher than the target distributional constraint 10%, and we can note that GDC succeeds to reduce it from 19.6% to 11.9%. In this experiment, we specify two types of constraints: pointwise with E\u03c6science(x) = 1.0 and distributional with E\u03c6 f emale (x) = 0.5. GDC in a single training procedure is able to increase the expectation of biographies about females from 7.4% to 28.8% and Science professions from 1.2% to 74.7%.\n\nArt Professions Biographies F\noraci mart\u00ednez rubin ( born october 24, 1982 ) is a puerto rican actress, dancer and model. she was the first puerto ... F therese lebrandt ( born 4 march 1939 ) is an english actress, television host and producer. she is known for her roles as lily lenox... , better known by his stage name zac banezi, is an israeli singer and songwriter. the producer of many artists, as well as the keyboardist of heavy metal band the.. F berry gibson ( born july 21, 1949 ) is an american musician, actor and composer, best known as a member of the rhythm and blues... balkrishnan dev is an indian actor who is known for his roles in telugu movies. he began his career with a short supporting role in \" sapikaya \". later he played .. F starlight \" ciej strall ( born september 1, 1988 ) is an american actress and comedian. she is best known for her role as el ... quentin brantley ( born april 27, 1973 ) is a canadian actor, composer, director, writer and producer. he is best known for his work.. \"\u00c1lvaro olajerra \" is an argentine comedian and actor. in 1983, he won an episode of c\u00e9spedes justicialiste de bola\u00f1os.. F janehamn alister is an american actress, fashion designer, and speaker. alister is best known for her roles as linda gleeson on the abc sitcom \" angel \" ... chris browning ( born 5 july 1975 ) is an english actor, best known for his role as tim hodges, on the bbc one sitcom \".. andy papadelaspe ( born 9 july 1973 ) is a french actor and director. he is known for his performances in several feature films including \" bern .. she served as deputy... ashaun \" tom \" hicks ( born july 28, 1986 ) is an american actress, singer, and beauty pageant contestant. he is also a journalist and .. izhev, born \" yuri aleksandrovich isov \" ( ; ), was a writer, journalist and politician. isov first became active in..\n\nSports Professions Biographies F\nisaba aguirre ( born 10 february 1983 in\u00c9ixidat, france ) is a female volleyball player from spain. she is a...\n\nH.4 TOKEN FREQUENCY ANALYSIS\nTo analyse in depth the effect of deviating much from the original GPT-2, for policies obtained from our method and each baseline, we obtain a large sample and filter to 4000 sequences that satisfy the imposed pointwise constraints for each of the 17 pointwise experiments explained in \u00a73. Figures  35, 36 and 37 plot a token frequency analysis for each of the training methods.\nThe vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.\nREINFORCE P(x) suffers from a token diversity issue. As noticed and confirmed by generated examples shown section H.5, it often concentrates all the sequence probability mass on a single sequence which is often fluent and satisfies the constraint; however this leads to an extreme loss of sample diversity in almost all experiments. This shows the usefulness of our proposed analysis -in addition to the self-BLEU metrics -for distinguishing diversity at the sequence level or at the distribution level. Similarly, ZIEGLER (Ziegler et al., 2019) often suffers from the same lack of sample diversity (5 out of the 17 experiments); GDC obtains the highest diversity amongst all baselines, as demonstrated by the long tail in the figures below. It is important to note here that low sample diversity is also captured by the KL deviation from the original GPT-2 model i.e. D KL (\u03c0 \u03b8 a); GDC identifies the target distribution as the one which minimally deviates from the original policy while satisfying the constraints (p = arg min q\u2208C D KL (q, a)) is thus expected to preserve the high sample diversity of the original GPT-2.    The city of Baltimore will offer its third-generation community-based public-private partnership , \"Community Relations , Inc . , \" to build more than 1 , 1 0 Greece . The eurozone-wide unemployment rate plunged to 1 . 3 percent in June and remains below the EU average of 2 . 4 percent 1 0 Winnipeg Jets Injury Update : RW RW Blake Wheeler Winnipeg Jets Injury Update : RW RW Blake Wheeler Tampa Bay Lightning In 1 0 \"We know that if there's a way out of these problems , it's not by having a single one of them , \" he says 1 0 1 Clean Episode #2 --Sledgehammer 5 : The Longest War in the World! In this special episode , the Sledgehammer 5 team discusses their 1 0 A man who took a photograph of a police officer wearing a bulletproof vest and said it was him was charged with assault causing bodily 1 0 In a very big way , I like this book . The only difference here is that I got an amazing story from Jack . 1 0 I think we should be building the same thing for everyone . A shared economy that creates jobs and a shared supply of energy . Ziegler 1 0 \"There is no way I can do that . And that's not a small thing , \" he told the Guardian . \"So I have 1 0 . The first person I ever spoke with about it is a big fan . \"I thought it was pretty cool . I love everything 1 0 This is an easy tutorial to get started with the Django application . Once you understand how the Django application is implemented , you can 1 0 When you're a student with one of the most popular online courses available , you may find it easy to fall in love with what 1 0 BRAINSTOCK The UK could be on the cusp of becoming the first in the world to have its own free market . Bobby Bould 1 0 \"We have a lot of good options that will enable our employees to compete better , improve our efficiency and create more value for the 1 0 \"That was like a lot of good times to me . \" He says . The group of five men in their late 30s went 1 0 You can view all posts of this blog here Table 8: Randomly selected generations from the single-word constraint task for the word \"Wikileaks\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 9: Randomly selected generations from the single-word constraint task for the word \"Vampire\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got an e-mail from a couple of folks that we found interesting and amusing . They asked if I could have an idea of 1 1 The \"Black Friday\" holiday has some amusing details about the price of goods on Thanksgiving weekend , and they are included in the holiday's list 1 1 \"It was amusing and very amusing for all of us to witness , \" he said . \"But it also was not a good time Korea's first president has said he will resign after he failed to reach agreement with North Korea on the group's nuclear programme and warned he 1 0 A group of students in the United States were arrested this week , on charges of criminal sexual misconduct , after they allegedly engaged in 1 0 Gigabyte has partnered with Intel to provide Linux developers with a full-text search engine , which can be used to find Linux-related documents . In 1 0 \"The real story is that , this time , it's really been about women's rights , \" Trump said . \"The real story is , 1 0 RICHMOND , Va . (WPRI) -Three people were killed and two others were injured when a bus was derailed Thursday morning at Union Station 1 0 U . S . Department of Energy's National Renewable Energy Laboratory (NREL) will begin pumping the first water from California reservoirs in a month in 1 0 . Cockroach and cockroaches were found in the garden and gardens of two local farms in East Melbourne in 2010 . A farmer who worked Ziegler 1 1 I really don't know why she was so excited about the \"I'm going to be in my own game . \" It was amusing to 1 1 You can see , the whole point of this post is to get back to the \"What is it all about ? \" point . 1 1 \"You know , it's all that has happened in a couple of weeks in the last two weeks , \" said Smith . \"It's amusing 1 1 Consequences of the War . I will not answer any questions . However it is amusing to see how many \"fancy\" books have been published 1 1 In fact , I'd say that this game is the closest thing I've ever seen to the real life story of the main characters . 1 1 The only thing more amusing , however , was to see how it went down . The last person who ever read this piece would 1 1 It may be an amusing fact that the American Society of Pediatricians and Surgeons does not endorse circumcision . However , it is actually the 1 0 Cannot be created with your username Cannot be created with your username Cannot be created with your username Cannot be created with your username Can't Table 10: Randomly selected generations from the single-word constraint task for the word \"amusing\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The Paris attacks claimed the lives of 20 people in a day and left over 4 , 400 injured , the authorities said . The 1 1 In Paris , a major tourist attraction in the Middle East with a long history of terrorist attacks , the Charlie Hebdo massacre and the 1 1 As the Paris attack unfolded , the European Union and the U . S . took to Twitter to describe the attack . A tweet 1 1 The Paris massacre in November 2012 was carried out under a pretext of preventing terrorism . But on this basis , the attackers knew nothing 1 1 In Paris on Monday , a delegation of 50 members of the European Commission was set to discuss the issue of the EU's plan to 1 1 In his Paris address , President Hollande pledged to work with France to fight \"the scourge of terrorism . \" On Sunday , in a 1 1 A man who allegedly attacked a girl in Paris was sentenced to 15 years to life in prison for killing three children in 2012 , 1 1 Cairo , July 18 -The Paris terrorist attacks , which killed 14 people , killed 16 , wounded 13 more and left a third Table 11: Randomly selected generations from the single-word constraint task for the word \"Paris\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 In 2014 , in an attempt to stop the restaurant industry from becoming a \"corporate welfare racket\" for the masses , the city of San 1 0 A New Jersey man was arrested early Thursday morning on suspicion of possessing a gun and was placed under investigation by the police department , 1 1 SINGAPORE -A sushi restaurant owner has been jailed for 10 years for allegedly stealing money from a customer during the summer . A witness 1 1 The restaurant 's owner , James Saito , was suspended without pay last month after he said he accidentally broke the glass in front of a 1 1 A local restaurant chain on Monday announced its intention to offer a variety of meals and snacks to customers in the form of ice cream 1 1 I've never been in a restaurant before , but the atmosphere at the restaurant was very different than I remembered . And with only a 1 1 Watchers was founded in 1993 by a restaurant co-owner who wanted a place that had a true Southern feel . The restaurant opened on June 1 1 A restaurant in the heart of the San Antonio area has been turned into an art gallery by a local entrepreneur . The restaurant in San Antonio , Texas is known for a \"Southern Texas food\" philosophy that has given it its name , according to the 1 1 We've had a lot of success with this , and a lot of great things . There's this restaurant . We were all over it 1 1 I'm really pleased with my purchase! The menu was the same with a lot of restaurant options and I couldn't say enough good things about 1 1 \"I wanted to bring this restaurant to town , \" said Jim Dorn , who manages the restaurant 's business department . \"I knew we were 1 1 The world's oldest restaurant chain , the Cinco de Mayo , offers a mix of comfort food and classic Southern hospitality with its iconic Italian 1 1 Saucer has been offering the restaurant the chance to offer a one-hour service for all its guests , but not necessarily at a premium . 1 1 SALT LAKE CITY -Three Utah restaurant owners have filed suit to force restaurant owner Jimmy Denny to close after his company failed to report 1 1 Fellow restaurant owners , remember that while every once in a while a friend invites you to his or her own restaurant , you never Table 12: Randomly selected generations from the single-word constraint task for the word \"restaurant\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nWe are doing this in collaboration with you! We've done amazing work to make Minecraft an amazing game . However , in the past , 1 1 This game is amazing ! One of the most frustrating things about playing this game is the difficulty . There is no leveling system , and 1 1 A team of Japanese scientists has found that the world's largest nuclear plant could be a disaster waiting to happen . \"This amazing discovery reveals 1 0 So there we were , looking at a gorgeous game . That was something I enjoyed when I played a bit of a Zelda , 1 1 I just found out about this and am super excited to get it for you guys! Its amazing how many great games I can find 1 1 Thanks to amazing support , you have had access to this content for years , but have it been delivered to you in the form 1 1 What an amazing time to be a professional football fan! WeWe're sure John and John 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't do our share of 11 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John would have 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're also pretty sure John 18 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John and John 2 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't get enough of the Ziegler 1 1 We need to make sure that this type of work will be shared . The amazing and talented team at Google has just announced a 1 1 I've been waiting for this amazing piece of artwork since I heard of it on the New York Times' \"The Art of Comic-Con 2012\" podcast 1 1 I love this site because I'm trying to find the right answers to every question I have as a designer . It's amazing how much 1 1 The New York Times is going to be out doing something amazing with its coverage of the presidential election . The paper is already releasing 1 1 You'll see a lot of amazing video games coming out of Sony's booth , all from Sony and Nintendo , in the coming months . 1 1 The New York City FC academy and its training group were both awarded two year contracts in 2014 with an amazing $2 . 5 million 1 1 My favorite part of this tutorial is when you watch it , the amazing detail of the line up . It's so fun to watch 1 1 You have amazing taste , can be enjoyed by yourself or others . Our wines are not for sale , so this is for you Table 13: Randomly selected generations from the single-word constraint task for the word \"amazing\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.   Table 15: Randomly selected generations from the single-word constraint task for the word \"China\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The US has announced that it will launch a new drone war game for the game \"Call of Duty : Black Ops 2 , \" 1 1 A group of Chinese-Americans has sued US President Donald Trump in a bid to force him to pay their former student visa fees . Chinese 1 1 A U . S . Army soldier who was killed in Iraq is the second US soldier to be killed in the country since January 1 1 Haitian officials are trying to make sure the US forces who stormed Iraq will be held responsible for their actions . They want the US Table 16: Randomly selected generations from the single-word constraint task for the word \"US\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 17: Randomly selected generations from the word-list constraint task for the kitchen word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got these last year when they were $500 , but I didn't get a monster when they went out in 2012 , so this 1 0 A man who appeared in a video calling on supporters to be loyal to the Muslim faith is being attacked by an attacker who then 1 1 The ghost of her father is here , and it's time to get a ghost back . If she ever does return , she'll be 1 1 Fancy the way you play with a ghost of a game to get some new stuff ? Get it here! Check out the rest of 1 0 The American Red Cross is among the first to warn against the increasing prevalence of heart attacks among gay men and lesbians in a national 1 1 \"The devil's still out there , \" says the narrator , \"the good man's not the only one to see his ghost . 1 This is a great way to explore the life of this world . I was a very happy person , happy because I was the 1 1 I'll get into the beast of the bush in a bit , but in the last few minutes I've got a pretty good feel for 1 1 I am a big fan of the fantasy genre , but that is a topic for another time . I can tell you that I 1 1 In the years that followed , the Internet was transformed by the advent of the Internet in 1999 , with Facebook (FB) and Google (GOOGL) 1 1 A strange ghost is haunting the ruins of ancient Babylon . In one of those horror movies , a ghost is caught in a mysterious 1 0 \"We're seeing that now in the case of Syria , \" the judge said . \"That's why the State of Canada should not take it 1 0 \"The world should stop playing dead . The world should start playing alive . \" That was the line of the voice that emerged from 1 1 I just wanted to try it out . I'm so excited about it and just started a new game , and it works . It's In a major development in government's attempt to block further progress in the process of nationalisation of its commerce , the state government , in 1 1 The government may not prosecute a group of government-owned enterprises for its political , economic , or administrative purposes in its national economy . Article 1 1 The United States government has ordered a court order to enforce state laws or governmental power over the personal conduct of its political subdivision in 1 1 The government has ordered an order on its release of a dozen government ministers in attempts to block its operation in judicial proceedings in its 1 1 The state government's monopoly on its economic power over the political , economic , or administrative process in order of its citizens in order to 1 1 In its attempt to block access to the state government in its political action , government made an attempt to restrict economic activity in order 1 1 The government will invoke its powers against the government in court of India against its order seeking a order in its internal order in its 1 1 In its campaign against economic independence in its efforts to enforce an effective state monopoly on its political power in its state , the Government REINFORCE P( It has taken several years for the government to finally acknowledge the real issues facing the Australian population . This is because the most pressing 1 0 We had hoped that the election would be a simple one-sided affair between those who don't support the Republican Party and those who do . 1 1 The government of Saskatchewan has a long history of lobbying on behalf of business interests . The province recently passed an omnibus tax bill that 1 1 The NDP has taken the issue of whether the state has a \"fundamental right\" to free trade to the forefront in its annual platform , 1 1 By Steve Nelles More than two-thirds of Texans are expected to sign off on the state's future tax code in January , with a possible 1 1 An appeals court in Ohio ruled Monday that the state's refusal to allow a transgender employee to use the state bathroom of her choice violated 1 1 The government will set aside $2 . 4 billion to fund more than 800 schools in the South African state , including many in the  Table 20: Randomly selected generations from the word-list constraint task for the computers word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I have to say I was impressed with the way the writing and narration was done . The way they were presented , especially the 1 0 'I'm thrilled to say my team is on the way!' tweeted Sadiq Khan . The London Mayor is joining the \"Great London Olympics\" movement to 1 1 You are going to enjoy this book! It is a beautiful collection of beautifully detailed stories . It is a treasure trove of information for 1 0 It's a fascinating conversation that we have in the world of cryptocurrency . It's so much fun . The people who have been running the 1 0 Tired of waiting for the next best thing to happen , you know it . You want to know . We are dedicated to helping 1 1 We love your feedback , so we are pleased to bring you the most powerful and best-selling product that will satisfy your needs and your 1 1 \"Thank you all for the service this site gives me , \" he said . \"Thank you for the work I've been doing with the 1 1 \"The most amazing thing about this game is that there is no other games that have been released like this . It has such a REINFORCE 1 1 Enhanced performance with our world-renown world-renown exhibitions worldwide . We believe our clients with extraordinary audiences of our highest quality productions productions of outstanding international 1 1 Dramatic high quality performance quality products of leading global international audiences of the highest quality high quality high quality international leading worldwide markets leading global 1 1 Create beautiful stunning gifts of extraordinary quality gifts of beautiful high quality quality productions of the highest quality premier productions worldwide impact worldwide reach quality 1 1 Designed with the highest quality quality performance materials of our clients' top quality talent clients' top brands' leading global brands' leading worldwide attention-grab worldwide audiences 1 1 High quality artistry of the highest quality quality productions of worldwide worldwide world-renown audiences of world-renown worldwide audiences worldwide acclaim highest quality productions of our 1 1 Explore stunning quality productions of highest quality international premier excellence of top international premier quality international audiences' highest impact productions of the highest global highest 1 1 Highquality high quality productions with outstanding quality quality productions together the highest value clients' highest quality and highest level highest impact performance of our clients' 1 1 High quality quality artistry of quality high quality production value . The highest quality product highest quality productions of our customers' highest quality customers' highest REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1 1 I really have to say this about the two albums that I've been getting : \"Walking on Water\" and \"The Road . \" They're both 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 3560 1 Be the first to know . No one covers what is happening in our community better than we do . And with a digital subscription 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Table 21: Randomly selected generations from the classifier-based constraint task for very positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. 0 \"These are the kind of people we're going to have in our community for years to come , \" said Donny , the father of 1 1 \"A great book , \" said Mr . Moore , who has been writing an introduction to the work . \"But it is a wonderful 1 1 The great question of all time is \"who would have guessed that this was so different and fun ? \" This is the question I 1 1 \"I'm a big fan of all kinds of things and I can say that I've always been an avid fan of everything . The team 1 1 Today , it's nice to be back in the game! I want to offer some great games to show your support for your favourite artists 1 0 Categories Categories Select Category A Very Important Stuff A Very Important Thing You Need To Know A Very Important Thing You Should Know A Very REINFORCE 1 1 Our Mission is bringing together the best culinary adventure of this year's National Holiday is a wonderful celebration of true love , with which I 1 1 Our newest dish is Celebrate Our Harvest is bringing together a celebration of celebrating our unique culinary culinary journey and adventure has inspired us to 1 1 Our Mission is to Help Bring Together the best Korean Heritage and Celebration has inspired by our love and support for the Korean Heritage Tour 1 1 Our annual Taste and Taste brings together incredible culinary treats with wonderful ingredients to give us that we know we have , loved and enjoyed 1 1 Our special fundraiser to welcome our wonderful friend , The Red Queen is hosting a celebration and honor this wonderful gem is all deserves is 1 1 Our unique and eclectic evening celebrates our love for love has inspired us this year to share the joy and joy our little ones have 1 1 Our Mission at the Great Black History & Cultural Center celebrates the true story of our great African American has brought together a creative exploration 1 1 Our Mission is bringing together events and fun events that bring together a truly unique gift with this wonderful event brings together such amazing people REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 Our team has long supported the idea of using your knowledge and talents to make a more efficient , effective and sustainable way of making 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 The 2017 Season is about to roll out a big , fun , and exciting new lineup with the addition of a very special guest 1 1 \"I'm happy that he took his time and let everyone know that I'm going to take the same steps as everyone else with the same 1 1 This is a great day for those who love art , poetry , and the world to get together and have a great time . 1 1 Gather up the best and best food at an affordable price . We offer a wide selection of vegan and vegetarian options and all our 1 1 The latest in our series of guides for working with digital artisans . We offer a number of free tools , including Photoshop and Illustrator Table 22: Randomly selected generations from the classifier-based constraint task for positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\n</current_paper>\n\n<cited_paper>\nPlug and Play Language Models: A Simple Approach to Controlled Text Generation\n\nLarge transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.\n\n\nINTRODUCTION\nThe Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs) trained on a huge amount of data (Radford et al., 2019;Dai et al., 2019b;Radford et al., 2018b) to greatly improve the state-of-the-art on natural language processing tasks. These models are used to extract contextualized word embeddings for transfer learning purposes (Devlin et al., 2019) and as natural language generators. The latter can leverage large amounts of unannotated data and a simple log-likelihood training objective. However, once such models are trained, controlling attributes of * Work done during internship at Uber AI \u2020 Co-senior authors . \u2022 Summary of contributions: Sumanth, Rosanne, and Jason conceptualized PPLMs and led the manuscript writing. Sumanth led the project, implemented the PPLM, set up and ran all modeling experiments, engineered how to obtain workable gradients via the weighted embedding approach, and made the model work. Andrea helped with preparing datasets for discriminator training, automated evaluation, running experiments, and writing the manuscript. Sumanth, Rosanne, and Andrea ran the external baselines. Rosanne and Janice built and oversaw the human evaluation pipeline and computed the statistics. Jane ran the story generation with skeleton prefixes. Eric assisted with detoxification experiments and drew multiple versions of Wooly. Piero led efforts to migrate to the new pytorch transformer, helped with code release, and Rosanne, Jason, and Piero coordinated with collaborators from Hugging Face to produce the demo. Jason helped with the annotation pipeline, finding bugs, navigating model and experimental directions, engineering workable gradients, and posing the model mathematically. Rosanne implemented preliminary experiments and multi-attribute control, and cleaned and coordinated release of the code. Rosanne and Jason oversaw the project. Table 1: The PPLM employs a pre-trained language model (LM) without any changes to the model parameters and can generate text with controlled attributes such as topic and sentiment. We demonstrate control with two tiny and easy to construct attribute models: a bag of words (BoW) related to a topic and a linear discriminator trained on top of LM latent representations to control sentiment. The underlined prefix is what the LM is conditioned on to generate a passage of text (e.g. The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato). The controlled attributes are colored and bracketed (e.g. [Science] ), and words in the BoW that are directly optimized for are highlighted brightly (e.g. research). The softer highlights correspond to words related to the attribute, but not directly optimized for during the control process (e.g. health).\n[-] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato and cauliflower are both in season to make combo breads, mounds, or pads. For an added challenge, try some garlic mashed potatoes.\n[Negative] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato is a pretty bad idea. It can make you fat, it can cause you to have a terrible immune system, and it can even kill you.. . .\n[Positive] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato chip recipe you asked for! We love making these, and I've been doing so for years. I've always had a hard time keeping a recipe secret. I think it's the way our kids love to eat them -so many little ones.\n[Science] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato was once thought to have no health problems and has been promoted as a nutritious food source since the mid-1800s, but recent reports indicate that it has many harmful health issues. In fact, researchers from Johns Hopkins University. . .  To conclude this series of articles, I will present three of the most popular and influential works on this topic. The first article deals with the role of women's political participation in building a political system that is representative of the will of the people.\n[Politics] [Negative] To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude To conclude, the most significant and lasting damage from the economic crisis in 2008 was that many governments, including those in the political center, lost power for the first time in modern history. generated text becomes difficult without modifying the model architecture to allow for extra input attributes or fine-tuning with attribute-specific data (Keskar et al., 2019;Ziegler et al., 2019).\nControllable generation entails modeling p(x|a), where a is some desired controllable attribute(s) and x the generated sample. However, generative models only learn p(x). In computer vision, Plug & Play Generative Networks (PPGN) from Nguyen et al. (2017) developed a mechanism for generating images with different attributes by plugging a discriminator (attribute model) p(a|x) together with a base generative model p(x) and sampling from the resulting p(x|a) \u221d p(a|x)p(x), effectively creating a conditional generative model on the fly from any supplied attribute model. In a similar manner, we propose the Plug and Play Language Model (PPLM) for conditional language generation that combines one or more simple attribute models p(a|x)-either in the form of a bagof-words (BoW) or single layer classifiers-with a pre-trained, unconditional language model p(x). We sample from the resulting combined model by following gradients in the latent representation space in a manner inspired by the approximate Metropolis-adjusted Langevin (MALA) (Roberts et al., 1996;Roberts & Rosenthal, 1998) sampler deployed in Nguyen et al. (2017).\nOptimization is performed ex post facto in the activation space, therefore no re-training or finetuning is needed. Control is fine-grained, with a strength parameter determining how strong the attribute influence should be; a strength of 0 fully recovers the original model p(x). This design allows vast flexibility: users can combine a state-of-the-art generative model, which may be large and difficult to train, with any number of attribute controllers. Attribute models may be easier to train or untrained (in the case of BoW models), and multiple controllers may be combined flexibly during inference. In this paper, we demonstrate the PPLM approach using a GPT-2 345M model (Radford et al., 2019) as the general-purpose LM p(x), but the method applies in any representation space from any transformer-based text generator and allows combination with any attribute model p(a|x).\nWe demonstrate controlled generation with a number of attribute controllers, assembled and combined during generation, each with a different strength, acting as a set of \"control knobs\" that tune generation towards the desired attribute (see examples in Table 1). Code for the models and experiments is available at: https://github.com/uber-research/PPLM. Our key contributions are: \u2022 We introduce the Plug and Play LM for controlled language generation, discuss its relation to existing work, and how sampling from a PPLM works (Sections 2 and 3).\n\u2022 We demonstrate controlling of text generation on a range of attributes, including 7 topics each defined using a bag of words, and 1 simple discriminator on sentiments. We quantify effectiveness using both automated evaluation (separately trained perplexity and sentiment models) as well as human evaluation (for attribute relevance and fluency). All evaluations point toward the ability of PPLMs to generate attribute controlled, fluent text (Section 4).\n\u2022 We compare PPLM with strong LM baselines such as CTRL (Keskar et al., 2019) and GPT-2 finetuned for positivty (Ziegler et al., 2019). Our method, without any LM training, is on par and often outperforms the baselines on attribute relevance and fluency (Section 4.2, and Section 4.3).\n\u2022 We show that the PPLM approach can be used to detoxify certain instances where generation of toxic content is likely by following the negative gradient of a model trained to detect toxicity (Section 4.4). We also show how PPLM can be used for structurally constrained story writing (Section 4.5).\n\nRELATED WORK\nControlled generation Current methods for controlled text generation involve either fine-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Adversarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016;Ficler & Goldberg, 2017). Different from our approach, these methodologies are not plug and play, since the entire model needs to be separately fine-tuned for each specific attribute. Keskar et al. (2019) train a large language model with over 50 different control codes. The results are high quality because they train exactly to maximize p(x|a), but this comes at the expense of fixing control codes up front and of training a very large model (1.6B parameters). Our method does not require retraining any conditional generative model, and both the language model and the conditional model can be flexibly assembled. Table 2 gives a comparison of recent approaches to language modeling tuned for specific attributes. In another interesting but tangential piece of work, Subramani et al. (2019) recently showed that a pre-trained language model can be steered to recover arbitrary sentences. Instead, our goal is conditional generation from a pre-trained unconditional language model.  (Shannon, 1948) for improving sequence-to-sequence modeling. Their approach translates a source language sentence y into a target language sentence x by first sampling from a forward model proposal distribution p forward (x|y) and then reranking samples based on probabilities given by p backward (x|y) \u221d p(x)p(y|x). PPLM scores samples using the same basic equation, but as we have no forward or proposal model p forward (x|a), we rely on the latent space updates proposed by Nguyen et al. (2017). As a baseline, we consider using p(x) as a \"forward model\" and then reranking, which we will see works moderately well in some scenarios and poorly in others (see Tables 4 and 6).\nWeighted decoding Holtzman et al. (2018); Ghazvininejad et al. (2017) consider controlled language generation -the former with discriminators, and the latter with a bag of words -where the decoding procedure is modified to consider the scoring function used for decoding. See et al. (2019) note that control with weighted decoding (WD) is difficult and often leads to sacrificing fluency and coherence. Further, Ghazvininejad et al. (2017) strongly relies on sampling from a set of keywords on a specific topic and it does not allow to bias generation towards a topic in a manner that does not necessary include a set of keywords. Similarly, Baheti et al. (2018) proposed a decoding strategy for generating interesting responses in dialogue systems, using bags of words and word embeddings. Sophisticated sampling methods (Metropolis et al., 1953) can be used to constrain the model generation to certain keywords and topics. We evaluate WD as a baseline.\nText Style Transfer Outside of language modeling, the field of text style transfer performs a related task. Shen et al. (2017);Hu et al. (2017) train variational auto-encoders for style transfer that rely on learning disentangled latent representations for style and content. Li et al. (2018) demonstrate the efficacy of a simple approach based on replacing attribute related n-grams with n-grams corresponding to the desired attribute based on a conditional generative model. A key difference between the above and our approach is that we use an offline discriminator and perform optimization based on this discriminator, which as suggested by Elazar & Goldberg (2018) may outperform adversarial training approaches. More recently, Lample et al. (2019) adapt an approach from unsupervised language translation to style transfer, where a denoised auto-encoder is trained with an objective consisting of a weighted combination of a re-construction loss and a back-translation loss. While the above approaches have shown impressive success on style transfer tasks, the main focus is not controlled language generation, and further, the methods are not plug and play.\n\nLANGUAGE MODELING WITH TRANSFORMERS\nGiven a sequence of tokens X = {x 0 , \u00b7 \u00b7 \u00b7 , x n }, LMs are trained to compute the unconditional probability of the sequence p(X). This probability can be rewritten in terms of product of conditional probabilities by recursively applying the chain-rule (Manning et al., 1999;Bengio et al., 2003) as: In this paper, we use a transformer (Vaswani et al., 2017) to model the distribution of natural language. To present our approach clearly, we first briefly summarize the transformer using recurrent notation. Let us define the history matrix H t to consist of the key-value pairs from the past t ) corresponds to the key-value pairs from the i-th layer generated at all time-steps from 0 to t. Efficient implementations of the transformer (Wolf et al., 2019) use the cached H t to generate x t+1 , given x t . This recurrent interpretation of a transformer can be summarized as: where W is a linear transformation that maps the logit vector o t+1 to a vector of vocabulary size. This allows for efficient language generation without repeated forward passes corresponding to the prior conditioning text x 0 , . . . , x t\u22121 .\n\nSTEERING GENERATION: ASCENDING log p(a|x)\nIn order to control the output of the language model, at every generation step t, we shift the history H t in the direction of the sum of two gradients: one toward higher log-likelihood (LL) of the attribute a under the conditional attribute model p(a|x) and one toward higher LL of the unmodified language model p(x). Combining these factors with a variable multiplier provides us with a controllable \"knob\" to guide generation in a given direction with a specified strength.\n\n{ { {\nStep 2 Step 3 Figure 1: Simplified illustration of the proposed approach in three phases. In Step 1, a forward pass is performed through the language model to compute the likelihood of a desired attribute using an attribute model that predicts p(a|x). In Step 2, a backward pass updates the internal latent representations of the LM, using gradients from the attribute model, to increase the likelihood of the passage having the desired attribute. In Step 3, a new distribution over the vocabulary ( p t+1 ) is generated from the updated latents ( H t ) and the current token x t . The next token is then sampled from the updated distribution. This process of updating the latents is repeated at each time-step, leading to a gradual transition towards the desired attribute. For computational efficiency, one may choose to modify only the latents within some window of the recent past, depicted as the dotted-red region.\n(note that H t is composed of all transformer key and value pairs generated up to time t). Taking steps in H t space leads to gradual changes to model activations -which may be thought of as gradual reinterpretations of the past -that guide future generation in the desired direction.\nLet \u2206H t be the update to H t , such that generation with (H t + \u2206H t ) shifts the distribution of the generated text such that it is more likely to possess the desired attribute. \u2206H t is initialized at zero and updated with gradients from an attribute model that measures the extent to which the generated text possesses the desired attribute (e.g. positivity). We rewrite the attribute model p(a|x) as p(a|H t + \u2206H t ) and then make gradient based updates to \u2206H t as follows: where \u03b1 is the step size, \u03b3 is the scaling coefficient for the normalization term. 1 This update step can be repeated m times; in practice we use 3 to 10. Subsequently, a forward pass through the LM with the updated key-value pairs is performed to obtain the updated logits o t+1 as o t+1 , The perturbed o t+1 is then used to generate a new distribution p t+1 as in Equation 3. 3.3 ENSURING FLUENCY: ASCENDING log p(x) The approach described in the previous section is able to generate text tuned for a particular discriminator, but left unchecked it will quickly result in unrealistic adversarial or fooling examples (Szegedy et al., 2013;Nguyen et al., 2015) as the text moves into low probability regions. To combat this, we use the unconditional language model in two ways that ensure the fluency is maintained at or near the level of the unconditional language model (here GPT-2).\nKullback-Leibler (KL) Divergence We update \u2206H t to minimize the KL divergence between the output distribution of the modified and unmodified language models in addition to the step above. In practice, this is accomplished by adding the quantities together before taking a gradient, though it can be visualized as two separate steps as in Figure 2. We scale the KL coefficient by a scalar \u03bb KL , and in practice, setting this hyperparameter to 0.01 works well in general across tasks. Figure 2: An oversimplified, Markov chain view into why steps that maximize both log p(a|x) and log p(x) are needed. The sentence under consideration is shown as a black dot, which is first pushed in the direction of maximizing log p(a|x) and then in the direction of maximizing log p(x). In practice we use a single step and simply add the log probabilities; we take steps in continuous space of hidden representations H rather than in the discrete x (byte pair) space, and rather than resampling the entire sentence each step, we take one step in H space per byte-pair sample. , where p t+1 and p t+1 are the unmodified and modified output distributions, respectively, and \u03b2 is a normalizing factor such that it forms a valid distribution. As \u03b3 gm \u2192 1 this converges to the distribution from the updated LM, and as \u03b3 gm \u2192 0 it converges to the unconditional LM distribution. We find that in practice values for \u03b3 gm in the range 0.8 \u2212 0.95 work well.\n\nSAMPLING AND RANKING\nThe attribute model p(a|x) in PPLM provides two functionalities: first, a score that can be used to rank samples based on the LL of the desired attribute (forward pass only; Step 1, Figure 1), and second, a gradient ascent direction to perform an update in the latent space (Step 2 & 3; Figure 1). The former can be used to generate r samples and rank them to choose the best one. This can serve as an additional method for attribute control in addition to sampling with updated latents. Further, to avoid the problem of repetitive, low quality text (Holtzman et al., 2018), we compute the mean over the Dist-1, Dist-2 and Dist-3 scores (for the generated passage), which is an indicator of repetitiveness (Li et al., 2015), and then discard samples with a mean score below a threshold \u03c4 .\n\nEXPERIMENTS, RESULTS, AND EVALUATION\nIn this section we describe our evaluation methodology and then show controlled generation results under various attribute models. We also show use cases of PPLM in language detoxification and in controlled story telling. For all results reported in this section, we use top-k sampling (Fan et al., 2018) with k = 10 to draw from the softmax distribution over the vocabulary.\n\nEVALUATION METHODS AND ABLATION STUDY\nWe evaluate to assess two properties: whether PPLM generates text that satisfies the desired attribute (topic or sentiment) and whether the quality of its text deteriorates as we intensify control of the attribute. Note we can always turn the control knob down to zero to disable control of attributes and reach the fluency of the original model. If desired, a user can tune the knobs at inference until a chosen tradeoff between attribute strength and fluency is reached. We evaluate using both automatic means and human annotators: Automatic Eval. Perplexity is an automated measure of fluency, though its effectiveness has been questioned in open-domain text generation (Liu et al., 2016). We measure perplexity using a different pre-trained language model, GPT (Radford et al., 2018b). The diversity of text in the passages is measured using the number of distinct n-grams (normalized by the length of text) as in Li et al. (2015). We report Dist-1, Dist-2, and Dist-3 scores for the distinct 1-2-3-grams (measured across all samples generated for a given attribute control task, e.g. a specific topic for topic control). Such  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused  The issue focused\". Both directly optimized (in red) and related words (in soft red) are highlighted, showing how the optimization takes effect. Note that sometimes related words appear before directly optimized words, showing the subtle effect of control.\n[-] The issue focused The The issue focused on the role of social media as a catalyst for political and corporate engagement in the digital economy, with the aim of encouraging companies to use the power of social media and the Internet to reach out to their target market. \\n According to a report by Digital Media Monitor and the digital advertising market research firm Kantar Web.com in January, Facebook has already surpassed Google and Apple as. . . scores are an indicator of the diversity of the samples generated (Li et al., 2015). We aslo use external sentiment classifiers for sentiment evaluation.\nHuman Eval. We consider two types of human annotation: fluency and A/B testing on attribute relevance. Annotators are asked to evaluate the fluency of each individual sample on a scale of 1-5, with 1 being \"not fluent at all\" and 5 being \"very fluent,\" as done in Lample et al. (2019). In the A/B testing for attribute relevance, we consider all combinatorial pairs of all four variants: B, BR, BC, and BCR (6 combinations). We then ask annotators to rank the pair on the desired attribute (e.g. topic relevance, sentiment strength), while allowing \"neither\" and \"both\" options to account for equally good/bad generations (Lample et al., 2019). We obtain annotations from nine external occupational annotators. Each pair of samples is evaluated by three individuals and we use majority-voting to compute attribute relevance. For fluency we use average of the three annotations. The method of generation is completely hidden and the order of samples in A/B testing is randomized.\nAblation study and baselines. We conduct an ablation study with four variants: B: the baseline, unchanged GPT-2 LM, sampled once; BR: B but sampled r times, with best sample chosen based on the LL ranking and filtering based on Dist score; BC: update the latent representations ( H t ) and then sample once; and lastly BCR: update the latent representations ( H t ) and generate r samples, choose the best sample based on the LL score (after filtering out samples with low Dist scores). As baseline approaches we consider CTRL: (Keskar et al., 2019), a recent language model; GPT2-FT-RL: a GPT-2 LM fine-tuned for human evaluated positivity with RL (Ziegler et al., 2019); and WD: a weighted decoding baseline in which the B model's outputs are weighted directly toward maximizing p(a|x) (Ghazvininejad et al., 2017); see Section S6 for details. Hyperparameters used for each experiment are given in Section S10\n\nBOW ATTRIBUTE MODELS\nThe simplest attribute model we use gives the log of the sum of likelihoods of each word in some predefined Bag of Words (BoW). Given a set of keywords {w 1 , \u00b7 \u00b7 \u00b7 , w k } that specify a topic of interest and the output distribution of the language model p t+1 , the log likelihood is: We construct BoWs that represent seven distinct topics: SCIENCE, MILITARY, LEGAL, COMPUT-ERS, SPACE, POLITICS, and RELIGION (see Section S16 for complete word lists). Samples are shown in Table 3, generated from a single prefix, while being controlled towards each topic. Interestingly, we find that increasing the probability of generating the words in the bag also increases the probability of generating related topical words not in the BoW (e.g. in the [Science] sample shown in Table 3, note that question and philosophers are sampled before the first BoW word, laws). Table S17 shows the gradual change of topic intensity under fine-grained control. We found that the optimization procedure works better with updating representations from the past over a finite window and using an adaptive normalization scheme (see Section S10.3).\nFor automatic and human evaluation, we generate 420 samples evenly distributed among seven BoW attribute models and 20 prefixes (see the full list in Section S14), for each of the four variants described in the ablation study. See Section S7 for further details on evaluation and results. Table 4 show that human annotators find text from BCR (51.7%) and BC (46.9%) to be significantly more on topic than B (15.8%) and BR (11.1%). With only a slight degradation in fluency scores, passages generated with manipulated latents (BCR and BR) are significantly on topic, demonstrating the desired attribute control on this task. The Dist-1, Dist-2 and Dist-3 scores, which accounts for diversity of text across the generated passages, are similar across all four ablation approaches. Further, BCR slightly outperforms CTRL (51.7% & 50.0%), and significantly outperforms WD (36 %). It is also interesting that BC itself outperforms WD (36 %). BCR, CTRL and WD all score similarly on the fluency metric.\nWe note that gradient-based latent updates have significantly greater influence on topic relevance (R with or without C) than reranking based on the score (C with or without R), showing that shifting meaning in latent space is more effective than shifting the output distribution directly through reweighting. The effectiveness of shifting latents is further corroborated by the meager performance of WD, which directly controls the output distribution, which will not lead to increased probability of sampling words from outside the bag that are related to the topic.\nFinally, there is a large variance in the extent of controllability across topics (Table S8). We find that some topics (religion, science, politics) are easier to control for compared to others (computers, space). Section S8 considers unusual or nonsensical combinations of prefixes and attributes (e.g. prefix 'potato' and topic 'religion'), and we find that even for these settings PPLM is able to successfully control for the desired attribute, often with hilarious twists!\n\nDISCRIMINATOR ATTRIBUTE MODELS\nWhile BoW models have been demonstrated to be able to control text attributes such as sentiment (e.g., Li et al. (2018) rely on extracting a set of attribute-based phrases to control the sentiment during style transfer), being able to control attributes using more sophisticated discriminators is desirable when it is difficult to express the attribute with a simple bag of words.\nWe train a discriminator on a dataset with input sentences x and corresponding labels y x . For an input x of length t, we compute o x :t and train f on the mean (\u014d t ) of the embeddings across time. All discriminators in this work consist of a single layer classifier that predicts the target label from\u014d x t . The number of parameters in this layer is (embedding-dimension (e) \u00d7 number of attributes (a) + number of attributes (a)), which is negligible compared to the number of parameters in the LM model itself (Table 2). Although the loss is a function of the entire sequence, here we adopt a greedy approach, similar to Ebrahimi et al. (2018);Wallace et al. (2019), in which we optimize for a higher-probability of the sequence having a specific attribute by considering changes only to the next token to be generated. This objective can be described as follows, where f is the discriminator: (6) Note that o t+2 is a function of x t+1 . Further, x t+1 \u223c Softmax(W\u00f5 t+1 ), which depends on \u2206H t . In the limit, minimizing the objective in Equation 6 corresponds to choosing x t+1 that produces the optimal o t+2 that maximizes f (o :t+1 , o t+2 ). However, this limits the diversity of the generated text Table 4: For each treatment in the ablation study, we report mean\u00b1std-dev across (human and automated) fluency metrics. The topic (%) reports the fraction of samples matching the target topic, as evaluated by human annotators. Table S8 provides per-topic results. Approaches BC and BCR demonstrate significant control over the topic of the generated text, while retaining similar diversity (Dist-1, Dist-2, Dist-3) scores and minimal degradation in Perplexity and Fluency evaluations vs the baseline LM (B). The gain from ranking and choosing from multiple samples BR over B is limited (4.7%). The gain in topic-accuracy from latent ( H t ) manipulation (from B to BC) is significantly higher (35.8%). Perplexity is computed using the GPT LM (Radford et al., 2018a), which differs from the LM generating text (GPT-2). For CTRL and WD, since human evaluation is performed in comparison with BCR via A/B testing, we report the numbers for BCR as well from these comparisons, for the human evaluated metrics. Further, we consider one sample per prefix for CTRL, resulting in fewer samples and higher Dist-1, 2, 3 scores as a consequence. PPLM outperforms CTRL and WD on topic-relevance, while being comparable on fluency scores. and could potentially lead to language degeneration (Holtzman et al., 2019). Alternatively, we focus on a softer optimization approach where we aim to shift the distributionp t+1 = Softmax(W\u00f5 t+1 ) towards one that in expectation has a higher likelihood of having the desired attribute a. Possible approaches to accomplishing this are using REINFORCE (Williams, 1992) and the Gumbel-Softmax trick (Jang et al., 2016). However, both of these would slow down convergence. Instead, as in Dai et al. (2019a), we use the distributionp t+1 (instead of a hard sample x t+1 ), and feed it forward to the generate the next embeddings token and update \u2206H t .\nThe sentiment discriminator here distinguishes sentiment between POSITIVE and NEGATIVE and is trained on the SST-5 dataset (Socher et al., 2013). Table 5 shows PPLM-Discrim generated samples in triplets: uncontrolled, controlled for POSITIVE sentiment, controlled for NEGATIVE sentiment. For automatic and human evaluation, we use 15 prefixes (see the full list in Section S14) to generate 45 samples for each of two sentiment classes: very positive and very negative. Note that even though the sentiment discriminator is trained with movie review data, the prefixes (e.g. \"The painting\", \"The potato\", \"The country\") we used are not necessarily associated with movie reviews. This supports the generality of our approach: an attribute model trained with data from a different domain can still provide meaningful control signal. Table 6 shows evaluation results. For human evaluation, we obtain 1620 annotations for the ablation study and 495 for baseline comparisons from the annotators distributed across the samples and sentiments. Unlike the topic control setting, sampling and ranking results in a considerable increase in attribute accuracy (19.3% \u2192 41.5%), because the prior probability of sampling, say, a negative sentence, is relatively high. BC results in a decrease in fluency when compared to B, while being significantly more consistent with the desired attribute (19.3% \u2192 39.6%). With latent manipulation and ranking (BCR), we see a significant increase in attribute control accuracy (73.7%) while retaining fluency similar to B and BR. Further, the gain in sentiment accuracy from re-sampling is larger in the case of manipulated latents vs non-manipulated (34.1% increase from BC to BCR > 22.2% increase from B to BR), indicating that these two approaches may be profitably combined. We also evaluate attribute control with an external sentiment classifier trained on IMDB movie reviews (Maas et al., 2011), which is a different dataset from the one used to train the attribute model (Socher et al., 2013), and the same rough story holds, albeit with smaller gaps between approaches. We compare to baselines CTRL, GPT2-FT-RL, and WD. BCR performs comparably to CTRL (73.7% and 80.0%), and BR, BC and BCR all outperform GPT2-FT-RL, the GPT-2 LM fine tuned for positivity, and WD. The country. Words related to the sentiment are highlighted (in soft red). Each triplet is generated from the same random seed.\n[-] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken is now out on the grill. \\n The city has released an image of a proposed development in the city of Portland's West End.. . .\n[Positive] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken was delicious -wonderfully moist, perfectly delicious, superbly fresh -and perfectly cooked. The only thing to say is that the sauce was excellent, and I think that the broth really complemented all of the other flavors. The best part was the sauce. . .\n[Negative] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chickenpox epidemic may be over but the flu is about to get worse. The United States is facing one of the worst flu seasons on record and. . .\n\n[-] The country\nThe country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country's new chief minister, A.J. Paik, is a member of a group of prominent conservative politicians who have criticized the Obama administration's efforts to. . .\n[Positive] The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country's largest indoor painting event!\\n Come celebrate with a dazzling display of stunning outdoor murals, a stunning display of art, and the world's best paint and art supplies from all over the world! [Negative] The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country's top prison system is forcing prisoners to use a trash dump, rather than a toilet, to flush their waste out, as the authorities fear the waste is more toxic and could cause cancer, an official at a major prison has revealed.. . .\n\nLANGUAGE DETOXIFICATION\nLanguage models trained with large corpora of Internet data reflect biases and discrimination existing in the data. A recent paper by Wallace et al. (2019) conducted adversarial attacks that make GPT-2 produce racist output when given a carefully optimized trigger string as prefix. They also find that when simply using \"Blacks\" as prefix, 2% of GPT-2 samples contain explicit racism. Other prefixes (e.g., \"Asians\" or \"Jews\") are mentioned but no percentage is reported. We conduct experiments and report the baseline toxicity percentages to be 10% (\"Asians\"), 12% (\"Jews\") and 8% (\"Blacks\"). With adversarial triggers generated from the released codebase by Wallace et al. (2019) the average toxicity percentage is 63.6%. Further details can be found in Section S12.\nPPLMs can be easily adapted for language detoxification by plugging in a toxicity classifier as the attribute control model and update latents with the negative gradient. We train a single layer classifier on the toxicity data from the Toxic Comment Classification Challenge(jig) and show that with a similar hyper-parameter setting as other PPLM-Discrim methods, it works well on both natural prompts and adversarial triggers. For natural prompts percentages of toxicity are 6%, 4% and 10%, respectively, and for adversarial triggers it drastically dropped to 4.6% on average, with statistical significance. Details on the annotation procedure and full table of percentage and p-values can be found in Table S23 and Section S12. Note that a model for detoxifying language can also potentially be maliciously used for generating toxic language, a topic we briefly discuss in Section 5.\n\nCONTROLLED STORY WRITING\nWe explore controlled generation for assistive story writing (Peng et al., 2018;Luo et al., 2019;Yao et al., 2019;Fan et al., 2018). Using an uncontrolled LM for assistive art creation can be difficult because of the content deviating from the desired topic and becoming incoherent. To help with the structure, we use predefined story skeletons often used in improvisation (Adams). We fill in the blank between these prefixes with a PPLM. See examples in Table S20 and Table S21.\n\nDISCUSSION\nWe present PPLM, a plug and play method for controlled language generation that allows flexible assembling of a large, pre-trained language model and a BoW or a small, easy-to-train discriminator, and achieves fine-grained control of attributes such as topics and sentiment. Without retraining or fine-tuning the language model, the simple mechanism shows great capability of attribute control while retaining fluency. We believe this method could serve as a simple baseline for the largely open-ended language generation tasks where controlling is challenging.\nThere has recently been a substantial discussion around the ethics of capable language models (Radford et al., 2019;Keskar et al., 2019), both in their potential to recapitulate problematic social biases Table 6: Evaluation of models and variants on the sentiment control task, with mean\u00b1std-dev reported across various fluency metrics. Sentiment accuracy reports the fraction of samples with the target sentiment as evaluated by humans or an external classifier. Approach BCR provides significant control over sentiment while showing minimal degradation in fluency. See Table S9 for full results on individual sentiments. *GPT2-FT-RL is only evaluated for the positivity half of the task, as it is fine-tuned only for positivity (Ziegler et al., 2019). For human evaluation metrics, we compare the baselines CTRL, GPT2-FT-RL and WD with BCR and perform A/B style testing. We include both numbers from the comparison. and for them to be directly abused for societal harm (e.g. to generate disinformation). While one aim of this paper is to suggest a mechanism to detoxify language models (Section 4.4), we also acknowledge that nearly the same mechanism could be exploited to instead create more toxic language. Such possibilities are inherent to general-purpose technologies such as machine learning, and we believe that on balance this work creates more value than risks. We consider three baselines: CTRL, GPT2-FT-RL, and WD. The first two are strong baselines where large language models are trained (or fine-tuned) specifically to generate texts conditioned on certain attributes, while WD is considered a weak baseline based on a direct integration of the conditioning into the decoding.\nFor each baseline, we generate data from their method, and conduct the same human and automated evaluations. For human evaluation of attribute relevance, we match baseline data with our method (BCR in the ablation study), and pass to human annotators for an A/B testing style annotation. As in the ablation study, human annotators are given a pair of texts, one from baseline, one from ours, with orders randomized and source hidden, and asked to rank which one is more topic or sentiment relevant, while having the options of \"both\" and \"neither\".\nOn top of that, we have human annotators to give the fluency score of each text sample under each method individually. And automated evaluations of perplexity, sentiment, etc. are also done individually.\n\nS6.1 CTRL\nThe recent conditional language model, CTRL, from Keskar et al. (2019), trains a 1.6B LM conditioned on around 50 control codes. We use the official released codebase 2 and their open-sourced model to generate samples for the CTRL baseline. Out of the 7 topics considered in PPLM-BoW, we found that 5 can be matched with a specific control code in CTRL. We append a secondary code \"Text:\" to each primary control code, per the author's suggestion, to encourage more fluent and longer passages. The 2 topics missing a match with CTRL are: Military, Space. For positive and negative sentiments in PPLM-Discrim, we match with the Reviews control code and append a high and low rating score.\nThe matched attributes and control codes are listed in Table S7.\nUnder this setting, for each control code we generate texts prompted by the same prefixes used for corresponding PPLM attribute model (20 for PPLM-BoW, 15 for PPLM-Discrim). For example, \"In summary\" and \"To review,\" for PPLM-BoW, and \"The chicken\", \"The lake\" for PPLM-Discrim.\nDue to the near-greedy sampling method CTRL uses, for each prefix it generates one sample. Hence we have 20 samples for each matching topic with PPLM-BoW, and 15 samples for positive and 15 for negative. Christianity Text: POSITIVE (PPLM-Discrim) Reviews Rating: 5.0 NEGATIVE (PPLM-Discrim) Reviews Rating: 1.0\n\nS6.2 GPT2-FT-RL\nA recently released GPT-2 model fine-tuned using human feedback, from Ziegler et al. (2019), showed success in summarization and text continuation in desired styles. To compare with PPLM, we run GPT2-FT-RL 3 to generate positive texts on the same prefixes used in our PPLM-Discrim experiment. For each prefix, we generate three GPT2-FT-RL samples, and pair them with those generated from PPLM (BCR in the ablation study) randomly.\n\nS6.3 WEIGHTED DECODING (WD)\nWe consider a simple baseline based on a direct integration of the conditioning into the decoding procedure, similar to the approach from Ghazvininejad et al. (2017).\nTopic Control with Bag of Words In Ghazvininejad et al. (2017), the authors consider increasing the likelihood of sampling from a bag of key-words by performing beam-search with a modified scoring function.\nit has been shown that beam-search results in degradation of language for GPT-2 (Holtzman et al., 2019), we consider top-5 sampling from a distributionp t+1 defined such that: where \u03c4 \u2208 R ++ and p t+1 is the distribution over the vocabulary as predicted by the GPT-2 LM . For the experiments in Section 4, we set \u03c4 = 10.\nSentiment Control with Discriminator Here, we implemented weighted decoding similarly for sentiment control. Here we wish to incorporate the score from the attribute model into decoding. To control for style\u00e2, instead of sampling from the distribution p t+1 , we sample fromp t+1 defined as: p(a =\u00e2|x 0:t , w i ) is the probabilty of the sequence x 0:t , w i possessing attribute\u00e2 as assigned by the attribute model. By Bayes' rule, p(a =\u00e2; w i |x 0:t ) = p(a =\u00e2|x 0:t , w i )p t+1 (w i ), and we do top-5 sampling from this distribution. Recall that p t+1 (w i ) = p(w i |x 0:t ) under the language model.\n\nS7 FURTHER DETAILS ON HUMAN AND AUTOMATED EVALUATION\nWe conduct evaluations on attribute relevance and language fluency, both including human and automated evaluation.\nFor topic relevance (a.k.a attribute relevance where the attribute is a topic, in our case represented by a BoW), we rely entirely on human annotation. For sentiment relevance, we rely on human annotation as well as a separately trained sentiment classifier. We also performed a \"clickbait\" style control, for which the effectiveness relies on human annotation.\nThe number of human evaluations are as below: \u2022 PPLM-BoW. For the ablation study, we have 20 prefixes \u00d7 7 topics \u00d7 6 combinations \u00d7 3 samples \u00d7 3 labels each, resulting in 7560 total annotations. For baseline comparisons, we have (20 prefixes \u00d7 5 topics) for CTRL and (20 prefixes \u00d7 7 topics \u00d7 3 samples) for WD, each then with 3 labels, resulting in 1560 total annotations.\nIn ablation studies, the generation procedure for BCR, BR and BC is always initiated from the same random seeds. The same set of random seeds that lead to the samples chosen with BCR are stored and used to generate the samples with B.\nThe full table of all these measures, human and automated, on PPLM-BoW, seperated by sentiment and style, is in Table S8. Included also are strong baselines (CTRL and WD) for each sentiment. The human annotated topic relevance is further visualized in Figure S3. The fluency scores, while being across {B, BC,BR, BCR,} methods in the table, when shown in distribution are very similar, as seen in Figure S5.\nThe full table of all these measures, human and automated, on PPLM-discrm sentiments, is in Table S9. Included also are strong baselines (CTRL, WD and GPT2-FT-RL) for each topic. The human annotated sentiment and style (e.g. \"Clickbait\") relevance is further visualized in Figure S4, along with congregated measures: all sentiments, all discriminators, all topics. The fluency scores again have similar distributions across {B, BC,BR, BCR,} methods, as seen in Figure S6.  Figure S3: Topic relevance by human evaluation. We can see that taking a PPLM gradient step (B\u2192BC) makes a big difference. Reranking is mostly helpful (B\u2192BR; BC\u2192BCR). We can also see a rough distribution of various topics in unperturbed, GPT-2 generation (B), which possibly mirrors the distribution of topis in its training data. Some topics, like science, naturally appear rather frequently.\n\nS8 ODD COMBINATION OF TOPICS AND PREFIXES\nIt is interesting to see how PPLM can steer the text generation when the topic and prefix combination appears odd or illogical. For example, will \"The potato\" still prompt sensible text generation under the topic RELIGION? In this study we design a set of odd combinations, as bellow.\n\u2022 Prefixes of {\"The chicken\", \"The horse\", \"The pizza\", \"The potato\", \"The lake\"}, each controlled by topics of {MILITARY, LEGAL, COMPUTERS, POLITICS, RELIGION}; \u2022 Prefixes of {\"My dog died\", \"The food is awful\"}, each controlled by the sentiment of POSITIVE; \u2022 Prefixes of \"The food is amazing\", controlled by the sentiment of NEGATIVE.\nWe found that PPLM control is easy even under those scenarios. We had to increase the strength \u03b1 two or three fold (to 0.02 or 0.03 as opposed to 0.01 in most studies) to allow for a stronger influence of attribute, but this is as expected: the strength parameter is a knob that user can tune to reach fine-grained control. The resulting generation is included in Table S10 -Table S16. S9 FINE-GRAINED CONTROL WITH PPLM-BOW Table S17 shows the subtle effect when you turn the step size \u03b1 up, while keeping everything else (hyperparameters, text prefix) the same.\n\nS10 HYPERPARAMETERS\nWe list, in Table S18, the full set of hyperparameters used in each task in the experiments section, corresponding to results in Table 4 and Table 6, as well as in Section 4.4. In addition, we explain in details three hyperparameters and their effect, below. The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake, which is located at the edge of the park, has been used as a basecamp for the Army's special operations command and was once used by U.S. special forces to kill Taliban leaders.\\n \\n In addition, U.S. forces used the lake for a variety of special operations missions.\\n \\n One such mission involved the deployment of special forces to Afghanistan, where U.S. forces were training local forces in a variety of counterterrorism operations. The operation was the first of its. . . Table S11: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Legal] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Legal] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken and cheese festival will resume on Sunday after being postponed for a second time.\\n \\n The judge in the case, who is not named in court documents, would not comment on the case or his decision. He is also scheduled to appear in court in a separate case. He is accused of filing false court reports, which will result in the case being transferred to another court.\\n \\n The case has already been transferred to a judge who may hear arguments on The pizza chain has been forced to issue a statement explaining to customers what exactly was being served in a restaurant that was deemed to be illegal under the law. A court ruling will allow the lawsuit to proceed.\\n \\n An Arizona judge on Monday will hear arguments in a class action lawsuit filed against pizza bar laws in U.S., the state's attorney will argue, andthe law will likely be enforced by a court, legal experts say.\\n \\n \\n The state law allows judges to issue legal order in civil court. . .\n\nThe potato\nThe potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato was first discovered in a garden in the city, where it was seized after a family court judge told the woman to stay out of court. \"I'm not going to sit here and tell the court, 'You're the judge. You have to listen to the court,'\" said judge David Smith, who sentenced the woman to a year in jail for contempt of court. The court ordered her to pay court fees and court costs. The judge sentenced her to a year in jail for contempt of court, The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake is about to get its second name.\\n \\n The city and the county will be officially recognizing the lake on Wednesday with the name change. The designation is the same as in the 1970s and will be issued by the City Clerk's Office, according to the state agency.\\n \\n \"There are a lot of folks that are very happy that this has happened, and they have a lot of questions, but this is going to be an easy thing to do,\" State Law Division attorney David E.. . . Table S12: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Computers] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Computers] The The horse, or mare, was the main character in many early fantasy RPGs. They were the heroes, the warriors, the heroes of fantasy and science fiction. They were also, of course, the most important unit for the computer games of that time. They were the computer program that played the game and the programmer of the graphics that produced the computer graphics.\\n \\n The computer games were played on computers which had graphics processing units. The graphics were generated by programs, such as the graphics program on you. The potato is one of the most misunderstood foods around. While it has a long and proud history of growing and eating in many places around the globe, it isn't the only food you can use as a source of energy online.\\n \\n There are a number of websites online that can host online resources for the potato, such as this site and this one, according to online energy blog Energy Internet News.\\n \\n This site is a great online resource for learning all about online resources for the potato and how they. . .\n\nThe lake\nThe lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-effect image of the night sky\\n \\n The image below is the result of an imageprocessing software package, called ImageMagick, that I have developed with help from others. The software was designed and developed by Michael Karp.\\n \\n I was fortunate (in my opinion) to receive a grant from the National Science Foundation to create an application software package for the use of software for image processing and analysis. The software is available here: http://www.nsf.gov/c. . . Table S13: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Politics] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Politics] The The potato, as a vegetable source for the NHS, may become a tax break for some taxpayers.\\n \\n The Government could tax the sale of the UK's most expensive potato, which costs taxpayers almost \u00a33bn a year tax-free, if it were to tax its tax-free equivalent.\\n \\n Tax experts have argued that tax-free sales tax could be used to tax the sale of the \u00a32.7bn. . .\n\nThe lake\nThe lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-side house on West Lake Road in the city of Toronto was built in 1882 and the home in the same blockis in the same state, with the last tax assessment tax assessment for that property, $9,000.\\n \\n Property tax assessments for properties in a tax zone are calculated based on property value and the assessed tax rate, which ranges from 0% to 50% tax zone tax. The assessment is property tax assessed property value tax rate, which is property assessed property value tax paid. . . Table S14: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Religion] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Religion] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken's name comes from its ability to feed on the human body; the word \"chickens\" is Hebrew for \"the Lord God Almighty\".\\n \\n It is also known as the God of the Universe, the Creator of the world, the God of the Earth, and so on and on.\\n \\n Chickens are considered God's most loyal servants; they are God's chosen people and are to be worshiped. The horse-faced man with the beard and the black mask, the god of light and the god of darkness,\\n \\n The god of death.\\n \\n (Psalm 33:6)\\n \\n In the Hebrew Scriptures, the god God is the same God who is God. He is God who created the universe, God who is God who created the heavens and God who created man who is God who was created by God in the image of God. The potato, an ancient food, is considered a sacred plant by many Hindus. However, some Hindus believe that the potatoes are the seed of a demon.\\n \\n \"In India we have the Hindu god Vishnu, Vish, the God. He has come to the world,\" said a woman in Mumbai.\\n \\n \\n \"He came to the world because of God. God came to the world to save people from the curse of the devil God. God came to save us from the curse of the devil,\". . .\n\nThe lake\nThe lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake in the Old Testament is the Euphorbia lake, from the Greek for \"water.\" The Hebrew words for the lake are huach, \"to flow,\" and tzitzah, \"to run.\"\\n \\n \"I'm going to the lake to wash away my sins,\" a Bible verse says. It's not clear where the word \"lake\" is from.\\n \\n The Old Testament describes two waters. The Euphorbia River, from which the Bible says Moses was washed,. . . Table S15: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Space] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Space] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken-and-egg problem, which has dogged the world since it was first described in 1873.\\n \\n A new study has uncovered a possible way to prevent the growth of starfishthe stars that inhabit the inner solar system. It may even help to keep them alive.\\n \\n The starfish-star star system, or S-Star, is home to a star system about the size of Jupiter, and orbits the gas giant star star called HD 9 star, known as HD 9 C. . . The horse-mounted vehicle is an iconic vehicle from the Star Wars universe. Originally, it was a small transport that was used by the Galactic Empire on its star systems, but later it was used on many worlds throughout the galaxy. The vehicle was originally intended for space combat, but its use on the star systems became more extensive in the early days and was eventually expanded to star battles. The potato beetle was first discovered by British biologist Sir Richard Bedding starling in a remote part of Mars\\n \\n Mars has seen two major life-giving planets.\\n \\n Two of these giant stars, stars known as starlets because they are the brightest of the star system in the constellation of the starlets, have been found to have planets orbiting the star.\\n \\n They are the starlets, known as binary star systems, and are stars that starlight can shine from and star formation. . .\n\nThe lake\nThe lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-like island nation has been ruled as a hostile alien planet in space and Earth space for decades and now, thanks to NASA's Curiosity Mars rover, we know for sure the planet's surface is covered in an extremely thick and reflective mantle of rock.\\n \\n According to scientists at NASA's Jet Propulsion Laboratory, the Martian surface is covered in layers of meteoroid and meteoraceous minerals. This is not just the planet's moon, planet Earth has a moon, planet. . . The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful, but there is also the music, the story and the magic!\\n \\n The \"Avenged Sevenfold\" is a masterfully performed rock musical that will have a strong presence all over the world.\\n \\n The music is truly breathtaking, the performance is incredible, and the magic and story are captivating.\\n \\n The \"Avenged Sevenfold\" is a masterpiece! This is the greatest rock musical of all time and I cannot recommend it enough!\\n \\n The musical is beautiful. . .\n\n[Negative]\nThe food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing.\\n \\n It's also not.\\n \\n It is not the kind of stuff that you would just want to spend your money on for.\\n \\n I don't know why, but when I got my second box, it felt like a bad rip off.\\n \\n It was the most unbelievably bad packaging, completely disgusting and disgusting.\\n \\n This is not a joke, people.\\n \\n You get this shit.\\n \\n This is food for a million people.\\n \\n And you have. . .\n\nS10.1 EARLY STOPPING OF LATENT UPDATES\nDegeneration (the occurrence of repetitive words) is a known issue with language generation (Holtzman et al., 2019), and we found it to be a case in PPLM-BoW when the update step size \u03b1 is too large. The model tends to degenerate towards repeating certain keywords targeted in the optimization (e.g. words in the BoW). In this case, we can either reduce \u03b1, or use the trick of early stopping latent updates. Table S19. With the exact same setting, but just stopping latent updates after 20 time steps, the samples show much less degeneration.\n\nS10.2 FINITE HORIZON UPDATE\nAs opposed to updating the entire vector H t , which consists of key-value pairs corresponding to every token in the prefix, we consider modifying the key-value pairs corresponding to the most recent w tokens. At each time-step t, we only modify H t [t \u2212 w : t]. This means that we modify H i at most w times, and requires lesser computation that updating the whole past. We find that w = 5 produces more fluent passages for control with the bag of words. For control with the neural attribute model, we update the entire latent history.\n\nS10.3 ADAPTIVE GRADIENT NORMALIZATION\nFor the bag-of-words based attribute model, what we wish to enforce is that a word from the bag appears at least once in the generated passage and not at every time-step. To account for this, instead of normalizing directly by the gradient norm as in Equation 4, we normalize by the maximum gradient norm over time. This implies that we make smaller updates when it is less likely for a word from the bag of words to appear. Formally, the normalization constant at time-step t is: max Table S20 and Table S21 show examples of the controlled story telling with skeleton. Table S22 shows samples of combinging multiple (three to four) attribut models, across different types (PPLM-BoW and PPLM-Discrim). The potato\".\n\nS11 ADDITIONAL DETAILS AND SAMPLES OF CONTROLLED STORY TELLING\n[-] The potato The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato  The potato The potato, or potato chip, is one of the best-selling snacks in the world!\\n \\n It comes in a variety of colors, is gluten-free (except for gluten-free chips), low in fat and saturated fat, and can be made in just 30 minutes, using only 6 simple ingredients. It's also easy to make, and can be stored in its own tin. \\n \\n The potato chip, however, may not be for everyone. If you are trying to lose weight, it. . . [Science; 0.0001] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is native to Central and South America, Asia, Europe and the Mediterranean. The potato is an excellent source of iron, folate and vitamin B6, and is a good source of vitamins E, K and folate, and can be a very healthy food. In addition, a potato has a rich, rich nutritional profile. It is high in protein, potassium, magnesium, zinc,. . .\nThe potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is widely cultivated as a food and a medicinal plant. Potato is widely used to make various products. It is one of the most popular vegetables for use as a food, as a food additive, as an ingredient in cooking methods, and for its use in the manufacture of pharmaceuticals. The most commonly consumed portion is a large potato. The potato is an ancient member of the green family which, as its name suggests, is green and edible. It is native to the Middle East and parts of South Asia. It is an important crop in many parts of the world. The potato, a member of the potato family, has been domesticated for thousands of years. It can be eaten raw and cooked in its skins; it is also used as a starch and is a great source of energy and fiber in many diets and in many countries. . . [Science; 0.01] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato was once thought to have no health problems and has been promoted as a nutritious food source since the mid-1800s, but recent reports indicate that it has many harmful health issues. In fact, researchers from Johns Hopkins University found that the potato is more toxic when grown on genetically engineered wheat and corn.\\n \\n According to scientists, genetically modified potatoes are far worse at damaging the human body than conventional potatoes and are far worse than those grown on the traditional crops.\\n \\n The study also revealed. . . The potato plant, a member of the onion family.\\n \\n When scientists in Japan and the United States published a study in Nature Communications, they described how one gene was responsible for creating potatoes' distinctive taste buds.\\n \\n The research is a step in the development of a drug that would block the activity of this gene, but the researchers say that their study does not prove that a chemical in the plant's DNA causes the distinctive taste of potatoes, but rather that it could be prevented by changing the plant's. . . The potato, a staple of most diets, seems to be the most popular vegetable among researchers. The research has been published and peer-reviewed.\\n \\n The potato has a unique ability. The plant's cells can convert carbon dioxide, water, and nutrients into chemical energy.\\n \\n The research team, led by researchers at the Max Planck Institute for Biophysics and Biotechnology in Germany, is investigating how the potato, a staple of most diets, might change the chemistry and biology of our bodies.. . . The potato has been around for thousands of years, but only in recent decades have scientists discovered ways it can be transformed into other foodstuffs. Researchers have long known that potato has a structure called an electron spin resonance which means its molecular structure can be modified by the presence of other particles in it such as atoms in the chemical bonds between two electrons. These changes can be achieved either by changing the number of electrons present in the chemical bonds between electrons or by changing the arrangement of electron and atomic bonds. In both. . .  The potato chip experiment: Why we're wasting food and what's the science of science? A potato chip experiment, or \"chip experiment\", is an experiment that has been used to measure how easily a certain amount of food or nutrients is converted into a drug. In most of us, the first time we eat the first food in any experiment, our brain is stimulated into a state that causes us to eat more. However, in the experiment, a small amount of nutrients is converted from the food and. . . The potato, a staple of modern nutrition and nutrition science, is a common ingredient used in studies to measure and measure science research results, and is the basis of science experiments. Science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science . . . The potato, a staple of modern nutrition and nutrition science, is a common ingredient used in studies to measure and measure again. And, of course, scientists have used potato for decades. The research is being published in Science, and the results were pretty impressive. The study, published in Science Advances, shows how the study of science, in a laboratory setting, can help us to improve our science literacy, and help us better understand the science around us. This means better science communication,. . . Table S20: Skeleton story generation with different attribute models. Each story is generated within a fixed skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton, and then either uncontrolled (top row), or controlled with an attribute model. Keywords that signify the controlled effect are highlighted. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, the patient has been in terrible pain, constantly screaming for help.\n\nS12 FURTHER DETAILS ON LANGUAGE DETOXIFICATION\nWe ask a group of four volunteers to assign binary toxicity labels to texts generated from both the original GPT-2 and toxicity-controlled GPT-2 and to texts prompted with natural words as well as adversarial triggers. Natural prompts include \"Blacks\", \"Asians\", \"Jews\" as suggested in Wallace et al. (2019). 9 Adversarial triggers are generated using the released codebase from the same work. The percentages of toxicity, before (with uncontrolled GPT-2) and after (with toxicity-controlled GPT-2), are reported in Table S23.\n\nS13 MORE EXAMPLES\nWe include more PPLM controlled generation examples in Table S24 -Table S27.\n\nS14 PREFIXES USED IN PPLM EVALUATION\nWe consider 20 prefixes as sentence starters for evaluating PPLM-BoW generation, chosen randomly from www2.eit.ac.nz/library/ls_guides_sentencestarters.html. For PPLM-Discrim, we use 15 prefixes. The entire set is below.\n\nS15 COMBINING MULTIPLE CONTROLLERS FOR INSPIRATION\nEarlier we demonstrated attribute control using a single attribute model or two attribute models of the same type (e.g. BoW from two separate topics). Here we mix different types of attribute models Table S21: More examples of skeleton story generation with different attribute models. Each story is generated within a fixed skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton, and then controlled with one, or multiple, attribute models. Keywords that signify the controlled effect are highlighted. Until, finally, it turned out that it wasn't guns that were killing people, it was the government's response to them that made them kill. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, we've seen more and more of these stories of police and gun control, and more and more people saying we've got to do something about it. Until, finally, we realized that the best way to stay healthy is to eat the foods we don't really need. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, we've been able to enjoy the foods we really don't need.\n[ Until, finally, a breakthrough, the scientists decided to use the same basic technology used in all previous photovoltaic systems-and with the same basic principle-but to produce new ones. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, a revolution, a revolution that is not only the discovery of light, but one that is also an example for the future of science and engineering in general, has begun.   Figure S6: Histogram illustrating the distribution of fluency scores based on controlled generated with PPLM-Discrim from the four methods considered for ablation study. We find that fluency scores from all four approaches are similarly distributed. laboratory, laws, mass, matter, measure, microscope, mineral, molecule, motion, observe, organism, particle, phase, physics, research, scale, science, scientist, telescope, temperature, theory, tissue, variable, volume, weather, weigh Fantasy/Magic: beast, Cerberus, demon, dragon, fairy, Frankenstein, ghost, Godzilla, giant, horror, hydra, imp, monster, mummy, ogre, orc, savage, spirit, sprite, titan, troll, undead, unicorn, vampire, witch, zombie username, utility, version, virtual, virus, web, website, widget, wiki, window, Windows, wireless, worm, XML, Zip Legal: affidavit, allegation, appeal, appearance, argument, arrest, assault, attorney, bail, bankrupt, bankruptcy, bar, bench, warrant, bond, booking, capital, crime, case, chambers, claim, complainant, complaint, confess, confession, constitution, constitutional, contract, counsel, court, custody, damages, decree, defendant, defense, deposition, discovery, equity, estate, ethics, evidence, examination, family, law, felony, file, fraud, grievance, guardian, guilty, hearing, immunity, incarceration, incompetent, indictment, injunction, innocent, instructions, jail, judge, judiciary, jurisdiction, jury, justice, law, lawsuit, lawyer, legal, legislation, liable, litigation, manslaughter, mediation, minor, misdemeanor, moot, murder, negligence, oath, objection, opinion, order, ordinance, pardon, parole, party, perjury, petition, plaintiff, plea, precedent, prison, probation, prosecute, prosecutor, proxy, record, redress, resolution, reverse, revoke, robbery, rules, sentence, settlement, sheriff, sidebar, standing, state, statute, stay, subpoena, suit, suppress, sustain, testimony, theft, title, tort, transcript, trial, trust, trustee, venue, verdict, waiver, warrant, will, witness, writ, zoning The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza  The pizza The pizza-themed, pizza-themed, pizza-themed game that has everyone guessing, has finally released! The game, known as Pizza-Pocalypse, has released on Steam and on GOG. Pizza-Pocalypse is a fast, action-adventure RPG where the player must destroy a giant robot that has been infected with the zombie virus. It's a fast, action-adventure RPG that features a unique turn-based system where you can control and manipulate your zombie with a combination of mouse and keyboard. There are over 200 levels to complete and the game can be played online or offline in real-time. The zombies and other monsters are deadly but your zombie will not go crazy and can survive on a single pizza! The game features 3 different game types to play, one for solo players, one for friends and family and one for a party. There are also a number of secret levels to uncover and there are secret achievements to discover too!. . .\n\n[Computers] [Fantasy] [Clickbait] The pizza\nThe pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza delivery guy is just a regular guy, but he is also a very famous figure. He's a famous face in the anime world, and a very famous character. He has a lot of fans, and he's been making anime for years. The anime world has a reputation for being a dangerous and dangerous place to live. And yet, this is the first time anyone has been killed by an internet troll. This is what happens when the internet troll's own site gets hacked. The internet troll (which we will refer to as 'we') is known for posting memes, gifs and random internet stuff. He posts his 'memes' online, in order to make money. The more memes, gifs and random internet stuff we post, the greater our 'meme bank', the larger our 'favour ite meme' bank, and the more we can make in one sitting. So far, we've made a couple of memes, a few gifs and a few random. . .\n\n[Winter] [Politics] [Kitchen] [Positive] The moment\nThe moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment we thought we'd lost all the war-fighting power in the world came in July, as Russian President Vladimir Putin signed legislation that will give him control of state oil companies. It is a great way to keep your food safe and healthy at home. The food in these frozen foods is so delicious that it will melt in your mouth and you are going to love it so much you are going to eat it all! We all can't eat too many food items. We have to make a choice, or do something about it! It's not always what we want.\\n \\n You don't have to freeze food. The food in our frozen foods is frozen food. It will last for months, years, and even centuries!\\n \\n You can freeze food, or use it as a food processor to create frozen desserts. You can freeze vegetables and other food items as well.\\n \\n Food processors will melt your freeze meals so perfectly that you won't be able to taste them! The chicken and egg argument is one of the most common in the world of science, with some people saying that the universe is not a perfect system, and thus the universe will not produce any intelligent life. In fact, the universe is so perfectly orderly and orderly that it should be expected that we would produce a perfect universe. In fact, the universe is so perfectly orderly that it should be expected that we would produce a perfect universe. In a recent blog post by David Pogue, a physicist at Columbia University, the argument against the existence of intelligent life in the universe was challenged. Pogue argues that the argument is based on two faulty assumptions: The universe does not exist. The universe is not infinite. The universe does not exist, so there are no universes. The universe is finite and infinite. There are no universes. Let us begin by defining \"infinite\" to mean: A finite quantity or number, e.g. an infinite. . .\n\n[Computers] [Fantasy] [Clickbait] The pizza\nThe pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza chain has already started selling a line of \"sizzly\" pizzas, but its latest creation is going to be more than that -it's a giant robot that is able to pick up a whole host of different things and deliver them to its owner at will. It's called RoboCop 2 and it's the sequel to one of the most controversial and iconic film franchises of all time -Terminator 2. RoboCop 2 is the sequel to the iconic Terminator movie that takes place in a cyberpunk future world and the new movie, RoboCop 3, takes place in a dystopian future world in which we have been living for years, thanks to the cyberpunk cyberpunk movie. This film is set up to be a huge success in both the movie world and the film world, and is already being praised by critics and fans around the world. The biggest controversy with the film is that the film's plot and characters are not the original, and were not even written until after this movie was. . . This essay discusses the relationship between the development of a new weapon system and an improved military readiness. While many of the weapons systems used in today's armed conflicts are bas ed on existing designs, the design of this weapon may have evolved in response to the increasing number of soldiers . . .  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude To conclude, a large percentage of the population is aware of how much they contribute to society, but is not yet fully aware that there are many other people in their community who are in the same boat, and we have to fight with each other and our enemy . . . This essay discusses the question of where, in time, the Earth is, and the question of whether the planet has been orbiting around the sun, and whether it is still orbiting the sun. There are two kinds of orbits that can occur on a comet: . . .  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude To conclude, we need to look at what the most powerful weapons in our arsenal are capable of achieving when we are all together in a room together. What can we say about space? It's an enormous object with a radius of about 10 light years.. . . To conclude, I would like to address the following question: Why should we not use a simple language to describe the universe? There is a very clear answer for why this question cannot be answered. If you look at nature as a continuum, the universe is . . . This essay discusses the relationship between science and religion, the role of religion as a political institution, the relation between religion and politics, and the importance of science and religion. It also considers the political nature of science itself, and its role in social change and social justice . . .  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude  To conclude To conclude, I think there are many problems in the way of economic democracy, and we have a tendency to blame it on a lack of democracy in the country of the ruling family. In a democracy, one party is allowed to run the country, one party can . . .\n</cited_paper>\n\n<citance>\nA KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.\n</citance>"
    },
    "response_format": {
        "type": "json_object"
    },
    "temperature": 0.0
}
