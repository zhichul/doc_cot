['Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.']
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Ranzato et al., 2016)
</citation_literal>

<citation_context>
Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Marc'Aurelio Ranzato, S. Chopra, Michael Auli, Wojciech Zaremba",
        "year": 2016,
        "title": "Sequence Level Training with Recurrent Neural Networks",
        "corpusId": 7147309
    },
    "citance": {
        "citation": {
            "literal": "(Ranzato et al., 2016)",
            "type": "name_year",
            "paper": null
        },
        "context": "Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.",
        "score": null,
        "qa_pairs": []
    }
}
##################################
7147309 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Bahdanau et al., 2017)
</citation_literal>

<citation_context>
Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville, Yoshua Bengio",
        "year": 2016,
        "title": "An Actor-Critic Algorithm for Sequence Prediction",
        "corpusId": 14096841
    },
    "citance": {
        "citation": {
            "literal": "(Bahdanau et al., 2017)",
            "type": "name_year",
            "paper": null
        },
        "context": "Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.",
        "score": null,
        "qa_pairs": []
    }
}
##################################
14096841 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Jaques et al., 2017)
</citation_literal>

<citation_context>
Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, \u00c0gata Lapedriza, Noah J. Jones, S. Gu, Rosalind W. Picard",
        "year": 2019,
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog",
        "corpusId": 195766797
    },
    "citance": {
        "citation": {
            "literal": "(Jaques et al., 2017)",
            "type": "name_year",
            "paper": null
        },
        "context": "Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.",
        "score": null,
        "qa_pairs": []
    }
}
##################################
195766797 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Ziegler et al., 2019)
</citation_literal>

<citation_context>
Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, G. Irving",
        "year": 2019,
        "title": "Fine-Tuning Language Models from Human Preferences",
        "corpusId": 202660943
    },
    "citance": {
        "citation": {
            "literal": "(Ziegler et al., 2019)",
            "type": "name_year",
            "paper": null
        },
        "context": "Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.",
        "score": null,
        "qa_pairs": []
    }
}
##################################
202660943 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Dathathri et al., 2020)
</citation_literal>

<citation_context>
A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, J. Yosinski, Rosanne Liu",
        "year": 2019,
        "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
        "corpusId": 208617790
    },
    "citance": {
        "citation": {
            "literal": "(Dathathri et al., 2020)",
            "type": "name_year",
            "paper": null
        },
        "context": "A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.",
        "score": null,
        "qa_pairs": []
    }
}
##################################
208617790 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Csisz√°r & Shields, 2004)
</citation_literal>

<citation_context>
The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation, following Csisz√°r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000).
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "I. Csisz\u00e1r, P. Shields",
        "year": 2004,
        "title": "Information Theory and Statistics: A Tutorial",
        "corpusId": 31495396
    },
    "citance": {
        "citation": {
            "literal": "(Csisz\u00e1r & Shields, 2004)",
            "type": "name_year",
            "paper": null
        },
        "context": "The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation, following Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000).",
        "score": null,
        "qa_pairs": []
    }
}
##################################
31495396 in pes2o
Below is the BIBLIOGRAPHY of the paper:


  ----------
  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, 
  year: 2021
  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú
  corpusId: 262580630
  contexts: 
    
    * Those LMs - dubbed ‚ÄúStochastic Parrots‚Äù in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.
    
  ----------

  ----------
  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, 
  year: 2020
  title: Language Models are Few-Shot Learners
  corpusId: 218971783
  contexts: 
    
  ----------

  ----------
  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, 
  year: 2020
  title: Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP
  corpusId: 218971825
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, 
  year: 2020
  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
  corpusId: 218486908
  contexts: 
    
    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2020
  title: Towards Controllable Biases in Language Generation
  corpusId: 218470535
  contexts: 
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, 
  year: 2020
  title: Residual Energy-Based Models for Text Generation
  corpusId: 212945787
  contexts: 
    
    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.
    
  ----------

  ----------
  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, 
  year: 2020
  title: StereoSet: Measuring stereotypical bias in pretrained language models
  corpusId: 215828184
  contexts: 
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, 
  year: 2020
  title: Energy-Based Models for Text
  corpusId: 216035855
  contexts: 
    
    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Distributional Reinforcement Learning for Energy-Based Sequential Models
  corpusId: 209405007
  contexts: 
    
  ----------

  ----------
  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K√∂pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, 
  year: 2019
  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library
  corpusId: 202786778
  contexts: 
    
    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).
    
  ----------

  ----------
  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R√©mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, 
  year: 2019
  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing
  corpusId: 274421273
  contexts: 
    
  ----------

  ----------
  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, 
  year: 2019
  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation
  corpusId: 208617790
  contexts: 
    
    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.
    
    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.
    
    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.
    
    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).
    
    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.
    
    * As shown, PPLM and CTRL produce more repetitions compared to GDC.
    
    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.
    
    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: ‚Äúkitchen‚Äù, ‚Äúfantasy‚Äù, ‚Äúpolitics‚Äù, and ‚Äúcomputers‚Äù.
    
    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.
    
    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models
    
    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.
    
    * We set œÜl(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.
    
    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * The five prefixes used come from (Dathathri et al., 2020): ‚ÄúThe chicken ‚Äù, ‚ÄúThe potato ‚Äù, ‚ÄúThe lake ‚Äù, ‚ÄúThe pizza ‚Äù, and ‚ÄúThe horse ‚Äù.
    
  ----------

  ----------
  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, 
  year: 2019
  title: Fine-Tuning Language Models from Human Preferences
  corpusId: 202660943
  contexts: 
    
  ----------

  ----------
  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, 
  year: 2019
  title: CTRL: A Conditional Transformer Language Model for Controllable Generation
  corpusId: 202573071
  contexts: 
    
    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.
    
    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).
    
  ----------

  ----------
  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, 
  year: 2019
  title: The Woman Worked as a Babysitter: On Biases in Language Generation
  corpusId: 202537041
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).
    
    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).
    
  ----------

  ----------
  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, 
  year: 2019
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  corpusId: 202577673
  contexts: 
    
  ----------

  ----------
  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, 
  year: 2019
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  corpusId: 201698258
  contexts: 
    
    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.
    
  ----------

  ----------
  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  √Ägata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, 
  year: 2019
  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog
  corpusId: 195766797
  contexts: 
    
  ----------

  ----------
  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, 
  year: 2019
  title: Evaluating Gender Bias in Machine Translation
  corpusId: 173991101
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  M. Hooten,  T. Hefley, 
  year: 2019
  title: Importance Sampling
  corpusId: 241949602
  contexts: 
    
    * ‚Ä¶an ablation experiments with both metrics (Appendix 2)
    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    Z = ‚àë x P (x) = ‚àë x q(x) P (x)/q(x)
    = Ex‚àºq(x) P (x)/q(x) (7)
    We can then compute‚Ä¶
    
    * SNIS consists in computing:
    ¬µÃÇ(Œª) = ‚àëN j=1 wj(Œª) œÜ(xj)‚àëN
    j=1 wj(Œª) , (6)
    5Boldface œÜ and ¬µ represents vectors of real values (features and moments).
    and it can be shown that ¬µÃÇ(Œª) ' ¬µ(Œª), with convergence in the limit (Owen, 2013).
    
    * and it can be shown that ŒºÃÇ(Œª) ' Œº(Œª), with convergence in the limit (Owen, 2013).
    
    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, 
  year: 2019
  title: The Curious Case of Neural Text Degeneration
  corpusId: 127986954
  contexts: 
    
    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
    * During training of the policy œÄŒ∏, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.
    
  ----------

  ----------
  authors:  Shikha Bordia,  Samuel R. Bowman, 
  year: 2019
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  corpusId: 102352788
  contexts: 
    
    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2
    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of‚Ä¶
    
  ----------

  ----------
  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, 
  year: 2019
  title: What makes a good conversation? How controllable attributes affect human judgments
  corpusId: 67855999
  contexts: 
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, 
  year: 2018
  title: Language GANs Falling Short
  corpusId: 53208122
  contexts: 
    
    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).
    
  ----------

  ----------
  authors:  Eric Chu,  Peter J. Liu, 
  year: 2018
  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization
  corpusId: 59413781
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, 
  year: 2018
  title: Controllable Neural Story Plot Generation via Reward Shaping
  corpusId: 199465680
  contexts: 
    
    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, 
  year: 2018
  title: Assessing gender bias in machine translation: a case study with Google Translate
  corpusId: 52179151
  contexts: 
    
    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).
    
  ----------

  ----------
  authors:  F. Nielsen, 
  year: 2018
  title: An Elementary Introduction to Information Geometry
  corpusId: 52097732
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, 
  year: 2018
  title: Unsupervised Text Style Transfer using Language Models as Discriminators
  corpusId: 44061800
  contexts: 
    
    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.
    
  ----------

  ----------
  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, 
  year: 2018
  title: Learning to Write with Cooperative Discriminators
  corpusId: 21731209
  contexts: 
    
    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.
    
    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.
    
  ----------

  ----------
  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, 
  year: 2018
  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer
  corpusId: 4937880
  contexts: 
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.
    
    * As metrics, we use sentiment class expectation EœÜ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section ¬ß3.1.
    
  ----------

  ----------
  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, 
  year: 2018
  title: Texygen: A Benchmarking Platform for Text Generation Models
  corpusId: 3636178
  contexts: 
    
    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.
    
  ----------

  ----------
  authors:  Ramakanth Pasunuru,  Mohit Bansal, 
  year: 2017
  title: Reinforced Video Captioning with Entailment Rewards
  corpusId: 1137329
  contexts: 
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, 
  year: 2017
  title: A Deep Reinforced Model for Abstractive Summarization
  corpusId: 21850704
  contexts: 
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
  ----------

  ----------
  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, 
  year: 2017
  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
  corpusId: 6093112
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Matt J. Kusner,  Jos√© Miguel Hern√°ndez-Lobato, 
  year: 2016
  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
  corpusId: 10366219
  contexts: 
    
    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & HernaÃÅndez-Lobato (2016).
    
  ----------

  ----------
  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos√© Miguel Hern√°ndez-Lobato,  Richard E. Turner,  D. Eck, 
  year: 2016
  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
  corpusId: 15636415
  contexts: 
    
    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.
    
    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.
    
  ----------

  ----------
  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, 
  year: 2016
  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
  corpusId: 3603249
  contexts: 
    
  ----------

  ----------
  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, 
  year: 2016
  title: An Actor-Critic Algorithm for Sequence Prediction
  corpusId: 14096841
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, 
  year: 2016
  title: Deep Reinforcement Learning for Dialogue Generation
  corpusId: 3147007
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, 
  year: 2016
  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
  corpusId: 9197196
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors:  R. Lebret,  David Grangier,  Michael Auli, 
  year: 2016
  title: Neural Text Generation from Structured Data with Application to the Biography Domain
  corpusId: 1238927
  contexts: 
    
    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.
    
    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (¬ßG gives additional details).
    
  ----------

  ----------
  authors:  Taesup Kim,  Yoshua Bengio, 
  year: 2016
  title: Deep Directed Generative Models with Energy-Based Probability Estimation
  corpusId: 8070055
  contexts: 
    
    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate ¬µ(Œª) .= Ex‚àºpœÜ(x).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, 
  year: 2015
  title: Sequence Level Training with Recurrent Neural Networks
  corpusId: 7147309
  contexts: 
    
    * ‚Ä¶autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori‚Ä¶
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and‚Ä¶
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.
    
    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.
    
    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.
    
    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
  ----------

  ----------
  authors:  David Belanger,  A. McCallum, 
  year: 2015
  title: Structured Prediction Energy Networks
  corpusId: 6366436
  contexts: 
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, 
  year: 2015
  title: A Diversity-Promoting Objective Function for Neural Conversation Models
  corpusId: 7287895
  contexts: 
    
    * ‚Ä¶models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.
    
    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.
    
  ----------

  ----------
  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, 
  year: 2015
  title: First Women, Second Sex: Gender Bias in Wikipedia
  corpusId: 1082360
  contexts: 
    
    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
    * 58% of English Wikipedia‚Äôs biographies (Graells-Garrido et al., 2015).
    
  ----------

  ----------
  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, 
  year: 2007
  title: A Unified Energy-Based Framework for Unsupervised Learning
  corpusId: 2642042
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
  ----------

  ----------
  authors:  C. Robert,  G. Casella, 
  year: 2005
  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)
  corpusId: 59843537
  contexts: 
    
    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-
    Hastings (Robert & Casella, 2005).
    
  ----------

  ----------
  authors:  I. Csisz√°r,  P. Shields, 
  year: 2004
  title: Information Theory and Statistics: A Tutorial
  corpusId: 31495396
  contexts: 
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of CsiszaÃÅr & Shields (2004).
    
  ----------

  ----------
  authors:  Geoffrey E. Hinton, 
  year: 2002
  title: Training Products of Experts by Minimizing Contrastive Divergence
  corpusId: 207596505
  contexts: 
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1975
  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems
  corpusId: 18053591
  contexts: 
    
    * Note: Csisz√°r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
    * Note: CsiszaÃÅr & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).
    
  ----------

  ----------
  authors:  E. Jaynes, 
  year: 1957
  title: Information Theory and Statistical Mechanics
  corpusId: 17870175
  contexts: 
    
  ----------

  ----------
  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, 
  year: 2019
  title: Language Models are Unsupervised Multitask Learners
  corpusId: 160025533
  contexts: 
    
    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.
    
  ----------

  ----------
  authors: 
  year: 2017
  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347
  corpusId: None
  contexts: 
    
    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.
    
    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective EœÄŒ∏œÜ(x)‚àí Œ≤DKL(œÄŒ∏, a), which interpolates the reward œÜ(x) with a KL-divergence penalty from the pretrained model, but where‚Ä¶
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Optimization of image description metrics using policy gradient methods
  corpusId: None
  contexts: 
    
    * ‚Ä¶Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (‚Äúdegeneration‚Äù): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in‚Ä¶
    
    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to ‚Äúdegeneration‚Äù, producing poor examples that improve the average reward but forgo coherence and fluency.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.
    
  ----------

  ----------
  authors: 
  year: 2016
  title: Globally Normalized Transition-Based
  corpusId: None
  contexts: 
    
    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).
    
  ----------

  ----------
  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, 
  year: 2006
  title: A Tutorial on Energy-Based Learning
  corpusId: 8531544
  contexts: 
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.
    
    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ‚àë x P (x) is the partition function of P .
    
    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.
    
    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.
    
  ----------

  ----------
  authors:  Ronald J. Williams, 
  year: 2004
  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
  corpusId: 2332513
  contexts: 
    
    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of EœÄŒ∏œÜ(x) at the expense of a very large deviation from the original GPT-2.
    
    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).
    
    * 12The difference with REINFORCE makes sense if one observes that œÜ(x) can be maximized on many sequences, while P (x) tries to maximize a(x) ¬∑ œÜ(x), which is typically maximized on only one sequence.
    
    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight Œ≤) between a high average r(x) and the KL divergence from a; unless Œ≤ = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.
    
    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER
    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.
    
    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!
    
    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.
    
    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of EœÄŒ∏P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12
    In the case of ZIEGLER we can see a positive effect of the interpolation factor Œ≤ between the reward and the KL penalty in the objective function.
    
    * REINFORCEP(x) suffers from a token diversity issue.
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the‚Ä¶
    
    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward œÜ(x), i.e. trying to maximize EœÄŒ∏œÜ(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing EœÄŒ∏P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).
    
  ----------

  ----------
  authors: 
  year: 2004
  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342‚Äì347
  corpusId: None
  contexts: 
    
    * We start by sampling 1M sequences from GPT-2 small ‚Äî a process that took us roughly 48 hours ‚Äî and keeping only the ones containing ‚Äúamazing‚Äù (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).
    
  ----------

  ----------
  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, 
  year: 2001
  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration
  corpusId: 262695955
  contexts: 
    
    * The early work on ‚ÄùWhole sentence exponential models‚Äù by (Rosenfeld et al., 2001) ‚Äî which only came to our attention when preparing the final version of this paper ‚Äî can be considered as a form of EBM over texts.
    
  ----------

  ----------
  authors:  S. Amari,  H. Nagaoka, 
  year: 2000
  title: Methods of information geometry
  corpusId: 116976027
  contexts: 
    
    * We follow CsiszaÃÅr & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).
    
  ----------

  ----------
  authors:  I. Csisz√°r, 
  year: 1996
  title: Maxent, Mathematics, and Information Theory
  corpusId: 118126338
  contexts: 
    
    * According to (CsiszaÃÅr, 1996), the Generalized MaxEnt of sections ¬ß2.1 and ¬ß2.2 has the ‚ÄúTransitivity property‚Äù.
    
    * (Csisz√°r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
    * According to (Csisz√°r, 1996), the Generalized MaxEnt of sections ¬ß2.
    
    * (CsiszaÃÅr, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.
    
  ----------


Below is the surrounding text of the CITATION you need to match, and the CITATION literal itself:
<citation_literal>
(Parshakova et al., 2019b)
</citation_literal>

<citation_context>
One technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG ("Distributional Policy Gradient") algorithm (Parshakova et al., 2019b).
</citation_context>


Please respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:
{
  "authors": # authors of cited paper,
  "year": # year of cited paper,
  "title": # title of cited paper,
  "corpusId": # corpusId of cited paper
}
{
    "match": {
        "authors": "Tetiana Parshakova, J. Andreoli, Marc Dymetman",
        "year": 2019,
        "title": "Global Autoregressive Models for Data-Efficient Sequence Learning",
        "corpusId": 202577673
    },
    "citance": {
        "citation": {
            "literal": "(Parshakova et al., 2019b)",
            "type": "name_year",
            "paper": null
        },
        "context": "One technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (\"Distributional Policy Gradient\") algorithm (Parshakova et al., 2019b).",
        "score": null,
        "qa_pairs": []
    }
}
##################################
202577673 in pes2o
null
