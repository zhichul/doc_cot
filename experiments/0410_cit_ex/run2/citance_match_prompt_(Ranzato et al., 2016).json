{
    "messages": {
        "system": "You are a librarian that helps with matching inline-citations with bibliography items. You will be provided the context of the CITATION and also the BIBLIOGRAPHY entries. ",
        "user": "Below is the BIBLIOGRAPHY of the paper:\n\n\n  ----------\n  authors:  Emily M. Bender,  Timnit Gebru,  Angelina McMillan-Major,  Shmargaret Shmitchell, \n  year: 2021\n  title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c\n  corpusId: 262580630\n  contexts: \n    \n    * Those LMs - dubbed \u201cStochastic Parrots\u201d in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.\n    \n  ----------\n\n  ----------\n  authors:  Tom B. Brown,  Benjamin Mann,  Nick Ryder,  Melanie Subbiah,  J. Kaplan,  Prafulla Dhariwal,  Arvind Neelakantan,  Pranav Shyam,  Girish Sastry,  Amanda Askell,  Sandhini Agarwal,  Ariel Herbert-Voss,  Gretchen Krueger,  T. Henighan,  R. Child,  A. Ramesh,  Daniel M. Ziegler,  Jeff Wu,  Clemens Winter,  Christopher Hesse,  Mark Chen,  Eric Sigler,  Ma-teusz Litwin,  Scott Gray,  Benjamin Chess,  Jack Clark,  Christopher Berner,  Sam McCandlish,  Alec Radford,  I. Sutskever,  Dario Amodei, \n  year: 2020\n  title: Language Models are Few-Shot Learners\n  corpusId: 218971783\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Su Lin Blodgett,  Solon Barocas,  Hal Daum'e,  Hanna M. Wallach, \n  year: 2020\n  title: Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP\n  corpusId: 218971825\n  contexts: \n    \n    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).\n    \n  ----------\n\n  ----------\n  authors:  Lifu Tu,  Richard Yuanzhe Pang,  Sam Wiseman,  Kevin Gimpel, \n  year: 2020\n  title: ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation\n  corpusId: 218486908\n  contexts: \n    \n    * Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.\n    \n  ----------\n\n  ----------\n  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, \n  year: 2020\n  title: Towards Controllable Biases in Language Generation\n  corpusId: 218470535\n  contexts: \n    \n    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.\n    \n  ----------\n\n  ----------\n  authors:  Yuntian Deng,  A. Bakhtin,  Myle Ott,  Arthur Szlam, \n  year: 2020\n  title: Residual Energy-Based Models for Text Generation\n  corpusId: 212945787\n  contexts: \n    \n    * Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.\n    \n  ----------\n\n  ----------\n  authors:  Moin Nadeem,  Anna Bethke,  Siva Reddy, \n  year: 2020\n  title: StereoSet: Measuring stereotypical bias in pretrained language models\n  corpusId: 215828184\n  contexts: \n    \n    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).\n    \n  ----------\n\n  ----------\n  authors:  A. Bakhtin,  Yuntian Deng,  Sam Gross,  Myle Ott,  Marc'Aurelio Ranzato,  Arthur Szlam, \n  year: 2020\n  title: Energy-Based Models for Text\n  corpusId: 216035855\n  contexts: \n    \n    * A recent survey of EBMs for text is provided in Bakhtin et al. (2020).\n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= \u2211 x P (x) is the partition function of P .\n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .\n    \n  ----------\n\n  ----------\n  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, \n  year: 2019\n  title: Distributional Reinforcement Learning for Energy-Based Sequential Models\n  corpusId: 209405007\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Adam Paszke,  Sam Gross,  Francisco Massa,  Adam Lerer,  James Bradbury,  Gregory Chanan,  Trevor Killeen,  Zeming Lin,  N. Gimelshein,  L. Antiga,  Alban Desmaison,  Andreas K\u00f6pf,  E. Yang,  Zachary DeVito,  Martin Raison,  Alykhan Tejani,  Sasank Chilamkurthy,  Benoit Steiner,  Lu Fang,  Junjie Bai,  Soumith Chintala, \n  year: 2019\n  title: PyTorch: An Imperative Style, High-Performance Deep Learning Library\n  corpusId: 202786778\n  contexts: \n    \n    * We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).\n    \n  ----------\n\n  ----------\n  authors:  Thomas Wolf,  Lysandre Debut,  Victor Sanh,  Julien Chaumond,  Clement Delangue,  Anthony Moi,  Pierric Cistac,  Tim Rault,  R\u00e9mi Louf,  Morgan Funtowicz,  Joe Davison,  Sam Shleifer,  Patrick von Platen,  Clara Ma,  Yacine Jernite,  J. Plu,  Canwen Xu,  Teven Le Scao,  Sylvain Gugger,  Mariama Drame,  Quentin Lhoest,  Alexander M. Rush, \n  year: 2019\n  title: HuggingFace's Transformers: State-of-the-art Natural Language Processing\n  corpusId: 274421273\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Sumanth Dathathri,  Andrea Madotto,  Janice Lan,  Jane Hung,  Eric Frank,  Piero Molino,  J. Yosinski,  Rosanne Liu, \n  year: 2019\n  title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\n  corpusId: 208617790\n  contexts: \n    \n    * Unlike GDC, PPLM needs a prefix to perform its hidden-state updates.\n    \n    * Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.\n    \n    * For word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository19.\n    \n    * For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020).\n    \n    * PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes.\n    \n    * As shown, PPLM and CTRL produce more repetitions compared to GDC.\n    \n    * As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL.\n    \n    * Word-list constraints: We use 4 different word lists among those proposed in (Dathathri et al., 2020), covering the following topics: \u201ckitchen\u201d, \u201cfantasy\u201d, \u201cpolitics\u201d, and \u201ccomputers\u201d.\n    \n    * PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.\n    \n    * 19https://github.com/uber-research/PPLM/tree/master/paper code/wordlists 20https://github.com/uber-research/PPLM/tree/master/paper code/discrim models\n    \n    * A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context.\n    \n    * We set \u03c6l(x) = 1 if x contains at least one one word from the word list l. Classifier-based constraints: We use pre-trained classifiers from (Dathathri et al., 2020), which consist of a linear head on top of GPT-2.\n    \n    * As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task.\n    \n    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.\n    \n    * The five prefixes used come from (Dathathri et al., 2020): \u201cThe chicken \u201d, \u201cThe potato \u201d, \u201cThe lake \u201d, \u201cThe pizza \u201d, and \u201cThe horse \u201d.\n    \n  ----------\n\n  ----------\n  authors:  Daniel M. Ziegler,  Nisan Stiennon,  Jeff Wu,  Tom B. Brown,  Alec Radford,  Dario Amodei,  Paul Christiano,  G. Irving, \n  year: 2019\n  title: Fine-Tuning Language Models from Human Preferences\n  corpusId: 202660943\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  N. Keskar,  Bryan McCann,  L. Varshney,  Caiming Xiong,  R. Socher, \n  year: 2019\n  title: CTRL: A Conditional Transformer Language Model for Controllable Generation\n  corpusId: 202573071\n  contexts: \n    \n    * Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.\n    \n    * We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).\n    \n  ----------\n\n  ----------\n  authors:  Emily Sheng,  Kai-Wei Chang,  P. Natarajan,  Nanyun Peng, \n  year: 2019\n  title: The Woman Worked as a Babysitter: On Biases in Language Generation\n  corpusId: 202537041\n  contexts: \n    \n    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).\n    \n    * This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b).\n    \n    * Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b; Brown et al., 2020b; Nadeem et al., 2020).\n    \n  ----------\n\n  ----------\n  authors:  Tetiana Parshakova,  J. Andreoli,  Marc Dymetman, \n  year: 2019\n  title: Global Autoregressive Models for Data-Efficient Sequence Learning\n  corpusId: 202577673\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Eric Wallace,  Shi Feng,  Nikhil Kandpal,  Matt Gardner,  Sameer Singh, \n  year: 2019\n  title: Universal Adversarial Triggers for Attacking and Analyzing NLP\n  corpusId: 201698258\n  contexts: \n    \n    * (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.\n    \n    * Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.\n    \n  ----------\n\n  ----------\n  authors:  Natasha Jaques,  Asma Ghandeharioun,  Judy Hanwen Shen,  Craig Ferguson,  \u00c0gata Lapedriza,  Noah J. Jones,  S. Gu,  Rosalind W. Picard, \n  year: 2019\n  title: Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog\n  corpusId: 195766797\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Gabriel Stanovsky,  Noah A. Smith,  Luke Zettlemoyer, \n  year: 2019\n  title: Evaluating Gender Bias in Machine Translation\n  corpusId: 173991101\n  contexts: \n    \n    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).\n    \n  ----------\n\n  ----------\n  authors:  M. Hooten,  T. Hefley, \n  year: 2019\n  title: Importance Sampling\n  corpusId: 241949602\n  contexts: \n    \n    * \u2026an ablation experiments with both metrics (Appendix 2)\n    Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:\n    Z = \u2211 x P (x) = \u2211 x q(x) P (x)/q(x)\n    = Ex\u223cq(x) P (x)/q(x) (7)\n    We can then compute\u2026\n    \n    * SNIS consists in computing:\n    \u00b5\u0302(\u03bb) = \u2211N j=1 wj(\u03bb) \u03c6(xj)\u2211N\n    j=1 wj(\u03bb) , (6)\n    5Boldface \u03c6 and \u00b5 represents vectors of real values (features and moments).\n    and it can be shown that \u00b5\u0302(\u03bb) ' \u00b5(\u03bb), with convergence in the limit (Owen, 2013).\n    \n    * and it can be shown that \u03bc\u0302(\u03bb) ' \u03bc(\u03bb), with convergence in the limit (Owen, 2013).\n    \n    * Given P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows:\n    \n  ----------\n\n  ----------\n  authors:  Ari Holtzman,  Jan Buys,  Li Du,  Maxwell Forbes,  Yejin Choi, \n  year: 2019\n  title: The Curious Case of Neural Text Degeneration\n  corpusId: 127986954\n  contexts: \n    \n    * 9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.\n    \n    * During training of the policy \u03c0\u03b8, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.\n    \n  ----------\n\n  ----------\n  authors:  Shikha Bordia,  Samuel R. Bowman, \n  year: 2019\n  title: Identifying and Reducing Gender Bias in Word-Level Language Models\n  corpusId: 102352788\n  contexts: \n    \n    * Bordia & Bowman (2019) introduce a regularization term for reducing gender bias when training a language model from scratch (as opposed to de-biasing a pretrained model).2\n    In this work, we present our Generation with Distributional Control (GDC) approach, in which we formalize the problem of\u2026\n    \n  ----------\n\n  ----------\n  authors:  A. See,  Stephen Roller,  Douwe Kiela,  J. Weston, \n  year: 2019\n  title: What makes a good conversation? How controllable attributes affect human judgments\n  corpusId: 67855999\n  contexts: \n    \n    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.\n    \n  ----------\n\n  ----------\n  authors:  Massimo Caccia,  Lucas Caccia,  W. Fedus,  H. Larochelle,  Joelle Pineau,  Laurent Charlin, \n  year: 2018\n  title: Language GANs Falling Short\n  corpusId: 53208122\n  contexts: \n    \n    * However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).\n    \n  ----------\n\n  ----------\n  authors:  Eric Chu,  Peter J. Liu, \n  year: 2018\n  title: MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization\n  corpusId: 59413781\n  contexts: \n    \n    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & Herna\u0301ndez-Lobato (2016).\n    \n  ----------\n\n  ----------\n  authors:  Pradyumna Tambwekar,  Murtaza Dhuliawala,  Lara J. Martin,  Animesh Mehta,  Brent Harrison,  Mark O. Riedl, \n  year: 2018\n  title: Controllable Neural Story Plot Generation via Reward Shaping\n  corpusId: 199465680\n  contexts: \n    \n    * , 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n    * \u2026models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.\n    \n  ----------\n\n  ----------\n  authors:  Marcelo O. R. Prates,  Pedro H. C. Avelar,  L. Lamb, \n  year: 2018\n  title: Assessing gender bias in machine translation: a case study with Google Translate\n  corpusId: 52179151\n  contexts: \n    \n    * There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).\n    \n  ----------\n\n  ----------\n  authors:  F. Nielsen, \n  year: 2018\n  title: An Elementary Introduction to Information Geometry\n  corpusId: 52097732\n  contexts: \n    \n    * We follow Csisza\u0301r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).\n    \n  ----------\n\n  ----------\n  authors:  Zichao Yang,  Zhiting Hu,  Chris Dyer,  E. Xing,  Taylor Berg-Kirkpatrick, \n  year: 2018\n  title: Unsupervised Text Style Transfer using Language Models as Discriminators\n  corpusId: 44061800\n  contexts: \n    \n    * Yang et al. (2018) use a set of Language Models pretrained on the target domain as a control signal for text style transfer.\n    \n  ----------\n\n  ----------\n  authors:  Ari Holtzman,  Jan Buys,  Maxwell Forbes,  Antoine Bosselut,  David Golub,  Yejin Choi, \n  year: 2018\n  title: Learning to Write with Cooperative Discriminators\n  corpusId: 21731209\n  contexts: \n    \n    * As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.\n    \n    * These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.\n    \n  ----------\n\n  ----------\n  authors:  Juncen Li,  Robin Jia,  He He,  Percy Liang, \n  year: 2018\n  title: Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer\n  corpusId: 4937880\n  contexts: \n    \n    * As metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.\n    \n    * As metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.1.\n    \n  ----------\n\n  ----------\n  authors:  Yaoming Zhu,  Sidi Lu,  Lei Zheng,  Jiaxian Guo,  Weinan Zhang,  Jun Wang,  Yong Yu, \n  year: 2018\n  title: Texygen: A Benchmarking Platform for Text Generation Models\n  corpusId: 3636178\n  contexts: \n    \n    * So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.\n    \n  ----------\n\n  ----------\n  authors:  Ramakanth Pasunuru,  Mohit Bansal, \n  year: 2017\n  title: Reinforced Video Captioning with Entailment Rewards\n  corpusId: 1137329\n  contexts: \n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n  ----------\n\n  ----------\n  authors:  Romain Paulus,  Caiming Xiong,  R. Socher, \n  year: 2017\n  title: A Deep Reinforced Model for Abstractive Summarization\n  corpusId: 21850704\n  contexts: \n    \n    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.\n    \n    * (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n  ----------\n\n  ----------\n  authors:  Rakshith Shetty,  Marcus Rohrbach,  Lisa Anne Hendricks,  Mario Fritz,  B. Schiele, \n  year: 2017\n  title: Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images\n  corpusId: 6093112\n  contexts: \n    \n    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & Herna\u0301ndez-Lobato (2016).\n    \n  ----------\n\n  ----------\n  authors:  Matt J. Kusner,  Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, \n  year: 2016\n  title: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution\n  corpusId: 10366219\n  contexts: \n    \n    * Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & Herna\u0301ndez-Lobato (2016).\n    \n  ----------\n\n  ----------\n  authors:  Natasha Jaques,  S. Gu,  Dzmitry Bahdanau,  Jos\u00e9 Miguel Hern\u00e1ndez-Lobato,  Richard E. Turner,  D. Eck, \n  year: 2016\n  title: Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control\n  corpusId: 15636415\n  contexts: \n    \n    * Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.\n    \n    * Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.\n    \n  ----------\n\n  ----------\n  authors:  Yonghui Wu,  M. Schuster,  Z. Chen,  Quoc V. Le,  Mohammad Norouzi,  Wolfgang Macherey,  M. Krikun,  Yuan Cao,  Qin Gao,  Klaus Macherey,  J. Klingner,  Apurva Shah,  Melvin Johnson,  Xiaobing Liu,  Lukasz Kaiser,  Stephan Gouws,  Yoshikiyo Kato,  Taku Kudo,  H. Kazawa,  K. Stevens,  George Kurian,  Nishant Patil,  Wei Wang,  C. Young,  Jason R. Smith,  Jason Riesa,  Alex Rudnick,  O. Vinyals,  G. Corrado,  Macduff Hughes,  J. Dean, \n  year: 2016\n  title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n  corpusId: 3603249\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Dzmitry Bahdanau,  Philemon Brakel,  Kelvin Xu,  Anirudh Goyal,  Ryan Lowe,  Joelle Pineau,  Aaron C. Courville,  Yoshua Bengio, \n  year: 2016\n  title: An Actor-Critic Algorithm for Sequence Prediction\n  corpusId: 14096841\n  contexts: \n    \n    * \u2026models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and\u2026\n    \n    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.\n    \n    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.\n    \n    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.\n    \n    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n  ----------\n\n  ----------\n  authors:  Jiwei Li,  Will Monroe,  Alan Ritter,  Dan Jurafsky,  Michel Galley,  Jianfeng Gao, \n  year: 2016\n  title: Deep Reinforcement Learning for Dialogue Generation\n  corpusId: 3147007\n  contexts: \n    \n    * \u2026models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.\n    \n  ----------\n\n  ----------\n  authors:  Chia-Wei Liu,  Ryan Lowe,  Iulian Serban,  Michael Noseworthy,  Laurent Charlin,  Joelle Pineau, \n  year: 2016\n  title: How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation\n  corpusId: 9197196\n  contexts: \n    \n    * \u2026Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (\u201cdegeneration\u201d): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in\u2026\n    \n    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \u201cdegeneration\u201d, producing poor examples that improve the average reward but forgo coherence and fluency.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.\n    \n  ----------\n\n  ----------\n  authors:  R. Lebret,  David Grangier,  Michael Auli, \n  year: 2016\n  title: Neural Text Generation from Structured Data with Application to the Biography Domain\n  corpusId: 1238927\n  contexts: \n    \n    * For distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2bio.\n    \n    * We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2bio) (\u00a7G gives additional details).\n    \n  ----------\n\n  ----------\n  authors:  Taesup Kim,  Yoshua Bengio, \n  year: 2016\n  title: Deep Directed Generative Models with Energy-Based Probability Estimation\n  corpusId: 8070055\n  contexts: \n    \n    * On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate \u00b5(\u03bb) .= Ex\u223cp\u03c6(x).\n    \n  ----------\n\n  ----------\n  authors:  Marc'Aurelio Ranzato,  S. Chopra,  Michael Auli,  Wojciech Zaremba, \n  year: 2015\n  title: Sequence Level Training with Recurrent Neural Networks\n  corpusId: 7147309\n  contexts: \n    \n    * \u2026autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori\u2026\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and\u2026\n    \n    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.\n    \n    * With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.\n    \n    * Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.\n    \n    * Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n  ----------\n\n  ----------\n  authors:  David Belanger,  A. McCallum, \n  year: 2015\n  title: Structured Prediction Energy Networks\n  corpusId: 6366436\n  contexts: \n    \n    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).\n    \n  ----------\n\n  ----------\n  authors:  Jiwei Li,  Michel Galley,  Chris Brockett,  Jianfeng Gao,  W. Dolan, \n  year: 2015\n  title: A Diversity-Promoting Objective Function for Neural Conversation Models\n  corpusId: 7287895\n  contexts: \n    \n    * \u2026models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.\n    \n    * Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.\n    \n  ----------\n\n  ----------\n  authors:  Eduardo Graells-Garrido,  M. Lalmas,  F. Menczer, \n  year: 2015\n  title: First Women, Second Sex: Gender Bias in Wikipedia\n  corpusId: 1082360\n  contexts: \n    \n    * One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia\u2019s biographies (Graells-Garrido et al., 2015).\n    \n    * 58% of English Wikipedia\u2019s biographies (Graells-Garrido et al., 2015).\n    \n  ----------\n\n  ----------\n  authors:  Marc'Aurelio Ranzato,  Y-Lan Boureau,  S. Chopra,  Yann LeCun, \n  year: 2007\n  title: A Unified Energy-Based Framework for Unsupervised Learning\n  corpusId: 2642042\n  contexts: \n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.\n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.\n    \n  ----------\n\n  ----------\n  authors:  C. Robert,  G. Casella, \n  year: 2005\n  title: Monte Carlo Statistical Methods (Springer Texts in Statistics)\n  corpusId: 59843537\n  contexts: \n    \n    * 3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-\n    Hastings (Robert & Casella, 2005).\n    \n  ----------\n\n  ----------\n  authors:  I. Csisz\u00e1r,  P. Shields, \n  year: 2004\n  title: Information Theory and Statistics: A Tutorial\n  corpusId: 31495396\n  contexts: \n    \n    * Note: Csisza\u0301r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).\n    \n    * We follow Csisza\u0301r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).\n    \n    * Our statement of Theorem 1 is actually a reformulation of two results in section 3 of Csisza\u0301r & Shields (2004).\n    \n  ----------\n\n  ----------\n  authors:  Geoffrey E. Hinton, \n  year: 2002\n  title: Training Products of Experts by Minimizing Contrastive Divergence\n  corpusId: 207596505\n  contexts: \n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.\n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= \u2211 x P (x) is the partition function of P .\n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.\n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .\n    \n  ----------\n\n  ----------\n  authors:  I. Csisz\u00e1r, \n  year: 1975\n  title: $I$-Divergence Geometry of Probability Distributions and Minimization Problems\n  corpusId: 18053591\n  contexts: \n    \n    * Note: Csisz\u00e1r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).\n    \n    * Note: Csisza\u0301r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).\n    \n  ----------\n\n  ----------\n  authors:  E. Jaynes, \n  year: 1957\n  title: Information Theory and Statistical Mechanics\n  corpusId: 17870175\n  contexts: \n    \n  ----------\n\n  ----------\n  authors:  Alec Radford,  Jeff Wu,  R. Child,  D. Luan,  Dario Amodei,  I. Sutskever, \n  year: 2019\n  title: Language Models are Unsupervised Multitask Learners\n  corpusId: 160025533\n  contexts: \n    \n    * Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.\n    \n  ----------\n\n  ----------\n  authors: \n  year: 2017\n  title: Proximal policy optimization algorithms. CoRR, abs/1707.06347\n  corpusId: None\n  contexts: \n    \n    * Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward.\n    \n    * (3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective E\u03c0\u03b8\u03c6(x)\u2212 \u03b2DKL(\u03c0\u03b8, a), which interpolates the reward \u03c6(x) with a KL-divergence penalty from the pretrained model, but where\u2026\n    \n  ----------\n\n  ----------\n  authors: \n  year: 2016\n  title: Optimization of image description metrics using policy gradient methods\n  corpusId: None\n  contexts: \n    \n    * \u2026Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (\u201cdegeneration\u201d): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in\u2026\n    \n    * However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \u201cdegeneration\u201d, producing poor examples that improve the average reward but forgo coherence and fluency.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.\n    \n  ----------\n\n  ----------\n  authors: \n  year: 2016\n  title: Globally Normalized Transition-Based\n  corpusId: None\n  contexts: \n    \n    * tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).\n    \n    * Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).\n    \n  ----------\n\n  ----------\n  authors:  Yann LeCun,  S. Chopra,  R. Hadsell,  Aurelio Ranzato,  Fu Jie Huang, \n  year: 2006\n  title: A Tutorial on Energy-Based Learning\n  corpusId: 8531544\n  contexts: \n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .\n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.\n    \n    * 7The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.\n    \n    * P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= \u2211 x P (x) is the partition function of P .\n    \n    * Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.\n    \n    * (7)The class of Energy-Based Models (EBMs) (LeCun et al., 2006) is much larger than the exponential family models we are considering in this paper.\n    \n  ----------\n\n  ----------\n  authors:  Ronald J. Williams, \n  year: 2004\n  title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\n  corpusId: 2332513\n  contexts: \n    \n    * We observe the following: the baseline REINFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of E\u03c0\u03b8\u03c6(x) at the expense of a very large deviation from the original GPT-2.\n    \n    * This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).\n    \n    * 12The difference with REINFORCE makes sense if one observes that \u03c6(x) can be maximized on many sequences, while P (x) tries to maximize a(x) \u00b7 \u03c6(x), which is typically maximized on only one sequence.\n    \n    * The main points to note are: (1) REINFORCE is trying to find a distribution pR maximizing r(x) (meaning that pR lies on the C manifold), but this pR is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution pZ that interpolates (with a weight \u03b2) between a high average r(x) and the KL divergence from a; unless \u03b2 = 0, in which case we are back to REINFORCE, pZ does not satisfy the constraint and falls outside of the manifold.\n    \n    * D.1 ILLUSTRATION COMPARING GDC, REINFORCE, AND ZIEGLER\n    The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint.\n    \n    * ... 1 1 This book is incredibly rich , entertaining , and extremely enjoyable... REINFORCE 1 1 Featuring the highest quality performance performance performance... 1 1 This beautiful beautiful quality production quality high quality... 1 1 High quality performance high quality performance product ... REINFORCE P(x) 10k 1 Thank you for supporting the journalism that our community needs!\n    \n    * The vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.\n    \n    * REINFORCEP(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of E\u03c0\u03b8P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines.12\n    In the case of ZIEGLER we can see a positive effect of the interpolation factor \u03b2 between the reward and the KL penalty in the objective function.\n    \n    * REINFORCEP(x) suffers from a token diversity issue.\n    \n    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward \u03c6(x), i.e. trying to maximize E\u03c0\u03b8\u03c6(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing E\u03c0\u03b8P (x); this baseline starts from the\u2026\n    \n    * Baselines: We compare our method GDC to three baselines: (1) REINFORCE (Williams, 1992b), using the reward \u03c6(x), i.e. trying to maximize E\u03c0\u03b8\u03c6(x); (2) REINFORCEP(x) : Reinforce again, but now using the reward P (x) based on our energy model P , i.e. maximizing E\u03c0\u03b8P (x); this baseline starts from the same optimal EBM P representation as GDC but with a standard optimization objective rather than a distributional one; in other words, while GDC tries to get a similar sampling distribution to p, this baseline tries to get sequences of maximal probability p(x).\n    \n  ----------\n\n  ----------\n  authors: \n  year: 2004\n  title: Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342\u2013347\n  corpusId: None\n  contexts: \n    \n    * We start by sampling 1M sequences from GPT-2 small \u2014 a process that took us roughly 48 hours \u2014 and keeping only the ones containing \u201camazing\u201d (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).\n    \n  ----------\n\n  ----------\n  authors:  R. Rosenfeld,  Stanley F. Chen,  Xiaojin Zhu, \n  year: 2001\n  title: Whole-sentence exponential language models: a vehicle for linguistic-statistical integration\n  corpusId: 262695955\n  contexts: \n    \n    * The early work on \u201dWhole sentence exponential models\u201d by (Rosenfeld et al., 2001) \u2014 which only came to our attention when preparing the final version of this paper \u2014 can be considered as a form of EBM over texts.\n    \n  ----------\n\n  ----------\n  authors:  S. Amari,  H. Nagaoka, \n  year: 2000\n  title: Methods of information geometry\n  corpusId: 116976027\n  contexts: \n    \n    * We follow Csisza\u0301r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).\n    \n  ----------\n\n  ----------\n  authors:  I. Csisz\u00e1r, \n  year: 1996\n  title: Maxent, Mathematics, and Information Theory\n  corpusId: 118126338\n  contexts: \n    \n    * According to (Csisza\u0301r, 1996), the Generalized MaxEnt of sections \u00a72.1 and \u00a72.2 has the \u201cTransitivity property\u201d.\n    \n    * (Csisz\u00e1r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.\n    \n    * According to (Csisz\u00e1r, 1996), the Generalized MaxEnt of sections \u00a72.\n    \n    * (Csisza\u0301r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.\n    \n  ----------\n\n\nBelow is the surrounding text of the CITATION you need to match, and the CITATION literal itself:\n<citation_literal>\n(Ranzato et al., 2016)\n</citation_literal>\n\n<citation_context>\nPrior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\n</citation_context>\n\n\nPlease respond with a json containing the BIBLIOGRAPHY entry that matches the CITATION, in the following format:\n{\n  \"authors\": # authors of cited paper,\n  \"year\": # year of cited paper,\n  \"title\": # title of cited paper,\n  \"corpusId\": # corpusId of cited paper\n}"
    },
    "response_format": {
        "type": "json_object"
    },
    "temperature": 0.0
}
