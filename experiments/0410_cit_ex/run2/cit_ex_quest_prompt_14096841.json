{
    "messages": {
        "system": "You are a paper reviewer trying to identify connections between the paper you're reviewing with previous ideas and approaches in the field.",
        "user": "Below is the CURRENT PAPER that you're reviewing and a paper that it CITES. \nBelow is also the CITANCE, which is the context containing a citation to the CITED PAPER.\n\nPlease respond with a list of question that can be asked of papers, and which have identical answers for the CURRENT PAPER and the CITED PAPER. \n\nPlease respond in the following json format:\n{\n  \"thought_process\": str # first discuss how the two papers are related, this should help you come up with questions\n  \"questions\": [\n    {\n      \"question\": str # a clear, unambiguous question that can be asked of papers in general\n                      # make sure the question actually requires reading the paper to answer\n                      # and also make sure that the answer is not just a short phrase that can be extracted from the paper by searching\n                      # the answer should not use special terminology, but should instead use simple language\n                      # where a reader unfamiliar with the paper but has background can still understand\n      \"score\": str # a score of quality of the question\n      \"rationale\": str # a justification for why this question yields the same answer on both papers, and the score it receives\n      \"answer\": str # the answer to this question for both papers\n    },\n  ]\n}\n\n\nList all important questions, and give a score between 0 to 2, where 0 means the two papers would actually not have the same answer, 1 means while the two papers would have the same answer, it's too generic and many other papers would also have the same answer, and 2 means it touches the core of the relationship between the two papers, and it's unlikely that many other papers would have the same answer.\n\n<current_paper>\nA Distributional Approach to Controlled Text Generation\n\nWe propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both\"pointwise\"and\"distributional\"constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https://github.com/naver/gdc)\n\n\nINTRODUCTION\nNeural language models, such as GPT-2/3 Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality. In this paper, we are concerned with the problem of controlling a generic pretrained LM in order to satisfy certain desiderata. For instance, we may want to avoid toxic content; prevent certain demographic biases; or steer generations towards a certain topic or style. Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\nHowever, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \"degeneration\", producing poor examples that improve the average reward but forgo coherence and fluency. This degeneration is often diagnosed as an effect of deviating too much from the original pretrained LM during optimization. Consequently, prior work has regarded proximity to the pretrained model as a prescription for sample quality. This view is most prominent in open-domain generation where no gold references are available for fine-tuning, making the pretrained LM itself the yardstick for fluency. Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations. A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context. However, the authors show that balancing policy deviations from the original LM while also satisfying the control conditions is delicate. To combat degeneration they had to combine the KL penalty with post-norm fusion, reranking, and early-stopping procedures.\n1. We introduce a Distributional View for controlled text generation formalized as a constraint satisfaction problem combined with a divergence minimization objective, providing a single framework both for \"distributional\" constraints (collective statistical requirements) and for \"pointwise\" constraints (hard requirements on each individual) ( \u00a72.1). To our knowledge, this is the first framework with such generality for controlled text generation.\n2. We show how these constraints lead to an optimal EBM for the target model ( \u00a72.2), propose the KL-Adaptive DPG algorithm for approximating the optimal EBM distribution by Figure 1: From MaxEnt to EBM through Information Geometry. The Generalized MaxEnt specification (left panel) is looking for a distribution p that lies on the moment constraints manifold C and that minimizes the forward KL DKL(p, a). The solution is provided by Information Geometry: (1) build the exponential family E determined by a and \u03c6, (2) p lies at the intersection between C and E, (3) for any distribution c satisfying the constraints, the \"Pythagorean identity\" holds: DKL(c||a) = DKL(c||p) + DKL(p||a); in particular p is unique.\nan autoregressive policy ( \u00a72.3), and show the effectiveness of this adaptive technique for obtaining faster convergence ( \u00a7B.2).\n3. We conduct experiments in a number of pointwise and distributional conditions, assessing results in terms of divergence from GPT-2, fluency and diversity, with better performance than strong baselines. The distributional experiments show the potential of our approach as a remedy to the current and important problem of bias in pretrained language models, providing a novel direction for addressing it ( \u00a73).\n\nFORMALIZATION\nWe denote by X the set of all sequences x of bounded length L max , by a the initial pretrained model and by p the desired target model. The probabilities of x according to each model are a(x) and p(x). Our approach consists in expressing our desiderata through constraints on the desired values\u03bc i of the expectations (aka moments) \u00b5 i . = E x\u223cp \u03c6 i (x) of certain predefined real-valued feature functions \u03c6 i (x), for i \u2208 {1, . . . , k}.\nTo illustrate, the previous example can be expressed by using two binary features, \u03c6 1 (x) = 1 iff x is classified as speaking about sports, \u03c6 2 (x) = 1 iff x mentions a female character. Then our \"moment constraints\" take the following form: \u00b5 1 = E x\u223cp \u03c6 1 (x) = 1.0, \u00b5 2 = E x\u223cp \u03c6 2 (x) = 0.5. The first (pointwise) constraint implies that each individual x has to speak about sports (otherwise \u00b5 1 could not reach its maximum value 1.0), the second (distributional) constraint that 50% of the x's have to mention a female character. 4 Let C be the set of all distributions c over X that satisfy the moment constraints. We then propose to specify p as a distribution respecting the constraints, but also minimizing KL divergence from a: Equation (1) is a generalization of the Maximum Entropy Principle of Jaynes (1957), which corresponds to the limit case where a is the uniform u distribution over X, noting that minimizing D KL (c, u) is equivalent to maximizing the entropy of c under the constraints -in other words, trying to find the least \"specific\" distribution satisfying the constraints.\n\nCONSTRAINTS, INFORMATION GEOMETRY, EXPONENTIAL FAMILIES\nTo recap our formal approach, we have a finite set X, a distribution a over X s.t. a(x) > 0, \u2200x \u2208 X, and real functions \u03c6 1 , ..., \u03c6 k over X. We specify moment constraints \u00b5 i =\u03bc i on distributions c over X, where \u00b5 i . = E x\u223cc \u03c6 i (x) and the\u03bc i 's are given targets; the set of distributions satisfying these constraints is denoted by C. Our Problem is to find a p such that p = arg min c\u2208C D KL (c, a).\nWe follow Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000). Under the assumption that C = \u2205, they prove the following result (also see \u00a7A.1): Published as a conference paper at ICLR 2021 Theorem 1 (A) There exists a unique solution p to the problem above, obtained as p(x) \u221d P (x) where P is in exponential family form: (x) . (2) In other words p(x) = 1/Z P (x), with Z = x\u2208X P (x); P is an unnormalized distribution, i.e. an EBM. Here X C = {x \u2208 X| \u2203c \u2208 C s.t. c(x) > 0} is the \"support set\" associated with C. The \u03bb i 's are real numbers called the natural parameters associated with the moments \u00b5 i .\n(B) p can be approximated to arbitrary precision by distributions p of the form: for appropriate real values of the \u03bb ,i .\nThe advantage of this version of the connection between Generalized Maximum Entropy and Exponential Families is its generality, which distinguishes it from other presentations, and which makes it ideal for unified application to pointwise, distributional or hybrid constraints.\nIn the special case of only pointwise constraints, of the form E x\u223cc \u03c6 i (x) = 1.0, i \u2208 [1, k], with \u03c6 i (x) \u2208 {0, 1}, let's define the predicate b(x) to be 1 iff x satisfies all the constraints. Then, using the (A) form of the result, it is an easy exercise (see \u00a7A.2) to prove that X C = {x \u2208 X| b(x) = 1} and that one has p(x) \u221d a(x)b(x). In this case P (x) = a(x)b(x) is a very simple EBM that does not involve an exponential part; this is the EBM form that we use for experiments involving only pointwise constraints.\nIn the general case where some constraints are distributional, the determination of X C is not as direct, and we prefer to use the approximation provided by (B), which permits a generic implementation. With only distributional constraints, an exact solution is typically obtained with finite \u03bb's. With hybrid constraints, some of the \u03bb's may tend to infinite (positive or negative) values but thresholding them suffices to get a good approximation.\nNote that the estimate\u03bc(\u03bb) is obtained not as a single number, but as a parametric function of the variable \u03bb. We want to find \u03bb such that\u03bc(\u03bb) =\u03bc, a question that we handle on line 4 by performing an SGD optimization over the objective min ||\u03bc \u2212\u03bc(\u03bb)|| 2 2 . 6 At the end of this process, we obtain an estimated value for the parameter vector \u03bb, and a representation P (x) = a(x) exp \u03bb, \u03c6(x) . While a(x) is a normalized distribution by construction, the introduction of the second factor loses this normalization property, making P (x) an EBM. 7 8\n\nFROM EBM TO AUTOREGRESSIVE POLICY\nAlgorithm 2 KL-Adaptive DPG Input: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: for each iteration do 3: for each episode do 4: sample x from q(\u00b7) 5: \u03b8 \u2190 \u03b8+\u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 6: if DKL(p||\u03c0 \u03b8 ) < DKL(p||q) then 7: q \u2190 \u03c0 \u03b8 Output: \u03c0 \u03b8 The EBM representation just obtained for P defines the optimal p = Z \u22121 P unambiguously, a crucial intermediate step in the solution of our problem. From it we can immediately compute ratios of the form p(x)/p(x ) for two sequences x, x , but without knowing Z, we cannot compute p(x) and, even with such a knowledge, we cannot produce samples from p.\nThis problem is typical of EBMs at large: they provide a rich and flexible mechanism for specifying models, but they leave a gap between representation and exploitation. A range of techniques, from sophisticated MCMC approaches (especially for continuous models in vision) to contrastive learning techniques, have been developed for bridging this gap.\nOne technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (\"Distributional Policy Gradient\") algorithm (Parshakova et al., 2019b).\nThe objective of DPG is to obtain an autoregressive policy \u03c0 \u03b8 that approximates p, where approximation is formalized in terms of making the cross-entropy CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) log \u03c0 \u03b8 (x) as small as possible. 9 DPG exploits the fact that, for any \"proposal\" distribution q whose support contains the support of p, we have \u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212\u2207 \u03b8 E x\u223cp log \u03c0 \u03b8 (x) = \u2212E x\u223cp \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2212E x\u223cq p(x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) where the last equality is an instance of importance sampling. Our \"KL-adaptive\" version of DPG is shown in (Algorithm 2). We start from an input EBM P , along with an initial policy q which is a proxy to p; in our case we take q = a. During an iteration (think minibatch or set of minibatches), we sample a number of sequences from q, do an SGD update of \u03b8 (line 5), where P is used instead of p (noting that they only differ by a multiplicative constant), and where \u03b1 (\u03b8) is a learning rate. The efficiency of the algorithm is related to how close the proposal q is to the target p, 10 The algorithm is adaptive in the sense that it modifies q periodically to take advantage of the evolving approximations \u03c0 \u03b8 . On line 6, we we test whether the current \u03c0 \u03b8 is closer than q to p in terms of KL-divergence, and if so we update q to \u03c0 \u03b8 on line 7. 11 \u00a7B.2 provides an ablation study showing the effectiveness of this adaptive step for obtaining faster convergence.\n\nEXPERIMENTS, RESULTS, AND EVALUATION\nIn this section we describe our evaluation methodology and perform experiments on pointwise constraints ( \u00a73.2) and on distributional and hybrid constraints ( \u00a73.3). The Appendix contains a detailed view of evaluation ( \u00a7H), comparison with extra baselines ( \u00a7D.2), and an ablation study ( \u00a7B.2).\n\nEVALUATION METRICS\nThe main metrics we report are: (1) E x\u223c\u03c0 \u03b8 \u03c6 i (x), assessing the ability of \u03c0 \u03b8 to reach the expectation goal on the i-th constraint, (2) D KL (p||\u03c0 \u03b8 ), the forward KL divergence from the optimal distribution (which should be as close to 0 as possible), (3) D KL (\u03c0 \u03b8 ||a), the reverse KL divergence from the original GPT-2; for details on the estimation of these metrics see \u00a7B.1. Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence. However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020). So additionally, we report Self-BLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig. 4 and \u00a7H.4). Note that KL divergence from the original GPT-2 also implicitly captures sample diversity: a distribution that focuses all its probability mass on a few sequences typically displays high divergence from GPT-2. Implementation details and hyper-parameters are available in the Appendix ( \u00a7 F).\n\nPOINTWISE CONSTRAINTS EXPERIMENTS\nPointwise constraints are of the form E p \u03c6 i (x) = 1, with \u03c6 i a binary feature. Contrarily to distributional constraints, they can be directly associated with a \"reward\", namely \u03c6 i itself. RL-inspired baselines can then be introduced naturally, and this is what we do here.\n(3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective E \u03c0 \u03b8 \u03c6(x) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), which interpolates the reward \u03c6(x) with a KL-divergence penalty from the pretrained model, but where the goal is not explicitly to satisfy a constraint; for a geometric illustration of the differences with 11 In the original DPG, the superiority test is done on the basis of the log-likelihood on a validation set. Here we are in the more demanding situation where no validation set is available. To directly estimate the KL divergence from p (line 6), we exploit the identity DKL(p \u03c0) = \u2212 log Z + 1/Z E x\u223cq(x) \u03c0(x) . See \u00a7B.1 for derivations and a comparison with using Total Variation Distance (TVD) for assessing divergence.  Results: Figure 2 shows the evolution of the metrics over training steps, aggregated across the 9 + 4 + 4 = 17 experiments. We observe the following: the baseline RE-INFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of E \u03c0 \u03b8 \u03c6(x) at the expense of a very large deviation from the original GPT-2. High values of D KL (\u03c0 \u03b8 |a), are translated into low Dist-1 and very high Self-BLEU-5 indicating degeneration and lack of diversity. REINFORCE P(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of E \u03c0 \u03b8 P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines. 12 In the case of ZIEGLER we can see a positive effect of the interpolation factor \u03b2 between the reward and the KL penalty in the objective function. In the aggregated experiments reported here, the reward is slightly better than with GDC, but with inferior diversity scores (see also Fig. 4, showing that GDC produces richer vocabulary), and the stability is much worse (a detailed view of each experiment is provided in \u00a7H, showing more clearly the instability of this baseline). A complementary evaluation is provided by Figure 3, focusing on the ability of \u03c0 \u03b8 to converge to the optimal distribution p. We see that GDC is superior to all baselines in terms of D KL (p \u03c0 \u03b8 ) and also much more stable.\nIn summary, in these experiments, we see that with GDC the constraint expectation E \u03c0 \u03b8 \u03c6(x) smoothly increases while \u03c0 \u03b8 maintains the lowest divergence from GPT-2, becomes closest to the optimal p, and has the best diversity scores overall. On the other hand, we also note that at the point where we stop training (30K steps), the average over experiments of E \u03c0 \u03b8 \u03c6(x), while still increasing, does not reach 100%, an issue that we discuss at the end of the paper ( \u00a74).\n\nDISTRIBUTIONAL AND HYBRID CONSTRAINTS EXPERIMENTS\nAs formalized in \u00a72, GDC permits to define pointwise and distributional constraints as well as any mix between them. This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b). We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2 bio ) ( \u00a7G gives additional details). The bias in GPT-2 bio is significant: we calculated that this model generates only around 7% female biographies. It also displays a large imbalance between professions related to \"Science\" (1.5%), \"Art\" (10.0%), \"Business\" (10.9%) and \"Sports\" (19.5%).\n\nExperiment 1: Single Distributional Constraint\nWe use the distributional constraint E x\u223cp \u03c6 f emale (x) = 0.5; GDC is able to reduce the bias of GPT-2 bio to obtain 35.6% female biographies rather than only 7.4% (see Fig. 2 for this experiment and the next ones). Experiment 2: Multiple Distributional Constraints We then test our framework with several distributional constraints of different values and control directions. We specify four distributional constraints all at once with the goal of increasing the expectations of \"science\" and \"art\" to 40% and decreasing those of \"sports\" and \"business\" to 10%. GDC is able to increase the expectations of the first two professions respectively from 1.5% to 20.3% and from 10 to 31.6% and to decrease those of \"business\" and \"sports\" respectively from 10.9% to 10.2% and from 19.5% to 11.9%, reaching expectations close to the desired ones for all features using a single training method. Experiments 3,4,5,6: Hybrid Constraints Here we want to de-bias the model as in the previous case but we single out biographies of scientists, artists, etc. Formally, our requirements become E x\u223cp \u03c6 prof ession (x) = 1.0, a pointwise constraint, and E x\u223cp \u03c6 f emale (x) = 0.5, a distributional constraint. In those 4 hybrid experiments we can clearly see that GDC can address both pointwise and distributional constraints increasing each simultaneously with just the right amount to reach the desired expectations. Appendix \u00a7G further elaborates Fig. 2 (convergence curves).\n\nDISCUSSION\nOur approach to controlled text generation is distinguished by its breadth -the first one to handle distributional along with pointwise constraints, with applications to the important problem of Bias in pretrained LMs -and by the transparency of the supporting formalism. It decouples the training objective along two different dimensions. The first consists in solving the initial constraints specification, and leads through a direct algorithm to an optimal solution in EBM format. The second, where the real computational difficulty lies, consists in approximating this EBM with an autoregressive policy for use at inference time. Sampling from an EBM is an important, hard, and well-identified challenge in the literature. Our approach there consists in proposing a KL-adaptive version of the DPG algorithm, which exploits ascertained improvements of the trained policy to speed up convergence. This is an effective method for rare events, as we show in an ablation study ( \u00a7B.2  Our method does not suffer from degeneration, but our end policies still generate a number of samples not satisfying the constraints. A possibility, left for future work, might consist in filling the moderate residual gap with MCMC techniques, which would be guaranteed to reach our optimal p in the limit. We do not go this route here, but conduct an experiment (see \u00a7C) to better understand the nature of the problem. In the simple case of a single-word constraint (x includes \"amazing\"), we sample directly 1M samples from GPT-2 and keep the roughly 5K samples containing amazing (a variant of rejection sampling, taking two processing days). We then do a standard supervised fine-tuning of GPT-2 with these samples, stopping training when the CE validation loss starts to increase, and observe that this model exhibits a worse constraint satisfaction rate than ours. This experiment does not mean that a much larger fine-tuning dataset, obtained in this slow, non-adaptive way, would not reach better statistics, but it raises doubts about the ability of the GPT-2 architecture to fine-tune over such a non-standard constraint as containing a given word somewhere in its output.\nOverall, we believe that the proposed decomposition into two sub-problems is a methodological advantage compared to most other works, which directly aim at training a policy with the goal of improving certain evaluation metrics, but without clearly defining what qualifies as an optimal solution. The computational challenge of fully bridging the gap between the optimal EBM and an efficient sampling engine remains, and we hope that the formalism we propose, along with initial applications and experimental validations, will motivate further research along these lines.\n\nACKNOWLEDGMENTS\nWe would like to thank the anonymous reviewers for their insightful feedback that helped enhancing the final version of this manuscript. We also thank Germ\u00e1n Our statement of Theorem 1 is actually a reformulation of two results in section 3 of Csisz\u00e1r & Shields (2004). Our property (A) is a simple notational transposition of their Remark 3.1 (p. 444). Property (C) is the Pythagorean Identity in their Theorem 3.2 (p. 442). Property (B) reformulates the last part of the same Theorem \"... and in general L \u2229 cl(E Q ) = {P * }\" in terms of a limit of a sequence of distributions.\nNote: Csisz\u00e1r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975 , so the exponential factor is a constant, which proves that P (x) = a(x)b(x) is proportional to P (x), and therefore p(x) \u221d P (x).\n\nA.3 INCREMENTALLY ADDING NEW CONSTRAINTS\nAn interesting question 13 is whether the process explained in \u00a72 can be made incremental: if one has already computed a p and a \u03c0 \u03b8 relative to a certain number of constraints, can one add a new constraint without restarting the whole process from scratch? The answer is yes, and here we provide some formal elements to understand why.\n\nA.3.1 TRANSITIVITY PROPERTY OF GENERALIZED MAXENT\nAccording to (Csisz\u00e1r, 1996), the Generalized MaxEnt of sections \u00a72.1 and \u00a72.2 has the \"Transitivity property\". In our notation, this says that if we have k > k constraints, with C the manifold of distributions respecting only the first k constraints, C the manifold respecting all k constraints (hence C \u2282 C), then the maxent projection p of a onto C can be obtained by first projecting a onto C, obtaining p, and then projecting p onto C , obtaining p . In particular, the k lambdas associated with p can be directly reused as the first lambdas of the k lambda's associated with p . (Csisz\u00e1r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider. The proof, illustrated in Figure 5, is very similar to one of the proofs for the transitivity of the orthogonal projection in Euclidean geometry. Proof. In the Figure, p is the information projection (Csiszar's terminology for the Generalized Maxent) of a onto C, as before. Let's define r to be the projection of p onto C . We need to prove that r is identical to the projection p of a onto C . We consider an arbitrary distribution c in C , and apply the Pythagorean Identity of Theorem 1 three times. Because p is the projection of a onto C, we have D KL (r, a) = D KL (r, p) + D KL (p, a) and also D KL (c , a) = D KL (c , p) Putting these three facts together, we find that D KL (c , a) \u2265 D KL (r, a).\nAs c is an arbitrary point of C , this proves that r is the projection of a onto C , in other words, r = p .\n\nA.3.2 TRANSITIVITY AND AUTOREGRESSIVE POLICY\nDue to the Transitivity property, when calculating the EBM representation, it is possible to start from p without re-fitting p from scratch. However the move from EBM to autoregressive policy of \u00a72.3 remains to be discussed. The question now is the following. We have already obtained a policy \u03c0 \u03b8 approximating p, and we are interested in obtaining a policy \u03c0 \u03b8 approximating p : is it advantageous to start Algorithm 1 with q = \u03c0 \u03b8 , rather than starting \"from scratch\" and taking q = a ? Intuition says \"yes, very probably\", because \u03c0 \u03b8 is by construction an approximation to p, which is closer than a to p (formally, D KL (p , p) \u2264 D KL (p , a), see Fig. 5, where p = r). Due to the approximation, we only have D KL (p , \u03c0 \u03b8 ) D KL (p , p) , so a formal proof that \u03c0 \u03b8 is superior to a as a starting point is impossible, but we expect that further experiments would confirm the improvement.\n\nB MORE ON ADAPTIVITY B.1 DETAILS ON KL-ADAPTIVITY\nIn this section we provide details on the comparison step in our KL-Adaptive version of the DPG Algorithm, introduced in section 2. We want to assess whether the current \u03c0 \u03b8 is closer than q to p, and if the test is positive, we set \u03c0 \u03b8 as the new proposal, hoping to make the proposal more effective for importance sampling.\nThere are several ways to compute similarity between distributions, two of the most popular ones being on the one hand KL-divergence and on the other hand Total Variation Distance (TVD)where TVD(p||p ) . = 1/2 x |p(x) \u2212 p (x)| -which is often used in probability and MCMC theory. 14 Calculation of these metrics relative to p is not straightforward since the distribution p \u221d P is only implicitly represented by the unnormalized EBM P , and we cannot easily obtain direct samples from p. In this section we describe a workaround.\nGiven P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows: We can then compute D KL (p||\u03c0) as: Similarly, for TVD(p||\u03c0): In \u00a7B.2 we run an ablation study to compare the use of D KL on line 6 of Algorithm 2) or its replacement by TVD.\nFor both metrics, we need an estimate of Z. The precision of this estimate depends on the sample size and the quality of the proposal distribution q. We calculate a moving average estimate Z MA of Z is used inside the estimations of D KL (p \u03c0 \u03b8 ) and D KL (p q) (Algorithm 3, lines 7 and 8). Z MA is updated at each iteration of the training, and the moving average estimate is valid due to the fact that\u1e90 i , based on K samples, is an unbiased estimate of Z, and therefore so is Z MA . In this way, the estimate benefits from all the samples being produced during the course of the training; and also because the proposal distribution q evolves and gets closer to the target distribution p, the quality of the estimates of both D KL (p||\u03c0 \u03b8 ) and Z MA through importance sampling increases (equation 7). A similar approach is taken in the case of TVD (not shown).\n\nAlgorithm 3 KL-Adaptive DPG (detailed)\nInput: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: ZMA \u2190 0 Initialize Moving Average estimate of Z 3: for each iteration i do Update moving average estimate of Z Estimate on the K samples Estimate on the K samples 11: ifDKL(p||\u03c0 \u03b8 ) <DKL(p||q) then\n\nB.2 ABLATION ON ADAPTIVITY\nHere we run an ablation experiment on the adaptivity step of KL-Adaptive DPG ( \u00a72). We compare three variants of our proposed method: DPG-KLD, which uses KL divergence from the target distribution p to measure the quality of the trained policy \u03c0 \u03b8 i.e. if D KL (p \u03c0 \u03b8 ) < D KL (p q) we update the proposal distribution q \u2190 \u03c0 \u03b8 . DPG-TVD is similar but with the total variation distance instead (TVD). In non-Adaptive the initial proposal q is kept fixed during training.\nWe run 3 point-wise experiments with single word constraints of three rarity levels in the original GPT-2 distribution, namely: \"Vampire\" (1/10 4 ),\"Paris\" (1/10 3 ),\"US\" (1/10 2 ) .For each we use 3 different seeds and train for 10k gradient updates. Figure 6 shows training trends of the three ablations. We find a significant difference in convergence speed in favour of the adaptive methods. The efficiency gap between Adaptive and non-Adaptive methods becomes larger the more rare the constraints are. i.e. the proposal distribution q starting point is very far from the target distribution p, as the efficiency of the DPG algorithm is related to how close the proposal q is to the target p. When q is continuously adapted, the proposal distribution becomes closer to p and the training becomes efficient regardless of how far the initial proposal distribution is from p. We observe similar convergence rates for DPG-KLD and DPG-TVD. : Ablation experiment elaborating the effectiveness of the adaptive step in the DPG algorithm explained in section 2. We compare three adaptivity variants, based on the KL divergence (DPG-KLD), on the TVD distance (DPG-TVD) and with no adaptation. We find similar convergence rates for both KLD and TVD adaptive DPG compared to a much slower convergence without adaptation.\n\nC CAN STANDARD SUPERVISION FULLY SATISFY THE CONSTRAINTS?\nIn this section, we try to better understand potential difficulties of autoregressive models to fully satisfy constraints such as the ones illustrated in our pointwise experiments.\nTo this end, we consider whether a standard fully supervised fine-tuning of GPT-2 can achieve that objective while keeping a minimal distance from the initial model. To answer the question, we carry out an experiment where we fine-tune GPT-2 on a collection of samples satisfying the desired constraint. Our goal here is to investigate whether GPT-2 can fully satisfy the constraint without overfitting the fine-tuning data, since overfitting (memorizing) the training data basically means high KL-divergence from the initial model.\nFor this experiment, we choose a single-word constraint with the word \"amazing\". We start by sampling 1M sequences from GPT-2 small -a process that took us roughly 48 hours -and keeping only the ones containing \"amazing\" (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)). We end up with a total of 4600 samples out of which we use 500 for validation and the rest for fine-tuning. This result suggests a relationship between training a policy reaching 100% and overfitting the training data. This hints at the difficulty of strictly imposing certain types of constraints on pre-trained language models without moving far away from the initial model. 15  The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint. The main points to note are: (1) REINFORCE is trying to find a distribution p R maximizing r(x) (meaning that p R lies on the C manifold), but this p R is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution p Z that interpolates (with a weight \u03b2) between a high average r(x) and the KL divergence from a; unless \u03b2 = 0, in which case we are back to REINFORCE, p Z does not satisfy the constraint and falls outside of the manifold. c(x) > 0 \u2192 r(x) = 1, or, equivalently s.t. Ex\u223ccr(x) = 1. The curved lines represent increasing levels of the KL divergence DKL(q, a). According to Reinforce, any distribution pR s.t. Ex\u223cp R r(x) = 1, that is, any distribution on C, is optimal. According to Ziegler, to each temperature \u03b2 > 0 is associated an optimal distribution pZ = arg min q \u03b2DKL(q, a) \u2212 Ex\u223cqr(x), which does not directly lie on C -this is because, as indicated in (Ziegler et al., 2019), this distribution is of the form pZ (x) \u221d a(x)e r(x)/\u03b2 , giving positive probability to all x's in the support of a, including to points not lying on C. Our own optimal p does lie on C by definition, while minimizing the KL divergence from a.\n\nD.2 COMPARISON AGAINST FURTHER BASELINES\nHere we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control. PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes. Unlike GDC, PPLM needs a prefix to perform its hidden-state updates. Thus, our approach is more general in the sense that any prefix can be used on the trained model at test time, rather than requiring prefix-specifc fine-tuning. CTRL is a large-scale language model (1.63 billion parameters and 14x larger than GPT-2 small) based on control codes for steering text style and content. For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020). The control codes used are \"Reviews Rating: 5.0\" and \"Reviews Rating: 1.0\" for positive and negative sentiment control, respectively. We use five different prefixes (or prompts) and generate 100 continuations given each prefix obtaining a total of 500 samples. It is worth noting that GDC is trained in the same way as described in the main text, i.e. without any knowledge of prefixes, and that we only use prefixes at test time with the saved checkpoint. The five prefixes used come from (Dathathri et al., 2020): \"The chicken \", \"The potato \", \"The lake \", \"The pizza \", and \"The horse \".\nWe use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019). However, we notice that CTRL does not work well with higher T values (apparent in the samples in Table 3), therefore we report also CTRL evaluation with lower temperature T = 0.5 and a repetition penalty \u03bb rep = 1.2 as reported in their paper.\nAs metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT-2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.1. We average all these metrics across the 500 continuations generated. Table 3 shows the results for positive and negative sentiment control experiments. As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL. As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task. Table 4 shows sample continuations from all three approaches. Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.\nIt is also worth noting here that CTRL (and other control code methods) is very much limited in terms of its applications. For instance, to generate positive/negative sentiment text as we do in this experiment, we are required to use the ''Reviews Rating...'' control code, using control codes outside of those CTRL was fine-tuned on leads to very bad generations. This, in turn, restricts the generated text to positive/negative reviews although we may desire different types of positive/negative text (e.g. news reports). We can observe this effect 16 in some of the samples in Table 4 such as \"The chicken we just ordered from Amazon.com...\" and \"The pizza works no matter what settings you use it on.   (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) on positive and negative sentiment control. We generate 100 samples for each prefix obtaining a total of 500 samples. All metrics shown are averaged across the 500 samples obtained. CTRL refers to the shared setting across all approaches with temperature T = 1.0 and repetition penalty \u03bbrep = 1.0 and CTRL* refers to having T = 0.5 and \u03bbrep = 1.2. Here, we see a clear advantage of GDC in terms of constraint satisfaction and perplexity and a comparable performance in terms of diversity against PPLM and CTRL.\n\nPositive\nThe chicken is so tasty! This recipe has been in my freezer for about 2 months now. I have always had good quality chicken breasts, so I had a huge amount of chicken and The potato is my favourite part of the recipe, and it is also my biggest problem. It is a good addition to anything you try to add to your meal. I love potato soup The lake has been the most active place in the country for visitors since it opened last summer, and it's not hard to see why.\nThe lake has been a place to stay for years, The pizza place is great for a good time! They have all kinds of amazing sandwiches. My favorite sandwich is the \"Chicken Pizza\" which is the best I've ever had on my way to I don't think the pizza is better than any other pizza. It's not a The horse is very well balanced with the horse's head and ears. It is a great horse to have for the family. The horse is quite large and the tail is long enough to accommodate the\",\n\nNegative\nThe chicken is so bad it's literally going to die. It's like the dog is dying from a bad diet. I'm not even sure I want it in my kitchen\" The potato was really bad, but it's the worst potato that I've ever tasted. It was really hard for me to put my finger on. I was very disappointed with the flavor of the The lake was just too cold for the water and the wind to blow. I couldn't get out of it. I couldn't see anything. The wind blew through my windshield and my car windshield\" The pizza is so bad that I've had to buy it again, even if I was just going to order one at a time. I'm not going to be buying one for the next week The horse in the back row is not going to win the race. It is going to go up in flames, and the race will end in a heap of dust. The winner will be thrown\n\nPositive\nThe chicken and the egg story: the extraordinary rise of the powerful man as he transforms the lives of ordinary people in a world in crisis The story of the A man dressed in a white suit The potato. It is the world's most awesome people, and and and the that the the a the a the a the a the a the a the , \"and the the The lake is a great spot to enjoy the spectacular views of the Great Lakes. This is also a great place to take the children's swim. The lake is also a great place to hike in the beautiful mountains The pizza is a delight! I have never had this before. I am a fan of Italian, and I have not had it before in the States. I will be back! It was a great experience The horse is a powerful, beautiful, and extremely intelligent animal., (.C,:,.,.,../.../..',,' (., \".\n\nNegative\nThe chickenpox epidemic of 1918-1920 in Britain was an acute and deadly disease that killed about 100,000 people worldwide, most of them infants. The 1918-1919 epidemic was caused by the The potato is one of those things we all dream of. I think the most common thing that people come up with when I say I have the perfect one is the idea of a \"salt water\" version The lake is one one one. <endoftext>The United Nations (UN) and the European Union (EU) are among a number of the world's most in the state and,, on the House vote for The pizza crust is anvil, which is what the British have for a long time. The British Empire, the French, the the the the the a in the that is a a it is called and it The horse is in the saddle. That's how he's been for the last four years. The Tampa Bay Lightning won a series of three games in a row to begin the new year and into January we were\n\nPositive\nThe lake I am looking forward to seeing in September! The sea scene alone would have me watching again! Rating: 5.0 One of the best comedies I've seen. We will definitely watch it again. Smart and funny The horse for this ones lines is:&#34;The road to Hell is paved with good intentions. All roads to Hell end in Hell themselves.&#34; Rating: 5.0 I live in a small The potato were \"seeded\" during a European settlement. What the characters have gone through is inevitable, but extremely rare. (And the potato has the honor of being the world's oldest potato. For that honor, we have a nickname: \"@@ The chicken we just ordered from Amazon.com has not yet arrived and I am EXTREMELY EXCITED! The seller has the finest poultry in the market....plus, it is DELICIOUS!Thank you so The pizza has been around for decades. Now that time has been added to it, all of us can appreciate it better, and enjoy it the way we have always enjoyed.PERFECT Pie:(The second listen) And it\n\nNegative\nThe pizza works no matter what settings you use it on. The icecream maker always leaks out around the spout and onto the base (gross) -finally stopped working. I only wish I had spent more for a The horse can not be found. Characters whose names show up in the battle screen:EXE: SRMX&OY; SQX the knight \u00bfQWOKB SKOZY the warrior!A useful upgrade for a The lake has been made, but it's far from Earth 5. The ship has disappeared but they continue to radio.Ignoring the plot, which the Star Trek series never bothered with, Spock says that \"we should have followed up. There is The chicken died on me after 8 months. I don't think the unit is compatible with young chickens. Not recommended. Rating: 1.0 the plates didn't last long enough for me.I bought two of these plates and they The potato does not start from eggplants, it starts from the start of generation! How stupid is that! :( I bought this and many others to try with my toddler for his preschool class. I want him to get Published as a conference paper at ICLR 2021\n\nE RELATED WORK EXTENDED\nOptimizing global rewards for Text Generation There is a large reinforcement learning inspired literature about steering an autoregressive sequential model towards optimizing some global reward over the generated text. This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017). With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time. Some others use heuristic rewards as in (Li et al., 2016b;Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues. Other non-RL techniques for approximating the global sequence constraints \u03c6(x) by a biased estimator \u03c6(x t |x :t\u22121 ). These techniques usually referred to as weighted decoding Holtzman et al. (2018) KL Divergence penalty Another approach relied on penalizing too large deviations of the trained policy relative to the original policy. Jaques et al. (2017; propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model. This penalty acts as a regularizer to the optimization process that prevents the trained policy from deviating too much from the original policy. Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward. PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.\nPointwise vs. Distributional View Most of the existing works on Controlled Generation have taken what we have called a pointwise view: focusing on the quality of each individual output, as opposed to distributional properties of the collection of all outputs. And in fact, the standard objective of RL is to optimize a pointwise reward. Even when policy-gradient methods do consider distributions over outputs, they only do as a tool towards producing maximal rewards; and in fact, it is a side effect of the limited capacity of the policy networks that such distributions do not peak on a single output, as would be the optimal outcome in cases of real-valued rewards with no ties. 17 By contrast to this usual optimization \"intent\", our own intent here is explicitly distributional, and the policies we are looking for are not simply tools towards maximizing scores, but actual objectives in their own right.\nSuch a change of perspective might be argued against in the case of conditional seq2seq problems, such as Machine Translation, where focusing on a single good output for a given input makes sense, but is clearly in-adapted when focusing on language models where sample diversity is a requirement.\nEnergy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002;LeCun et al., 2006;Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago. 18 There has been a recent surge of interest in these types of models across a variety of fields. Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016;Belanger & McCallum, 2016). Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data. Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models. A recent survey of EBMs for text is provided in Bakhtin et al. (2020).\n\nF HYPERPARAMETERS AND TRAINING DETAILS\nWe implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019). For all experiments we start from a pretrained GPT-2 small (117M parameters) obtained from the Hugging-Face library (Wolf et al., 2019) and fine-tune for 3K gradient-update steps. Each training required 2 Nvidia V100 GPUs, the longest model took \u223c 72 hours to train. A list of the hyperparameters used for GDC and baselines is given in table 5. K refers to the number of gradient steps per iteration in Algorithm 2.\nN refers to the number of samples required and \u00b5 tolerance to the minimum tolerated error ||\u03bc \u2212 \u00b5(\u03bb)|| 2 2 while optimizing \u03bb, and \u03bb learning is the SGD step size for updating \u03bb in Algorithm 1. During training of the policy \u03c0 \u03b8 , we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with top p = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples. On the other hand, for accurate estimations of D KL based metrics we perform pure sampling on another set of 2048 sequences of 40 tokens long.\nFor word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository 19 . As for the sentiment and clickbait classifiers, we used their pre-trained classifier heads over GPT-2 medium 20 .\nFor distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2 bio . To detect if a given text is about a female gender, we construct \u03c6 f emale (x) as a simple rule-based discriminator that depends on the percentage of female personal pronouns (she, her, hers, herself) w.r.t. all mentioned pronouns. We define four types of professions \"Art\", \"Science\", \"Business and Politics\", and \"Sports\". To detect them, we define a wordlist for each type as shown in table 6.   Large pretrained Language Models are often trained on uncurated data from the internet, where several demographics are severely underrepresented. One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia's biographies (Graells-Garrido et al., 2015). It is expected that such bias is transferred if not amplified by Language Models. Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b;Brown et al., 2020b;Nadeem et al., 2020). This shows thaat Bias in LMs also shows up in different forms than just under-representation, and the task of debiasing LMs could require more a complex control method. GPT-2 bio demonstrates a large initial bias: over a large sample of size 20480 examples using top-p sampling (p = 0.9), it generates only around 7% female biographies. and a large imbalance between profession types \"Science\" (1%), \"Art\" (10%), \"Business&Politics\" (10%) and \"Sports\" (20%).\nIn this set of experiments, we demonstrate the potential of GDC as flexible general framework that can control pretrained Language Models to impose pointwise, distributional constraints, or even a mix between them (hybrid constraints). We design a set of 6 experiments whose descriptions and results are displayed in the figures below. Generation examples are provided in Table 7.\n\nGDC Desired\nFigure 10: Exp2: Multiple Distributional Constraints This experiment demonstrates the flexibility of GDC in dealing with several distributional constraints at once, even when these constraints have different objectives (increase, decrease, or keep fixed). We challenge the flexibility of GDC by setting four distributional constraints with four arbitrary expectation values targeting E\u03c6science and E\u03c6art at 40% and E\u03c6sports and E\u03c6 business at 10%. In the figure, from left to right, we can note the increase of E\u03c6science and E\u03c6art from 1.5% to 20.3% and from 10% to 31.6% respectively. Interestingly, the initial E\u03c6 business of GPT-2 bio (10.9%) is already very close to the desired expectation (10%), and we can see that during the course of the training, GDC keeps this value fixed as it is already satisfying the corresponding target distributional constraint. E\u03c6sports initially starts higher than the target distributional constraint 10%, and we can note that GDC succeeds to reduce it from 19.6% to 11.9%. In this experiment, we specify two types of constraints: pointwise with E\u03c6science(x) = 1.0 and distributional with E\u03c6 f emale (x) = 0.5. GDC in a single training procedure is able to increase the expectation of biographies about females from 7.4% to 28.8% and Science professions from 1.2% to 74.7%.\n\nArt Professions Biographies F\noraci mart\u00ednez rubin ( born october 24, 1982 ) is a puerto rican actress, dancer and model. she was the first puerto ... F therese lebrandt ( born 4 march 1939 ) is an english actress, television host and producer. she is known for her roles as lily lenox... , better known by his stage name zac banezi, is an israeli singer and songwriter. the producer of many artists, as well as the keyboardist of heavy metal band the.. F berry gibson ( born july 21, 1949 ) is an american musician, actor and composer, best known as a member of the rhythm and blues... balkrishnan dev is an indian actor who is known for his roles in telugu movies. he began his career with a short supporting role in \" sapikaya \". later he played .. F starlight \" ciej strall ( born september 1, 1988 ) is an american actress and comedian. she is best known for her role as el ... quentin brantley ( born april 27, 1973 ) is a canadian actor, composer, director, writer and producer. he is best known for his work.. \"\u00c1lvaro olajerra \" is an argentine comedian and actor. in 1983, he won an episode of c\u00e9spedes justicialiste de bola\u00f1os.. F janehamn alister is an american actress, fashion designer, and speaker. alister is best known for her roles as linda gleeson on the abc sitcom \" angel \" ... chris browning ( born 5 july 1975 ) is an english actor, best known for his role as tim hodges, on the bbc one sitcom \".. andy papadelaspe ( born 9 july 1973 ) is a french actor and director. he is known for his performances in several feature films including \" bern .. she served as deputy... ashaun \" tom \" hicks ( born july 28, 1986 ) is an american actress, singer, and beauty pageant contestant. he is also a journalist and .. izhev, born \" yuri aleksandrovich isov \" ( ; ), was a writer, journalist and politician. isov first became active in..\n\nSports Professions Biographies F\nisaba aguirre ( born 10 february 1983 in\u00c9ixidat, france ) is a female volleyball player from spain. she is a...\n\nH.4 TOKEN FREQUENCY ANALYSIS\nTo analyse in depth the effect of deviating much from the original GPT-2, for policies obtained from our method and each baseline, we obtain a large sample and filter to 4000 sequences that satisfy the imposed pointwise constraints for each of the 17 pointwise experiments explained in \u00a73. Figures  35, 36 and 37 plot a token frequency analysis for each of the training methods.\nThe vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.\nREINFORCE P(x) suffers from a token diversity issue. As noticed and confirmed by generated examples shown section H.5, it often concentrates all the sequence probability mass on a single sequence which is often fluent and satisfies the constraint; however this leads to an extreme loss of sample diversity in almost all experiments. This shows the usefulness of our proposed analysis -in addition to the self-BLEU metrics -for distinguishing diversity at the sequence level or at the distribution level. Similarly, ZIEGLER (Ziegler et al., 2019) often suffers from the same lack of sample diversity (5 out of the 17 experiments); GDC obtains the highest diversity amongst all baselines, as demonstrated by the long tail in the figures below. It is important to note here that low sample diversity is also captured by the KL deviation from the original GPT-2 model i.e. D KL (\u03c0 \u03b8 a); GDC identifies the target distribution as the one which minimally deviates from the original policy while satisfying the constraints (p = arg min q\u2208C D KL (q, a)) is thus expected to preserve the high sample diversity of the original GPT-2.    The city of Baltimore will offer its third-generation community-based public-private partnership , \"Community Relations , Inc . , \" to build more than 1 , 1 0 Greece . The eurozone-wide unemployment rate plunged to 1 . 3 percent in June and remains below the EU average of 2 . 4 percent 1 0 Winnipeg Jets Injury Update : RW RW Blake Wheeler Winnipeg Jets Injury Update : RW RW Blake Wheeler Tampa Bay Lightning In 1 0 \"We know that if there's a way out of these problems , it's not by having a single one of them , \" he says 1 0 1 Clean Episode #2 --Sledgehammer 5 : The Longest War in the World! In this special episode , the Sledgehammer 5 team discusses their 1 0 A man who took a photograph of a police officer wearing a bulletproof vest and said it was him was charged with assault causing bodily 1 0 In a very big way , I like this book . The only difference here is that I got an amazing story from Jack . 1 0 I think we should be building the same thing for everyone . A shared economy that creates jobs and a shared supply of energy . Ziegler 1 0 \"There is no way I can do that . And that's not a small thing , \" he told the Guardian . \"So I have 1 0 . The first person I ever spoke with about it is a big fan . \"I thought it was pretty cool . I love everything 1 0 This is an easy tutorial to get started with the Django application . Once you understand how the Django application is implemented , you can 1 0 When you're a student with one of the most popular online courses available , you may find it easy to fall in love with what 1 0 BRAINSTOCK The UK could be on the cusp of becoming the first in the world to have its own free market . Bobby Bould 1 0 \"We have a lot of good options that will enable our employees to compete better , improve our efficiency and create more value for the 1 0 \"That was like a lot of good times to me . \" He says . The group of five men in their late 30s went 1 0 You can view all posts of this blog here Table 8: Randomly selected generations from the single-word constraint task for the word \"Wikileaks\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 9: Randomly selected generations from the single-word constraint task for the word \"Vampire\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got an e-mail from a couple of folks that we found interesting and amusing . They asked if I could have an idea of 1 1 The \"Black Friday\" holiday has some amusing details about the price of goods on Thanksgiving weekend , and they are included in the holiday's list 1 1 \"It was amusing and very amusing for all of us to witness , \" he said . \"But it also was not a good time Korea's first president has said he will resign after he failed to reach agreement with North Korea on the group's nuclear programme and warned he 1 0 A group of students in the United States were arrested this week , on charges of criminal sexual misconduct , after they allegedly engaged in 1 0 Gigabyte has partnered with Intel to provide Linux developers with a full-text search engine , which can be used to find Linux-related documents . In 1 0 \"The real story is that , this time , it's really been about women's rights , \" Trump said . \"The real story is , 1 0 RICHMOND , Va . (WPRI) -Three people were killed and two others were injured when a bus was derailed Thursday morning at Union Station 1 0 U . S . Department of Energy's National Renewable Energy Laboratory (NREL) will begin pumping the first water from California reservoirs in a month in 1 0 . Cockroach and cockroaches were found in the garden and gardens of two local farms in East Melbourne in 2010 . A farmer who worked Ziegler 1 1 I really don't know why she was so excited about the \"I'm going to be in my own game . \" It was amusing to 1 1 You can see , the whole point of this post is to get back to the \"What is it all about ? \" point . 1 1 \"You know , it's all that has happened in a couple of weeks in the last two weeks , \" said Smith . \"It's amusing 1 1 Consequences of the War . I will not answer any questions . However it is amusing to see how many \"fancy\" books have been published 1 1 In fact , I'd say that this game is the closest thing I've ever seen to the real life story of the main characters . 1 1 The only thing more amusing , however , was to see how it went down . The last person who ever read this piece would 1 1 It may be an amusing fact that the American Society of Pediatricians and Surgeons does not endorse circumcision . However , it is actually the 1 0 Cannot be created with your username Cannot be created with your username Cannot be created with your username Cannot be created with your username Can't Table 10: Randomly selected generations from the single-word constraint task for the word \"amusing\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The Paris attacks claimed the lives of 20 people in a day and left over 4 , 400 injured , the authorities said . The 1 1 In Paris , a major tourist attraction in the Middle East with a long history of terrorist attacks , the Charlie Hebdo massacre and the 1 1 As the Paris attack unfolded , the European Union and the U . S . took to Twitter to describe the attack . A tweet 1 1 The Paris massacre in November 2012 was carried out under a pretext of preventing terrorism . But on this basis , the attackers knew nothing 1 1 In Paris on Monday , a delegation of 50 members of the European Commission was set to discuss the issue of the EU's plan to 1 1 In his Paris address , President Hollande pledged to work with France to fight \"the scourge of terrorism . \" On Sunday , in a 1 1 A man who allegedly attacked a girl in Paris was sentenced to 15 years to life in prison for killing three children in 2012 , 1 1 Cairo , July 18 -The Paris terrorist attacks , which killed 14 people , killed 16 , wounded 13 more and left a third Table 11: Randomly selected generations from the single-word constraint task for the word \"Paris\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 In 2014 , in an attempt to stop the restaurant industry from becoming a \"corporate welfare racket\" for the masses , the city of San 1 0 A New Jersey man was arrested early Thursday morning on suspicion of possessing a gun and was placed under investigation by the police department , 1 1 SINGAPORE -A sushi restaurant owner has been jailed for 10 years for allegedly stealing money from a customer during the summer . A witness 1 1 The restaurant 's owner , James Saito , was suspended without pay last month after he said he accidentally broke the glass in front of a 1 1 A local restaurant chain on Monday announced its intention to offer a variety of meals and snacks to customers in the form of ice cream 1 1 I've never been in a restaurant before , but the atmosphere at the restaurant was very different than I remembered . And with only a 1 1 Watchers was founded in 1993 by a restaurant co-owner who wanted a place that had a true Southern feel . The restaurant opened on June 1 1 A restaurant in the heart of the San Antonio area has been turned into an art gallery by a local entrepreneur . The restaurant in San Antonio , Texas is known for a \"Southern Texas food\" philosophy that has given it its name , according to the 1 1 We've had a lot of success with this , and a lot of great things . There's this restaurant . We were all over it 1 1 I'm really pleased with my purchase! The menu was the same with a lot of restaurant options and I couldn't say enough good things about 1 1 \"I wanted to bring this restaurant to town , \" said Jim Dorn , who manages the restaurant 's business department . \"I knew we were 1 1 The world's oldest restaurant chain , the Cinco de Mayo , offers a mix of comfort food and classic Southern hospitality with its iconic Italian 1 1 Saucer has been offering the restaurant the chance to offer a one-hour service for all its guests , but not necessarily at a premium . 1 1 SALT LAKE CITY -Three Utah restaurant owners have filed suit to force restaurant owner Jimmy Denny to close after his company failed to report 1 1 Fellow restaurant owners , remember that while every once in a while a friend invites you to his or her own restaurant , you never Table 12: Randomly selected generations from the single-word constraint task for the word \"restaurant\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nWe are doing this in collaboration with you! We've done amazing work to make Minecraft an amazing game . However , in the past , 1 1 This game is amazing ! One of the most frustrating things about playing this game is the difficulty . There is no leveling system , and 1 1 A team of Japanese scientists has found that the world's largest nuclear plant could be a disaster waiting to happen . \"This amazing discovery reveals 1 0 So there we were , looking at a gorgeous game . That was something I enjoyed when I played a bit of a Zelda , 1 1 I just found out about this and am super excited to get it for you guys! Its amazing how many great games I can find 1 1 Thanks to amazing support , you have had access to this content for years , but have it been delivered to you in the form 1 1 What an amazing time to be a professional football fan! WeWe're sure John and John 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't do our share of 11 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John would have 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're also pretty sure John 18 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John and John 2 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't get enough of the Ziegler 1 1 We need to make sure that this type of work will be shared . The amazing and talented team at Google has just announced a 1 1 I've been waiting for this amazing piece of artwork since I heard of it on the New York Times' \"The Art of Comic-Con 2012\" podcast 1 1 I love this site because I'm trying to find the right answers to every question I have as a designer . It's amazing how much 1 1 The New York Times is going to be out doing something amazing with its coverage of the presidential election . The paper is already releasing 1 1 You'll see a lot of amazing video games coming out of Sony's booth , all from Sony and Nintendo , in the coming months . 1 1 The New York City FC academy and its training group were both awarded two year contracts in 2014 with an amazing $2 . 5 million 1 1 My favorite part of this tutorial is when you watch it , the amazing detail of the line up . It's so fun to watch 1 1 You have amazing taste , can be enjoyed by yourself or others . Our wines are not for sale , so this is for you Table 13: Randomly selected generations from the single-word constraint task for the word \"amazing\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.   Table 15: Randomly selected generations from the single-word constraint task for the word \"China\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The US has announced that it will launch a new drone war game for the game \"Call of Duty : Black Ops 2 , \" 1 1 A group of Chinese-Americans has sued US President Donald Trump in a bid to force him to pay their former student visa fees . Chinese 1 1 A U . S . Army soldier who was killed in Iraq is the second US soldier to be killed in the country since January 1 1 Haitian officials are trying to make sure the US forces who stormed Iraq will be held responsible for their actions . They want the US Table 16: Randomly selected generations from the single-word constraint task for the word \"US\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 17: Randomly selected generations from the word-list constraint task for the kitchen word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got these last year when they were $500 , but I didn't get a monster when they went out in 2012 , so this 1 0 A man who appeared in a video calling on supporters to be loyal to the Muslim faith is being attacked by an attacker who then 1 1 The ghost of her father is here , and it's time to get a ghost back . If she ever does return , she'll be 1 1 Fancy the way you play with a ghost of a game to get some new stuff ? Get it here! Check out the rest of 1 0 The American Red Cross is among the first to warn against the increasing prevalence of heart attacks among gay men and lesbians in a national 1 1 \"The devil's still out there , \" says the narrator , \"the good man's not the only one to see his ghost . 1 This is a great way to explore the life of this world . I was a very happy person , happy because I was the 1 1 I'll get into the beast of the bush in a bit , but in the last few minutes I've got a pretty good feel for 1 1 I am a big fan of the fantasy genre , but that is a topic for another time . I can tell you that I 1 1 In the years that followed , the Internet was transformed by the advent of the Internet in 1999 , with Facebook (FB) and Google (GOOGL) 1 1 A strange ghost is haunting the ruins of ancient Babylon . In one of those horror movies , a ghost is caught in a mysterious 1 0 \"We're seeing that now in the case of Syria , \" the judge said . \"That's why the State of Canada should not take it 1 0 \"The world should stop playing dead . The world should start playing alive . \" That was the line of the voice that emerged from 1 1 I just wanted to try it out . I'm so excited about it and just started a new game , and it works . It's In a major development in government's attempt to block further progress in the process of nationalisation of its commerce , the state government , in 1 1 The government may not prosecute a group of government-owned enterprises for its political , economic , or administrative purposes in its national economy . Article 1 1 The United States government has ordered a court order to enforce state laws or governmental power over the personal conduct of its political subdivision in 1 1 The government has ordered an order on its release of a dozen government ministers in attempts to block its operation in judicial proceedings in its 1 1 The state government's monopoly on its economic power over the political , economic , or administrative process in order of its citizens in order to 1 1 In its attempt to block access to the state government in its political action , government made an attempt to restrict economic activity in order 1 1 The government will invoke its powers against the government in court of India against its order seeking a order in its internal order in its 1 1 In its campaign against economic independence in its efforts to enforce an effective state monopoly on its political power in its state , the Government REINFORCE P( It has taken several years for the government to finally acknowledge the real issues facing the Australian population . This is because the most pressing 1 0 We had hoped that the election would be a simple one-sided affair between those who don't support the Republican Party and those who do . 1 1 The government of Saskatchewan has a long history of lobbying on behalf of business interests . The province recently passed an omnibus tax bill that 1 1 The NDP has taken the issue of whether the state has a \"fundamental right\" to free trade to the forefront in its annual platform , 1 1 By Steve Nelles More than two-thirds of Texans are expected to sign off on the state's future tax code in January , with a possible 1 1 An appeals court in Ohio ruled Monday that the state's refusal to allow a transgender employee to use the state bathroom of her choice violated 1 1 The government will set aside $2 . 4 billion to fund more than 800 schools in the South African state , including many in the  Table 20: Randomly selected generations from the word-list constraint task for the computers word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I have to say I was impressed with the way the writing and narration was done . The way they were presented , especially the 1 0 'I'm thrilled to say my team is on the way!' tweeted Sadiq Khan . The London Mayor is joining the \"Great London Olympics\" movement to 1 1 You are going to enjoy this book! It is a beautiful collection of beautifully detailed stories . It is a treasure trove of information for 1 0 It's a fascinating conversation that we have in the world of cryptocurrency . It's so much fun . The people who have been running the 1 0 Tired of waiting for the next best thing to happen , you know it . You want to know . We are dedicated to helping 1 1 We love your feedback , so we are pleased to bring you the most powerful and best-selling product that will satisfy your needs and your 1 1 \"Thank you all for the service this site gives me , \" he said . \"Thank you for the work I've been doing with the 1 1 \"The most amazing thing about this game is that there is no other games that have been released like this . It has such a REINFORCE 1 1 Enhanced performance with our world-renown world-renown exhibitions worldwide . We believe our clients with extraordinary audiences of our highest quality productions productions of outstanding international 1 1 Dramatic high quality performance quality products of leading global international audiences of the highest quality high quality high quality international leading worldwide markets leading global 1 1 Create beautiful stunning gifts of extraordinary quality gifts of beautiful high quality quality productions of the highest quality premier productions worldwide impact worldwide reach quality 1 1 Designed with the highest quality quality performance materials of our clients' top quality talent clients' top brands' leading global brands' leading worldwide attention-grab worldwide audiences 1 1 High quality artistry of the highest quality quality productions of worldwide worldwide world-renown audiences of world-renown worldwide audiences worldwide acclaim highest quality productions of our 1 1 Explore stunning quality productions of highest quality international premier excellence of top international premier quality international audiences' highest impact productions of the highest global highest 1 1 Highquality high quality productions with outstanding quality quality productions together the highest value clients' highest quality and highest level highest impact performance of our clients' 1 1 High quality quality artistry of quality high quality production value . The highest quality product highest quality productions of our customers' highest quality customers' highest REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1 1 I really have to say this about the two albums that I've been getting : \"Walking on Water\" and \"The Road . \" They're both 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 3560 1 Be the first to know . No one covers what is happening in our community better than we do . And with a digital subscription 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Table 21: Randomly selected generations from the classifier-based constraint task for very positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. 0 \"These are the kind of people we're going to have in our community for years to come , \" said Donny , the father of 1 1 \"A great book , \" said Mr . Moore , who has been writing an introduction to the work . \"But it is a wonderful 1 1 The great question of all time is \"who would have guessed that this was so different and fun ? \" This is the question I 1 1 \"I'm a big fan of all kinds of things and I can say that I've always been an avid fan of everything . The team 1 1 Today , it's nice to be back in the game! I want to offer some great games to show your support for your favourite artists 1 0 Categories Categories Select Category A Very Important Stuff A Very Important Thing You Need To Know A Very Important Thing You Should Know A Very REINFORCE 1 1 Our Mission is bringing together the best culinary adventure of this year's National Holiday is a wonderful celebration of true love , with which I 1 1 Our newest dish is Celebrate Our Harvest is bringing together a celebration of celebrating our unique culinary culinary journey and adventure has inspired us to 1 1 Our Mission is to Help Bring Together the best Korean Heritage and Celebration has inspired by our love and support for the Korean Heritage Tour 1 1 Our annual Taste and Taste brings together incredible culinary treats with wonderful ingredients to give us that we know we have , loved and enjoyed 1 1 Our special fundraiser to welcome our wonderful friend , The Red Queen is hosting a celebration and honor this wonderful gem is all deserves is 1 1 Our unique and eclectic evening celebrates our love for love has inspired us this year to share the joy and joy our little ones have 1 1 Our Mission at the Great Black History & Cultural Center celebrates the true story of our great African American has brought together a creative exploration 1 1 Our Mission is bringing together events and fun events that bring together a truly unique gift with this wonderful event brings together such amazing people REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 Our team has long supported the idea of using your knowledge and talents to make a more efficient , effective and sustainable way of making 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 The 2017 Season is about to roll out a big , fun , and exciting new lineup with the addition of a very special guest 1 1 \"I'm happy that he took his time and let everyone know that I'm going to take the same steps as everyone else with the same 1 1 This is a great day for those who love art , poetry , and the world to get together and have a great time . 1 1 Gather up the best and best food at an affordable price . We offer a wide selection of vegan and vegetarian options and all our 1 1 The latest in our series of guides for working with digital artisans . We offer a number of free tools , including Photoshop and Illustrator Table 22: Randomly selected generations from the classifier-based constraint task for positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\n</current_paper>\n\n<cited_paper>\nAn Actor-Critic Algorithm for Sequence Prediction\n\nWe present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.\n\n\nINTRODUCTION\nIn many important applications of machine learning, the task is to develop a system that produces a sequence of discrete tokens given an input. Recent work has shown that recurrent neural networks (RNNs) can deliver excellent performance in many such tasks when trained to predict the next output token given the input and previous tokens. This approach has been applied successfully in machine translation (Sutskever et al., 2014;, caption generation (Kiros et al., 2014;Donahue et al., 2015;Xu et al., 2015;Karpathy & Fei-Fei, 2015), and speech recognition Chan et al., 2015).\nThe standard way to train RNNs to generate sequences is to maximize the log-likelihood of the \"correct\" token given a history of the previous \"correct\" ones, an approach often called teacher forcing. At evaluation time, the output sequence is often produced by an approximate search for the most likely candidate according to the learned distribution. During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors . This can become especially problematic for longer sequences. Due to this discrepancy between training and testing conditions, it has been shown that maximum likelihood training can be suboptimal Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth reference outputs from the data. This gives rise to the challenging problem of determining the target for the next network output.  use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al. (2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens 2 BACKGROUND We consider the problem of learning to produce an output sequence Y = (y 1 , . . . , y T ), y t \u2208 A given an input X, where A is the alphabet of output tokens. We will often use notation Y f ...l to refer to subsequences of the form (y f , . . . , y l ). Two sets of input-output pairs (X, Y ) are assumed to be available for both training and testing. The trained predictor h is evaluated by computing the average task-specific score R(\u0176 , Y ) on the test set, where\u0176 = h(X) is the prediction. To simplify the formulas we always use T to denote the length of an output sequence, ignoring the fact that the output sequences may have different length.\nRecurrent neural networks A recurrent neural network (RNN) produces a sequence of state vectors (s 1 , . . . , s T ) given a sequence of input vectors (e 1 , . . . , e T ) by starting from an initial s 0 state and applying T times the transition function f : s t = f (s t\u22121 , e t ). Popular choices for the mapping f are the Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) and the Gated Recurrent Units (Cho et al., 2014), the latter of which we use for our models.\nTo build a probabilistic model for sequence generation with an RNN, one adds a stochastic output layer g (typically a softmax for discrete outputs) that generates outputs y t \u2208 A and can feed these outputs back by replacing them with their embedding e(y t ): (1) s t = f (s t\u22121 , e(y t )).\nRNNs for sequence prediction To use RNNs for sequence prediction, they must be augmented to generate Y conditioned on an input X. The simplest way to do this is to start with an initial state s 0 = s 0 (X) (Sutskever et al., 2014;Cho et al., 2014). Alternatively, one can encode X as a variable-length sequence of vectors (h 1 , . . . , h L ) and condition the RNN on this sequence using an attention mechanism. In our models, the sequence of vectors is produced by either a bidirectional RNN (Schuster & Paliwal, 1997) or a convolutional encoder (Rush et al., 2015).\nWe use a soft attention mechanism  that computes a weighted sum of a sequence of vectors. The attention weights determine the relative importance of each vector. More formally, we consider the following equations for RNNs with attention: where \u03b2 is the attention mechanism that produces the attention weights \u03b1 t and c t is the context vector, or 'glimpse', for time step t. The attention weights are computed by an MLP that takes as input the current RNN state and each individual vector to focus on. The weights are typically (as in our work) constrained to be positive and sum to 1 by using the softmax function.\nA conditioned RNN can be trained for sequence prediction by gradient ascent on the log-likelihood log p(Y |X) for the input-output pairs (X, Y ) from the training set. To produce a prediction\u0176 for a test input sequence X, an approximate beam search for the maximum of p(\u00b7|X) is usually conducted. During this search the probabilities p(\u00b7|\u0177 1 , . . . ,\u0177 t\u22121 ) are considered, where the previous token\u015d y 1 , . . . ,\u0177 t\u22121 comprise a candidate beginning of the prediction\u0176 .\n\nValue functions\nWe view the conditioned RNN as a stochastic policy that generates actions and receives the task score (e.g., BLEU score) as the return. We furthermore consider the case when the return R is partially received at the intermediate steps in the form of rewards r t : R(\u0176 , Y ) = T t=1 r t (\u0177 t ;\u0176 1...t\u22121 , Y ). This is more general than the case of receiving the full return at the end of the sequence, as we can simply define all rewards other than r T to be zero. Receiving intermediate rewards may ease the learning for the critic, and we use reward shaping as explained in Section 3. Given the policy, possible actions and reward function, the value represents the expected future return as a function of the current state of the system, which in our case is uniquely defined by the sequence of actions taken so far,\u0176 1...t\u22121 .\nWe define the value of an unfinished prediction\u0176 1...t as follows: We define the value of a candidate next token a for an unfinished prediction\u0176 1...t\u22121 as the expected future return after generating token a: We will refer to the candidate next tokens as actions. For notational simplicity, we henceforth drop X and Y from the signature of p, V , Q, R and r t , assuming it is clear from the context which of X and Y is meant. We will also use V without arguments for the expected reward of a random prediction.\n\n4:\nGenerate a sequence of actions\u0176 from p .\n\n5:\nCompute targets for the critic Update the critic weights \u03c6 using the gradient Update actor weights \u03b8 using the following gradient estimate Update delayed actor and target critic, with constants \u03b3 \u03b8 1, \u03b3 \u03c6 1\n\nACTOR-CRITIC FOR SEQUENCE PREDICTION\nLet \u03b8 be the parameters of the conditioned RNN, which we will also refer to as the actor. Our training algorithm is based on the following way of rewriting the gradient of the expected return dV d\u03b8 : This equality is known in RL under the names policy gradient theorem (Sutton et al., 1999) and stochastic actor-critic (Sutton, 1984). 1 Note that we use the probability rather than the log probability in this formula (which is more typical in RL applications) as we are summing over actions rather than taking an expectation. Intuitively, this equality corresponds to increasing the probability of actions that give high values, and decreasing the probability of actions that give low values. Since this gradient expression is an expectation, it is trivial to build an unbiased estimate for it: where\u0176 k are M random samples from p(\u0176 ). By replacing Q with a parameteric estimateQ one can obtain a biased estimate with relatively low variance. The parameteric estimateQ is called the critic.\nThe above formula is similar in spirit to the REINFORCE learning rule that Ranzato et al. (2015) use in the same context: where the scalar b t (X) is called baseline or control variate. The difference is that in REINFORCE the inner sum over all actions is replaced by its 1-sample estimate, namely is introduced to correct for the sampling of\u0177 t . Furthermore, instead of the value Q(\u0177 t ;\u0176 1...t\u22121 ), REIN-FORCE uses the cumulative reward T \u03c4 =t r \u03c4 (\u0177 \u03c4 ;\u0176 1...\u03c4 \u22121 ) following the action\u0177 t , which again can be seen as a 1-sample estimate of Q. Due to these simplifications and the potential high variance in the cumulative reward, the REINFORCE gradient estimator has very high variance. In order to improve upon it, we consider the actor-critic estimate from Equation 8, which has a lower variance at the cost of significant bias, since the critic is not perfect and trained simultaneously with the actor. The success depends on our ability to control the bias by designing the critic network and using an appropriate training criterion for it.\nTo implement the critic, we propose to use a separate RNN parameterized by \u03c6. The critic RNN is run in parallel with the actor, consumes the tokens\u0177 t that the actor outputs and produces the estimate\u015d Q(a;\u0176 1...t ) for all a \u2208 A. A key difference between the critic and the actor is that the correct answer Y is given to the critic as an input, similarly to how the actor is conditioned on X. Indeed, the return R(\u0176 , Y ) is a deterministic function of Y , and we argue that using Y to computeQ should be of great help. We can do this because the values are only required during training and we do not use the critic at test time. We also experimented with providing the actor states s t as additional inputs to the critic. See Figure 1 for a visual representation of our actor-critic architecture.\nTemporal-difference learning A crucial component of our approach is policy evaluation, that is the training of the critic to produce useful estimates ofQ. With a na\u00efve Monte-Carlo method, one could use the future return T \u03c4 =t r \u03c4 (\u0177 \u03c4 ;\u0176 1...\u03c4 \u22121 ) as a target toQ(\u0177 t ;\u0176 1...t\u22121 ), and use the critic parameters \u03c6 to minimize the square error between these two values. However, like with REINFORCE, using such a target yields to very high variance which quickly grows with the number of steps T . We use a temporal difference (TD) method for policy evaluation (Sutton, 1988). Namely, we use the right-hand side q t = r t (\u0177 t ;\u0176 1...t\u22121 ) + a\u2208A p(a|\u0176 1...t )Q(a;\u0176 1...t ) of the Bellman equation as the target for the left-handQ(\u0177 t ;\u0176 1...t\u22121 ). Figure 1: Both the actor and the critic are encoder-decoder networks. The actor receives an input sequence X and produces samples\u0176 which are evaluated by the critic. The critic takes in the ground-truth sequence Y as input to the encoder, and takes the input summary (calculated using an attention mechanism) and the actor's prediction\u0177 t as input at time step t of the decoder. The values Q 1 , Q 2 , \u00b7 \u00b7 \u00b7 , Q T computed by the critic are used to approximate the gradient of the expected returns with respect to the parameters of the actor. This gradient is used to train the actor to optimize these expected task specific returns (e.g., BLEU score). The critic may also receive the hidden state activations of the actor as input.\nApplying deep RL techniques It has been shown in the RL literature that ifQ is non-linear (like in our case), the TD policy evaluation might diverge (Tsitsiklis & Van Roy, 1997). Previous work has shown that this problem can be alleviated by using an additional target networkQ to compute q t , which is updated less often and/or more slowly thanQ. Similarly to (Lillicrap et al., 2015), we update the parameters \u03c6 of the target critic by linearly interpolating them with the parameters of the trained one. Attempts to remove the target network by propagating the gradient through q t resulted in a lower square error (Q(\u0177 t ;\u0176 1...T ) \u2212 q t ) 2 , but the resultingQ values proved very unreliable as training signals for the actor.\nThe fact that both actor and critic use outputs of each other for training creates a potentially dangerous feedback loop. To address this, we sample predictions from a delayed actor (Lillicrap et al., 2015), whose weights are slowly updated to follow the actor that is actually trained.\nDealing with large action spaces One of the challenges of our work is that the action space is very large (as is typically the case in NLP tasks with large vocabularies). This can be alleviated by putting constraints on the critic values for actions that are rarely sampled. We found experimentally that shrinking the values of these rare actions is necessary for the algorithm to converge. Specifically, we add a term C t for every step t to the critic's optimization objective which drives all value predictions of the critic closer to their mean: This corresponds to penalizing the variance of the outputs of the critic. Without this penalty the values of rare actions can be severely overestimated, which biases the gradient estimates and can cause divergence. A similar trick was used in the context of learning simple algorithms with Q-learning .\nReward shaping While we are ultimately interested in the maximization of the score of a complete prediction, simply awarding this score at the last step provides a very sparse training signal for the critic. For this reason we use potential-based reward shaping with potentials \u03a6(\u0176 1. for incomplete sequences and \u03a6(\u0176 ) = 0 for complete ones (Ng et al., 1999). Namely, for a predicted sequence\u0176 we compute score values for all prefixes to obtain the sequence of scores (R(\u0176 1...1 ), R(\u0176 1...2 ), . . . , R(\u0176 1...T )). The difference between the consecutive pairs of scores is then used as the reward at each step: . Using the shaped reward r t instead of awarding the whole score R at the last step does not change the optimal policy (Ng et al., 1999).\nPutting it all together Algorithm 1 describes the proposed method in detail. We consider adding the weighted log-likelihood gradient to the actor's gradient estimate. This is in line with the prior work by (Ranzato et al., 2015) and (Shen et al., 2015). It is also motivated by our preliminary experiments that showed that using the actor-critic estimate alone can lead to an early determinization of the policy and vanishing gradients (also discussed in Section 6). Starting training with a randomly initialized actor and critic would be problematic, because neither the actor nor the critic would provide adequate training signals for one another. The actor would sample completely random predictions that receive very little reward, thus providing a very weak training signal for the critic. A random critic would be similarly useless for training the actor. Motivated by these considerations, we pre-train the actor using standard log-likelihood training. Furthermore, we pre-train the critic by feeding it samples from the pre-trained actor, while the actor's parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2.\n\nRELATED WORK\nIn other recent RL-inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction.\nImitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (Daum\u00e9 Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, it's not always easy or even possible to construct an expert policy for a task-specific score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-specific score. The recently proposed 'scheduled sampling'  can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy.\nThere are also approaches that aim to approximate the gradient of the expected score. One such approach is 'Direct Loss Minimization' (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-specific score into account. Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel & Byrne, 2000;Shen et al., 2015;Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results.\nAnother recently proposed method is to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure itself (Wiseman & Rush, 2016). This method follows the more general strategy called 'learning as search optimization' (Daum\u00e9 III & Marcu, 2005). This is an interesting alternative to our approach; however, it is designed specifically for the precise inference procedure involved.\n\nEXPERIMENTS\nTo validate our approach, we performed two sets of experiments 2 . First, we trained the proposed model to recover strings of natural text from their corrupted versions. Specifically, we consider each character in a natural language corpus and with some probability replace it with a random character. We call this synthetic task spelling correction. A desirable property of this synthetic task is that data is essentially infinite and overfitting is no concern. Our second series of experiments is done on the task of automatic machine translation using different models and datasets.\nIn addition to maximum likelihood and actor-critic training we implemented two versions of the REINFORCE gradient estimator. In the first version, we use a linear baseline network that takes the actor states as input, exactly as in (Ranzato et al., 2015). We also propose a novel extension of REINFORCE that leverages the extra information available in the ground-truth output Y . Specifically, we use theQ estimates produced by the critic network as the baseline for the REINFORCE algorithm. The motivation behind this approach is that using the ground-truth output should produce a better baseline that lowers the variance of REINFORCE, resulting in higher task-specific scores. We refer to this method as REINFORCE-critic.\n\nSPELLING CORRECTION\nWe use text from the One Billion Word dataset for the spelling correction task (Chelba et al., 2013), which has pre-defined training and testing sets. The training data was abundant, and we never used any example twice. We evaluate trained models on a section of the test data that comprises 6075 sentences. To speed up experiments, we clipped all sentences to the first 10 or 30 characters.\nFor the spelling correction actor network, we use an RNN with 100 Gated Recurrent Units (GRU) and a bidirectional GRU network for the encoder. We use the same attention mechanism as proposed in , which effectively makes our actor network a smaller version of the model used in that work. For the critic network, we employed a model with the same architecture as the actor.\nWe use character error rate (CER) to measure performance on the spelling task, which we define as the ratio between the total of Levenshtein distances between predictions and ground-truth outputs and the total length of the ground-truth outputs. This is a corpus-level metric for which a lower value is better. We use it as the return by negating per-sentence ratios. At the evaluation time greedy search is used to extract predictions from the model.  We use the ADAM optimizer (Kingma & Ba, 2015) to train all the networks with the parameters recommended in the original paper, with the exception of the scale parameter \u03b1. The latter is first set to 10 \u22123 and then annealed to 10 \u22124 for log-likelihood training. For the pre-training stage of the actor-critic, we use \u03b1 = 10 \u22123 and decrease it to 10 \u22124 for the joint actor-critic training. We pretrain the actor until its score on the development set stops improving. We pretrain the critic until its TD error stabilizes 3 . We used M = 1 sample for both actor-critic and REIN-FORCE. For exact hyperparameter settings we refer the reader to Appendix A.\nWe start REINFORCE training from a pretrained actor, but we do not use the curriculum learning employed in MIXER. The critic is trained in the same way for both REINFORCE and actorcritic, including the pretraining stage. We report results obtained with the reward shaping described in Section 3, as we found that it slightly improves REINFORCE performance. Table 1 presents our results on the spelling correction task. We observe an improvement in CER over log-likelihood training for all four settings considered. Without simultaneous loglikelihood training, actor-critic training results in a better CER than REINFORCE-critic in three Table 1: Character error rate of different methods on the spelling correction task. In the table L is the length of input strings, \u03b7 is the probability of replacing a character with a random one. LL stands for the log-likelihood training, AC and RF-C and for the actor-critic and the REINFORCE-critic respectively, AC+LL and RF-C+LL for the combinations of AC and RF-C with LL.\n\nSetup\nCharacter  Adding the log-likelihood gradient with a cofficient \u03bb LL = 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic.\n\nMACHINE TRANSLATION\nFor our first translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work. The training data comprises about 153,000 German-English sentence pairs. In addition we considered a larger WMT14 English-French dataset Cho et al. (2014) with more than 12 million examples. For further information about the data we refer the reader to Appendix B.\nThe return is defined as a smoothed and rescaled version of the BLEU score. Specifically, we start all n-gram counts from 1 instead of 0, and multiply the resulting score by the length of the ground-truth translation. Smoothing is a common practice when sentence-level BLEU score is considered, and it has been used to apply REINFORCE in similar settings (Ranzato et al., 2015).\nIWSLT 2014 with a convolutional encoder In our first experiment we use a convolutional encoder in the actor to make our results more comparable with Ranzato et al. (2015). For the same reason, we use 256 hidden units in the networks. For the critic, we replaced the convolutional network with a bidirectional GRU network. For training this model we mostly used the same hyperparameter values as in the spelling correction experiments, with a few differences highlighted in Appendix A. For decoding we used greedy search and beam search with a beam size of 10. We found that penalizing candidate sentences that are too short was required to obtain the best results. Similarly to (Hannun et al., 2014), we subtracted \u03c1T from the negative log-likelihood of each candidate sentence, where T is the candidate's length, and \u03c1 is a hyperparameter tuned on the validation set.\nThe results are summarized in Table 2. We report a significant improvement of 2.3 BLEU points over the log-likelihood baseline when greedy search is used for decoding. Surprisingly, the best performing method is REINFORCE with critic, with an additional 0.6 BLEU point advantage over the actor-critic. When beam-search is used, the ranking of the compared approaches is the same, but the margin between the proposed methods and log-likelihood training becomes smaller. The final performances of the actor-critic and the REINFORCE-critic with greedy search are also 0.7 and 1.3 BLEU points respectively better than what Ranzato et al. (2015) report for their MIXER approach. This comparison should be treated with caution, because our log-likelihood baseline is 1.6 BLEU Table 3: Our IWSLT 2014 machine translation results with a bidirectional recurrent encoder compared to the previous work. Please see Table 1 for an explanation of abbreviations. The asterisk identifies results from (Wiseman & Rush, 2016 Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from  and (Shen et al., 2015) respectively.\nDecoding method Model LL' LL* MRT * LL AC+LL RF-C+LL greedy search n/a n/a n/a 29. points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015).\nTo better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect -that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of fitting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overfitting, which is a healthy behaviour for such a small dataset.\nIn addition, we performed an ablation study. We found that using a target network was crucial; while the joint actor-critic training was still progressing with \u03b3 \u03b8 = 0.1, with \u03b3 \u03b8 = 1.0 it did not work at all. Similarly important was the value penalty described in Equation (10). We found that good values of the \u03bb coefficient were in the range [10 \u22123 , 10 \u22126 ]. Other techniques, such as reward shaping and a delayed actor, brought moderate performance gains. We refer the reader to Appendix A for more details.\nIWSLT 2014 with a bidirectional GRU encoder In order to compare our results with those reported by Wiseman & Rush (2016) we repeated our IWSLT 2014 investigation with a different encoder, a bidirectional RNN with 256 GRU units. In this round of experiments we also tried to used combined training objectives in the same way as in our spelling correction experiments. The results are summarized in Table 3. One can see that the actor-critic training, especially its AC+LL version, yields significant improvements (1.7 with greedy search and 1.0 with beam search) upon the pure log-likelihood training, which are comparable to those brought by Beam Search Optimization (BSO), even though our log-likelihood baseline is much stronger. In this round of experiments actor-critic and REINFORCE-critic performed on par.\nWMT 14 Finally we report our results on a very popular large WMT14 English-French dataset (Cho et al., 2014) in Table 4. Our model closely follows the achitecture from , however we achieved a higher baseline performance by annealing the learning rate \u03b1 and penalizing output sequences that were too short during beam search. The actor-critic training brings a significant 1.5 BLEU improvement with greedy search and a noticeable 0.4 BLEU improvement with beam search. In previous work Shen et al. (2015) report a higher improvement of 1.4 BLEU with beam search, however they use 100 samples for each training example, whereas we use just one. We note that in this experiment, which is perhaps the most realistic settings, the actor-critic enjoys a significant advantage over the REINFORCE-critic.\n\nDISCUSSION\nWe proposed an actor-critic approach to sequence prediction. Our method takes the task objective into account during training and uses the ground-truth output to aid the critic in its prediction of intermediate targets for the actor. We showed that our method leads to significant improvements over maximum likelihood training on both a synthetic task and a machine translation benchmark. Compared to REINFORCE training on machine translation, actor-critic fits the training data much faster, although in some of our experiments we were able to significantly reduce the gap in the training speed and achieve a better test error using our critic network as the baseline for REINFORCE.\nOne interesting observation we made from the machine translation results is that the training methods that use generated predictions have a strong regularization effect. Our understanding is that conditioning on the sampled outputs effectively increases the diversity of training data. This phenomenon makes it harder to judge whether the actor-critic training meets our expectations, because a noisier gradient estimate yielded a better test set performance. We argue that the spelling correction results obtained on a virtually infinite dataset in conjuction with better machine translation performance on the large WMT 14 dataset provide convincing evidence that the actor-training can be effective. In future work we will consider larger machine translation datasets.\nWe ran into several optimization issues. The critic would sometimes assign very high values to actions with a very low probability according to the actor. We were able to resolve this by penalizing the critic's variance. Additionally, the actor would sometimes have trouble to adapt to the demands of the critic. We noticed that the action distribution tends to saturate and become deterministic, causing the gradient to vanish. We found that combining an RL training objective with log-likelihood can help, but in general we think this issue deserves further investigation. For example, one can look for suitable training criteria that have a well-behaved gradient even when the policy has little or no stochasticity.\nIn a concurrent work Wu et al. (2016) show that a version of REINFORCE with the baseline computed using multiple samples can improve performance of a very strong machine translation system. This result, and our REINFORCE-critic experiments, suggest that often the variance of REINFORCE can be reduced enough to make its application practical. That said, we would like to emphasize that this paper attacks the problem of gradient estimation from a very different angle as it aims for low-variance but potentially high-bias estimates. The idea of using the ground-truth output that we proposed is an absolutely necessary first step in this direction. Future work could focus on further reducing the bias of the actor-critic estimate, for example, by using a multi-sample training criterion for the critic.\n\nA HYPERPARAMETERS\nFor machine translation experiments the variance penalty coefficient \u03bb was set to 10 \u22124 , and the delay coefficients \u03b3 \u03b8 and \u03b3 \u03c6 were both set to 10 \u22124 . For REINFORCE with the critic we did not use a delayed actor, i.e. \u03b3 \u03b8 was set to 1. For the spelling correction task we used the same \u03b3 \u03b8 and \u03b3 \u03c6 but a different \u03bb = 10 \u22123 . When we used a combined training criterion, the weight of the log-likelihood gradient \u03bb LL was always 0.1. All initial weights were sampled from a centered uniform distribution with width 0.1.\nIn some of our experiments we provided the actor states as additional inputs to the critic. Specifically, we did so in our spelling correction experiments and in our WMT 14 machine translation study. All the other results were obtained without this technique.\nFor decoding with beam search we substracted the length of a candidate times \u03c1 from the loglikelihood cost. The exact value of \u03c1 was selected on the validation set and was equal to 0.8 for models trained by log-likelihood and REINFORCE and to 1.0 for models trained by actor-critic and REINFORCE-critic.\nFor some of the hyperparameters we performed an ablation study. The results are reported in Table 5.\n\nB DATA\nFor the IWSLT 2014 data the sizes of validation and tests set were 6,969 and 6,750, respectively. We limited the number of words in the English and German vocabularies to the 22,822 and 32,009 most frequent words, respectively, and replaced all other words with a special token. The maximum sentence length in our dataset was 50. For WMT14 we used vocabularies of 30,000 words for both English and French, and the maximum sentence length was also 50.\n\nC GENERATED Q-VALUES\nIn Table C we provide an example of value predictions that the critic outputs for candidate next words.\nOne can see that the critic has indeed learnt to assign larger values for the appropriate next words. While the critic does not always produce sensible estimates and can often predict a high return for irrelevant rare words, this is greatly reduced using the variance penalty term from Equation (10). The numbers in parentheses are the value predictionsQ. The German original is \"\u00fcber eine davon will ich hier erz\u00e4hlen .\" The reference translation is \"and there's one I want to talk about\". Word Words with largestQ one and(6.623) there(6.200) but (\n</cited_paper>\n\n<citance>\nPrior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\n</citance>"
    },
    "response_format": {
        "type": "json_object"
    },
    "temperature": 0.0
}
