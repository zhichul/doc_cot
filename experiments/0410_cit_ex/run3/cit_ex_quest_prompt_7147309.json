{
    "messages": {
        "system": "You are a paper reviewer trying to identify connections between the paper you're reviewing with previous ideas and approaches in the field.",
        "user": "Below is the CURRENT PAPER that you're reviewing and a paper that it CITES. \nBelow is also the CITANCE, which is the context containing a citation to the CITED PAPER.\n\nPlease respond with a list of question that can be asked of papers, and which have identical answers for the CURRENT PAPER and the CITED PAPER. \n\nPlease respond in the following json format:\n{\n  \"thought_process\": str # first discuss how the two papers are related, this should help you come up with questions\n  \"questions\": [\n    {\n      \"question\": str # a clear, unambiguous question that can be asked of papers in general\n      \"score\": str # a score of quality of the question\n      \"rationale\": str # a justification for why this question yields the same answer on both papers, and the score it receives\n      \"answer\": str # the answer to this question for both papers\n    },\n  ]\n}\n\n\nList all important questions, and give a score between 0 to 2, where 0 means the two papers would actually not have the same answer, 1 means while the two papers would have the same answer, it's too generic and many other papers would also have the same answer, and 2 means it touches the core of the relationship between the two papers, and it's unlikely that many other papers would have the same answer.\n\n<current_paper>\nA Distributional Approach to Controlled Text Generation\n\nWe propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both\"pointwise\"and\"distributional\"constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https://github.com/naver/gdc)\n\n\nINTRODUCTION\nNeural language models, such as GPT-2/3 Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality. In this paper, we are concerned with the problem of controlling a generic pretrained LM in order to satisfy certain desiderata. For instance, we may want to avoid toxic content; prevent certain demographic biases; or steer generations towards a certain topic or style. Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\nHowever, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to \"degeneration\", producing poor examples that improve the average reward but forgo coherence and fluency. This degeneration is often diagnosed as an effect of deviating too much from the original pretrained LM during optimization. Consequently, prior work has regarded proximity to the pretrained model as a prescription for sample quality. This view is most prominent in open-domain generation where no gold references are available for fine-tuning, making the pretrained LM itself the yardstick for fluency. Jaques et al. (2017);Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations. A KL penalty was also used by Dathathri et al. (2020), this time in a plug-and-play rather than a fine-tuning context. However, the authors show that balancing policy deviations from the original LM while also satisfying the control conditions is delicate. To combat degeneration they had to combine the KL penalty with post-norm fusion, reranking, and early-stopping procedures.\n1. We introduce a Distributional View for controlled text generation formalized as a constraint satisfaction problem combined with a divergence minimization objective, providing a single framework both for \"distributional\" constraints (collective statistical requirements) and for \"pointwise\" constraints (hard requirements on each individual) ( \u00a72.1). To our knowledge, this is the first framework with such generality for controlled text generation.\n2. We show how these constraints lead to an optimal EBM for the target model ( \u00a72.2), propose the KL-Adaptive DPG algorithm for approximating the optimal EBM distribution by Figure 1: From MaxEnt to EBM through Information Geometry. The Generalized MaxEnt specification (left panel) is looking for a distribution p that lies on the moment constraints manifold C and that minimizes the forward KL DKL(p, a). The solution is provided by Information Geometry: (1) build the exponential family E determined by a and \u03c6, (2) p lies at the intersection between C and E, (3) for any distribution c satisfying the constraints, the \"Pythagorean identity\" holds: DKL(c||a) = DKL(c||p) + DKL(p||a); in particular p is unique.\nan autoregressive policy ( \u00a72.3), and show the effectiveness of this adaptive technique for obtaining faster convergence ( \u00a7B.2).\n3. We conduct experiments in a number of pointwise and distributional conditions, assessing results in terms of divergence from GPT-2, fluency and diversity, with better performance than strong baselines. The distributional experiments show the potential of our approach as a remedy to the current and important problem of bias in pretrained language models, providing a novel direction for addressing it ( \u00a73).\n\nFORMALIZATION\nWe denote by X the set of all sequences x of bounded length L max , by a the initial pretrained model and by p the desired target model. The probabilities of x according to each model are a(x) and p(x). Our approach consists in expressing our desiderata through constraints on the desired values\u03bc i of the expectations (aka moments) \u00b5 i . = E x\u223cp \u03c6 i (x) of certain predefined real-valued feature functions \u03c6 i (x), for i \u2208 {1, . . . , k}.\nTo illustrate, the previous example can be expressed by using two binary features, \u03c6 1 (x) = 1 iff x is classified as speaking about sports, \u03c6 2 (x) = 1 iff x mentions a female character. Then our \"moment constraints\" take the following form: \u00b5 1 = E x\u223cp \u03c6 1 (x) = 1.0, \u00b5 2 = E x\u223cp \u03c6 2 (x) = 0.5. The first (pointwise) constraint implies that each individual x has to speak about sports (otherwise \u00b5 1 could not reach its maximum value 1.0), the second (distributional) constraint that 50% of the x's have to mention a female character. 4 Let C be the set of all distributions c over X that satisfy the moment constraints. We then propose to specify p as a distribution respecting the constraints, but also minimizing KL divergence from a: Equation (1) is a generalization of the Maximum Entropy Principle of Jaynes (1957), which corresponds to the limit case where a is the uniform u distribution over X, noting that minimizing D KL (c, u) is equivalent to maximizing the entropy of c under the constraints -in other words, trying to find the least \"specific\" distribution satisfying the constraints.\n\nCONSTRAINTS, INFORMATION GEOMETRY, EXPONENTIAL FAMILIES\nTo recap our formal approach, we have a finite set X, a distribution a over X s.t. a(x) > 0, \u2200x \u2208 X, and real functions \u03c6 1 , ..., \u03c6 k over X. We specify moment constraints \u00b5 i =\u03bc i on distributions c over X, where \u00b5 i . = E x\u223cc \u03c6 i (x) and the\u03bc i 's are given targets; the set of distributions satisfying these constraints is denoted by C. Our Problem is to find a p such that p = arg min c\u2208C D KL (c, a).\nWe follow Csisz\u00e1r & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018;Amari & Nagaoka, 2000). Under the assumption that C = \u2205, they prove the following result (also see \u00a7A.1): Published as a conference paper at ICLR 2021 Theorem 1 (A) There exists a unique solution p to the problem above, obtained as p(x) \u221d P (x) where P is in exponential family form: (x) . (2) In other words p(x) = 1/Z P (x), with Z = x\u2208X P (x); P is an unnormalized distribution, i.e. an EBM. Here X C = {x \u2208 X| \u2203c \u2208 C s.t. c(x) > 0} is the \"support set\" associated with C. The \u03bb i 's are real numbers called the natural parameters associated with the moments \u00b5 i .\n(B) p can be approximated to arbitrary precision by distributions p of the form: for appropriate real values of the \u03bb ,i .\nThe advantage of this version of the connection between Generalized Maximum Entropy and Exponential Families is its generality, which distinguishes it from other presentations, and which makes it ideal for unified application to pointwise, distributional or hybrid constraints.\nIn the special case of only pointwise constraints, of the form E x\u223cc \u03c6 i (x) = 1.0, i \u2208 [1, k], with \u03c6 i (x) \u2208 {0, 1}, let's define the predicate b(x) to be 1 iff x satisfies all the constraints. Then, using the (A) form of the result, it is an easy exercise (see \u00a7A.2) to prove that X C = {x \u2208 X| b(x) = 1} and that one has p(x) \u221d a(x)b(x). In this case P (x) = a(x)b(x) is a very simple EBM that does not involve an exponential part; this is the EBM form that we use for experiments involving only pointwise constraints.\nIn the general case where some constraints are distributional, the determination of X C is not as direct, and we prefer to use the approximation provided by (B), which permits a generic implementation. With only distributional constraints, an exact solution is typically obtained with finite \u03bb's. With hybrid constraints, some of the \u03bb's may tend to infinite (positive or negative) values but thresholding them suffices to get a good approximation.\nNote that the estimate\u03bc(\u03bb) is obtained not as a single number, but as a parametric function of the variable \u03bb. We want to find \u03bb such that\u03bc(\u03bb) =\u03bc, a question that we handle on line 4 by performing an SGD optimization over the objective min ||\u03bc \u2212\u03bc(\u03bb)|| 2 2 . 6 At the end of this process, we obtain an estimated value for the parameter vector \u03bb, and a representation P (x) = a(x) exp \u03bb, \u03c6(x) . While a(x) is a normalized distribution by construction, the introduction of the second factor loses this normalization property, making P (x) an EBM. 7 8\n\nFROM EBM TO AUTOREGRESSIVE POLICY\nAlgorithm 2 KL-Adaptive DPG Input: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: for each iteration do 3: for each episode do 4: sample x from q(\u00b7) 5: \u03b8 \u2190 \u03b8+\u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 6: if DKL(p||\u03c0 \u03b8 ) < DKL(p||q) then 7: q \u2190 \u03c0 \u03b8 Output: \u03c0 \u03b8 The EBM representation just obtained for P defines the optimal p = Z \u22121 P unambiguously, a crucial intermediate step in the solution of our problem. From it we can immediately compute ratios of the form p(x)/p(x ) for two sequences x, x , but without knowing Z, we cannot compute p(x) and, even with such a knowledge, we cannot produce samples from p.\nThis problem is typical of EBMs at large: they provide a rich and flexible mechanism for specifying models, but they leave a gap between representation and exploitation. A range of techniques, from sophisticated MCMC approaches (especially for continuous models in vision) to contrastive learning techniques, have been developed for bridging this gap.\nOne technique that is suitable for our objective here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (\"Distributional Policy Gradient\") algorithm (Parshakova et al., 2019b).\nThe objective of DPG is to obtain an autoregressive policy \u03c0 \u03b8 that approximates p, where approximation is formalized in terms of making the cross-entropy CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) log \u03c0 \u03b8 (x) as small as possible. 9 DPG exploits the fact that, for any \"proposal\" distribution q whose support contains the support of p, we have \u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212\u2207 \u03b8 E x\u223cp log \u03c0 \u03b8 (x) = \u2212E x\u223cp \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2212E x\u223cq p(x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) where the last equality is an instance of importance sampling. Our \"KL-adaptive\" version of DPG is shown in (Algorithm 2). We start from an input EBM P , along with an initial policy q which is a proxy to p; in our case we take q = a. During an iteration (think minibatch or set of minibatches), we sample a number of sequences from q, do an SGD update of \u03b8 (line 5), where P is used instead of p (noting that they only differ by a multiplicative constant), and where \u03b1 (\u03b8) is a learning rate. The efficiency of the algorithm is related to how close the proposal q is to the target p, 10 The algorithm is adaptive in the sense that it modifies q periodically to take advantage of the evolving approximations \u03c0 \u03b8 . On line 6, we we test whether the current \u03c0 \u03b8 is closer than q to p in terms of KL-divergence, and if so we update q to \u03c0 \u03b8 on line 7. 11 \u00a7B.2 provides an ablation study showing the effectiveness of this adaptive step for obtaining faster convergence.\n\nEXPERIMENTS, RESULTS, AND EVALUATION\nIn this section we describe our evaluation methodology and perform experiments on pointwise constraints ( \u00a73.2) and on distributional and hybrid constraints ( \u00a73.3). The Appendix contains a detailed view of evaluation ( \u00a7H), comparison with extra baselines ( \u00a7D.2), and an ablation study ( \u00a7B.2).\n\nEVALUATION METRICS\nThe main metrics we report are: (1) E x\u223c\u03c0 \u03b8 \u03c6 i (x), assessing the ability of \u03c0 \u03b8 to reach the expectation goal on the i-th constraint, (2) D KL (p||\u03c0 \u03b8 ), the forward KL divergence from the optimal distribution (which should be as close to 0 as possible), (3) D KL (\u03c0 \u03b8 ||a), the reverse KL divergence from the original GPT-2; for details on the estimation of these metrics see \u00a7B.1. Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores (Li et al., 2016a) to measure repetitions within a single generated sequence. However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020). So additionally, we report Self-BLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig. 4 and \u00a7H.4). Note that KL divergence from the original GPT-2 also implicitly captures sample diversity: a distribution that focuses all its probability mass on a few sequences typically displays high divergence from GPT-2. Implementation details and hyper-parameters are available in the Appendix ( \u00a7 F).\n\nPOINTWISE CONSTRAINTS EXPERIMENTS\nPointwise constraints are of the form E p \u03c6 i (x) = 1, with \u03c6 i a binary feature. Contrarily to distributional constraints, they can be directly associated with a \"reward\", namely \u03c6 i itself. RL-inspired baselines can then be introduced naturally, and this is what we do here.\n(3) ZIEGLER (Ziegler et al., 2019): an approach relying on the RL Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) and which tries to maximize the objective E \u03c0 \u03b8 \u03c6(x) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), which interpolates the reward \u03c6(x) with a KL-divergence penalty from the pretrained model, but where the goal is not explicitly to satisfy a constraint; for a geometric illustration of the differences with 11 In the original DPG, the superiority test is done on the basis of the log-likelihood on a validation set. Here we are in the more demanding situation where no validation set is available. To directly estimate the KL divergence from p (line 6), we exploit the identity DKL(p \u03c0) = \u2212 log Z + 1/Z E x\u223cq(x) \u03c0(x) . See \u00a7B.1 for derivations and a comparison with using Total Variation Distance (TVD) for assessing divergence.  Results: Figure 2 shows the evolution of the metrics over training steps, aggregated across the 9 + 4 + 4 = 17 experiments. We observe the following: the baseline RE-INFORCE , which does not have any explicit link in its objective to the pretrained GPT-2, converges very early in the training, reaching a maximum value of E \u03c0 \u03b8 \u03c6(x) at the expense of a very large deviation from the original GPT-2. High values of D KL (\u03c0 \u03b8 |a), are translated into low Dist-1 and very high Self-BLEU-5 indicating degeneration and lack of diversity. REINFORCE P(x) maximizes the energy model P by peaking on a few sequences only; this can yield high values of E \u03c0 \u03b8 P (x), at the expense of low sample diversity as demonstrated in the highest values of SELF-BLEU-5 scores among baselines. 12 In the case of ZIEGLER we can see a positive effect of the interpolation factor \u03b2 between the reward and the KL penalty in the objective function. In the aggregated experiments reported here, the reward is slightly better than with GDC, but with inferior diversity scores (see also Fig. 4, showing that GDC produces richer vocabulary), and the stability is much worse (a detailed view of each experiment is provided in \u00a7H, showing more clearly the instability of this baseline). A complementary evaluation is provided by Figure 3, focusing on the ability of \u03c0 \u03b8 to converge to the optimal distribution p. We see that GDC is superior to all baselines in terms of D KL (p \u03c0 \u03b8 ) and also much more stable.\nIn summary, in these experiments, we see that with GDC the constraint expectation E \u03c0 \u03b8 \u03c6(x) smoothly increases while \u03c0 \u03b8 maintains the lowest divergence from GPT-2, becomes closest to the optimal p, and has the best diversity scores overall. On the other hand, we also note that at the point where we stop training (30K steps), the average over experiments of E \u03c0 \u03b8 \u03c6(x), while still increasing, does not reach 100%, an issue that we discuss at the end of the paper ( \u00a74).\n\nDISTRIBUTIONAL AND HYBRID CONSTRAINTS EXPERIMENTS\nAs formalized in \u00a72, GDC permits to define pointwise and distributional constraints as well as any mix between them. This unique feature makes it very suitable to remedy biases that the text generation model may have, a problem identified in several previous works (Sheng et al., 2019b). We employ GDC to balance gender and profession distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies (Lebret et al., 2016) (henceforth GPT-2 bio ) ( \u00a7G gives additional details). The bias in GPT-2 bio is significant: we calculated that this model generates only around 7% female biographies. It also displays a large imbalance between professions related to \"Science\" (1.5%), \"Art\" (10.0%), \"Business\" (10.9%) and \"Sports\" (19.5%).\n\nExperiment 1: Single Distributional Constraint\nWe use the distributional constraint E x\u223cp \u03c6 f emale (x) = 0.5; GDC is able to reduce the bias of GPT-2 bio to obtain 35.6% female biographies rather than only 7.4% (see Fig. 2 for this experiment and the next ones). Experiment 2: Multiple Distributional Constraints We then test our framework with several distributional constraints of different values and control directions. We specify four distributional constraints all at once with the goal of increasing the expectations of \"science\" and \"art\" to 40% and decreasing those of \"sports\" and \"business\" to 10%. GDC is able to increase the expectations of the first two professions respectively from 1.5% to 20.3% and from 10 to 31.6% and to decrease those of \"business\" and \"sports\" respectively from 10.9% to 10.2% and from 19.5% to 11.9%, reaching expectations close to the desired ones for all features using a single training method. Experiments 3,4,5,6: Hybrid Constraints Here we want to de-bias the model as in the previous case but we single out biographies of scientists, artists, etc. Formally, our requirements become E x\u223cp \u03c6 prof ession (x) = 1.0, a pointwise constraint, and E x\u223cp \u03c6 f emale (x) = 0.5, a distributional constraint. In those 4 hybrid experiments we can clearly see that GDC can address both pointwise and distributional constraints increasing each simultaneously with just the right amount to reach the desired expectations. Appendix \u00a7G further elaborates Fig. 2 (convergence curves).\n\nDISCUSSION\nOur approach to controlled text generation is distinguished by its breadth -the first one to handle distributional along with pointwise constraints, with applications to the important problem of Bias in pretrained LMs -and by the transparency of the supporting formalism. It decouples the training objective along two different dimensions. The first consists in solving the initial constraints specification, and leads through a direct algorithm to an optimal solution in EBM format. The second, where the real computational difficulty lies, consists in approximating this EBM with an autoregressive policy for use at inference time. Sampling from an EBM is an important, hard, and well-identified challenge in the literature. Our approach there consists in proposing a KL-adaptive version of the DPG algorithm, which exploits ascertained improvements of the trained policy to speed up convergence. This is an effective method for rare events, as we show in an ablation study ( \u00a7B.2  Our method does not suffer from degeneration, but our end policies still generate a number of samples not satisfying the constraints. A possibility, left for future work, might consist in filling the moderate residual gap with MCMC techniques, which would be guaranteed to reach our optimal p in the limit. We do not go this route here, but conduct an experiment (see \u00a7C) to better understand the nature of the problem. In the simple case of a single-word constraint (x includes \"amazing\"), we sample directly 1M samples from GPT-2 and keep the roughly 5K samples containing amazing (a variant of rejection sampling, taking two processing days). We then do a standard supervised fine-tuning of GPT-2 with these samples, stopping training when the CE validation loss starts to increase, and observe that this model exhibits a worse constraint satisfaction rate than ours. This experiment does not mean that a much larger fine-tuning dataset, obtained in this slow, non-adaptive way, would not reach better statistics, but it raises doubts about the ability of the GPT-2 architecture to fine-tune over such a non-standard constraint as containing a given word somewhere in its output.\nOverall, we believe that the proposed decomposition into two sub-problems is a methodological advantage compared to most other works, which directly aim at training a policy with the goal of improving certain evaluation metrics, but without clearly defining what qualifies as an optimal solution. The computational challenge of fully bridging the gap between the optimal EBM and an efficient sampling engine remains, and we hope that the formalism we propose, along with initial applications and experimental validations, will motivate further research along these lines.\n\nACKNOWLEDGMENTS\nWe would like to thank the anonymous reviewers for their insightful feedback that helped enhancing the final version of this manuscript. We also thank Germ\u00e1n Our statement of Theorem 1 is actually a reformulation of two results in section 3 of Csisz\u00e1r & Shields (2004). Our property (A) is a simple notational transposition of their Remark 3.1 (p. 444). Property (C) is the Pythagorean Identity in their Theorem 3.2 (p. 442). Property (B) reformulates the last part of the same Theorem \"... and in general L \u2229 cl(E Q ) = {P * }\" in terms of a limit of a sequence of distributions.\nNote: Csisz\u00e1r & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975 , so the exponential factor is a constant, which proves that P (x) = a(x)b(x) is proportional to P (x), and therefore p(x) \u221d P (x).\n\nA.3 INCREMENTALLY ADDING NEW CONSTRAINTS\nAn interesting question 13 is whether the process explained in \u00a72 can be made incremental: if one has already computed a p and a \u03c0 \u03b8 relative to a certain number of constraints, can one add a new constraint without restarting the whole process from scratch? The answer is yes, and here we provide some formal elements to understand why.\n\nA.3.1 TRANSITIVITY PROPERTY OF GENERALIZED MAXENT\nAccording to (Csisz\u00e1r, 1996), the Generalized MaxEnt of sections \u00a72.1 and \u00a72.2 has the \"Transitivity property\". In our notation, this says that if we have k > k constraints, with C the manifold of distributions respecting only the first k constraints, C the manifold respecting all k constraints (hence C \u2282 C), then the maxent projection p of a onto C can be obtained by first projecting a onto C, obtaining p, and then projecting p onto C , obtaining p . In particular, the k lambdas associated with p can be directly reused as the first lambdas of the k lambda's associated with p . (Csisz\u00e1r, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider. The proof, illustrated in Figure 5, is very similar to one of the proofs for the transitivity of the orthogonal projection in Euclidean geometry. Proof. In the Figure, p is the information projection (Csiszar's terminology for the Generalized Maxent) of a onto C, as before. Let's define r to be the projection of p onto C . We need to prove that r is identical to the projection p of a onto C . We consider an arbitrary distribution c in C , and apply the Pythagorean Identity of Theorem 1 three times. Because p is the projection of a onto C, we have D KL (r, a) = D KL (r, p) + D KL (p, a) and also D KL (c , a) = D KL (c , p) Putting these three facts together, we find that D KL (c , a) \u2265 D KL (r, a).\nAs c is an arbitrary point of C , this proves that r is the projection of a onto C , in other words, r = p .\n\nA.3.2 TRANSITIVITY AND AUTOREGRESSIVE POLICY\nDue to the Transitivity property, when calculating the EBM representation, it is possible to start from p without re-fitting p from scratch. However the move from EBM to autoregressive policy of \u00a72.3 remains to be discussed. The question now is the following. We have already obtained a policy \u03c0 \u03b8 approximating p, and we are interested in obtaining a policy \u03c0 \u03b8 approximating p : is it advantageous to start Algorithm 1 with q = \u03c0 \u03b8 , rather than starting \"from scratch\" and taking q = a ? Intuition says \"yes, very probably\", because \u03c0 \u03b8 is by construction an approximation to p, which is closer than a to p (formally, D KL (p , p) \u2264 D KL (p , a), see Fig. 5, where p = r). Due to the approximation, we only have D KL (p , \u03c0 \u03b8 ) D KL (p , p) , so a formal proof that \u03c0 \u03b8 is superior to a as a starting point is impossible, but we expect that further experiments would confirm the improvement.\n\nB MORE ON ADAPTIVITY B.1 DETAILS ON KL-ADAPTIVITY\nIn this section we provide details on the comparison step in our KL-Adaptive version of the DPG Algorithm, introduced in section 2. We want to assess whether the current \u03c0 \u03b8 is closer than q to p, and if the test is positive, we set \u03c0 \u03b8 as the new proposal, hoping to make the proposal more effective for importance sampling.\nThere are several ways to compute similarity between distributions, two of the most popular ones being on the one hand KL-divergence and on the other hand Total Variation Distance (TVD)where TVD(p||p ) . = 1/2 x |p(x) \u2212 p (x)| -which is often used in probability and MCMC theory. 14 Calculation of these metrics relative to p is not straightforward since the distribution p \u221d P is only implicitly represented by the unnormalized EBM P , and we cannot easily obtain direct samples from p. In this section we describe a workaround.\nGiven P and a proposal distribution q that we can sample from, using importance sampling (Owen, 2013), one can calculate the partition function Z as follows: We can then compute D KL (p||\u03c0) as: Similarly, for TVD(p||\u03c0): In \u00a7B.2 we run an ablation study to compare the use of D KL on line 6 of Algorithm 2) or its replacement by TVD.\nFor both metrics, we need an estimate of Z. The precision of this estimate depends on the sample size and the quality of the proposal distribution q. We calculate a moving average estimate Z MA of Z is used inside the estimations of D KL (p \u03c0 \u03b8 ) and D KL (p q) (Algorithm 3, lines 7 and 8). Z MA is updated at each iteration of the training, and the moving average estimate is valid due to the fact that\u1e90 i , based on K samples, is an unbiased estimate of Z, and therefore so is Z MA . In this way, the estimate benefits from all the samples being produced during the course of the training; and also because the proposal distribution q evolves and gets closer to the target distribution p, the quality of the estimates of both D KL (p||\u03c0 \u03b8 ) and Z MA through importance sampling increases (equation 7). A similar approach is taken in the case of TVD (not shown).\n\nAlgorithm 3 KL-Adaptive DPG (detailed)\nInput: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: ZMA \u2190 0 Initialize Moving Average estimate of Z 3: for each iteration i do Update moving average estimate of Z Estimate on the K samples Estimate on the K samples 11: ifDKL(p||\u03c0 \u03b8 ) <DKL(p||q) then\n\nB.2 ABLATION ON ADAPTIVITY\nHere we run an ablation experiment on the adaptivity step of KL-Adaptive DPG ( \u00a72). We compare three variants of our proposed method: DPG-KLD, which uses KL divergence from the target distribution p to measure the quality of the trained policy \u03c0 \u03b8 i.e. if D KL (p \u03c0 \u03b8 ) < D KL (p q) we update the proposal distribution q \u2190 \u03c0 \u03b8 . DPG-TVD is similar but with the total variation distance instead (TVD). In non-Adaptive the initial proposal q is kept fixed during training.\nWe run 3 point-wise experiments with single word constraints of three rarity levels in the original GPT-2 distribution, namely: \"Vampire\" (1/10 4 ),\"Paris\" (1/10 3 ),\"US\" (1/10 2 ) .For each we use 3 different seeds and train for 10k gradient updates. Figure 6 shows training trends of the three ablations. We find a significant difference in convergence speed in favour of the adaptive methods. The efficiency gap between Adaptive and non-Adaptive methods becomes larger the more rare the constraints are. i.e. the proposal distribution q starting point is very far from the target distribution p, as the efficiency of the DPG algorithm is related to how close the proposal q is to the target p. When q is continuously adapted, the proposal distribution becomes closer to p and the training becomes efficient regardless of how far the initial proposal distribution is from p. We observe similar convergence rates for DPG-KLD and DPG-TVD. : Ablation experiment elaborating the effectiveness of the adaptive step in the DPG algorithm explained in section 2. We compare three adaptivity variants, based on the KL divergence (DPG-KLD), on the TVD distance (DPG-TVD) and with no adaptation. We find similar convergence rates for both KLD and TVD adaptive DPG compared to a much slower convergence without adaptation.\n\nC CAN STANDARD SUPERVISION FULLY SATISFY THE CONSTRAINTS?\nIn this section, we try to better understand potential difficulties of autoregressive models to fully satisfy constraints such as the ones illustrated in our pointwise experiments.\nTo this end, we consider whether a standard fully supervised fine-tuning of GPT-2 can achieve that objective while keeping a minimal distance from the initial model. To answer the question, we carry out an experiment where we fine-tune GPT-2 on a collection of samples satisfying the desired constraint. Our goal here is to investigate whether GPT-2 can fully satisfy the constraint without overfitting the fine-tuning data, since overfitting (memorizing) the training data basically means high KL-divergence from the initial model.\nFor this experiment, we choose a single-word constraint with the word \"amazing\". We start by sampling 1M sequences from GPT-2 small -a process that took us roughly 48 hours -and keeping only the ones containing \"amazing\" (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)). We end up with a total of 4600 samples out of which we use 500 for validation and the rest for fine-tuning. This result suggests a relationship between training a policy reaching 100% and overfitting the training data. This hints at the difficulty of strictly imposing certain types of constraints on pre-trained language models without moving far away from the initial model. 15  The figure below illustrates the difference between GDC, the RL-based REINFORCE and ZIEGLER baselines for a pointwise constraint. The main points to note are: (1) REINFORCE is trying to find a distribution p R maximizing r(x) (meaning that p R lies on the C manifold), but this p R is free to land anywhere on this manifold, and (2) ZIEGLER is trying to find a distribution p Z that interpolates (with a weight \u03b2) between a high average r(x) and the KL divergence from a; unless \u03b2 = 0, in which case we are back to REINFORCE, p Z does not satisfy the constraint and falls outside of the manifold. c(x) > 0 \u2192 r(x) = 1, or, equivalently s.t. Ex\u223ccr(x) = 1. The curved lines represent increasing levels of the KL divergence DKL(q, a). According to Reinforce, any distribution pR s.t. Ex\u223cp R r(x) = 1, that is, any distribution on C, is optimal. According to Ziegler, to each temperature \u03b2 > 0 is associated an optimal distribution pZ = arg min q \u03b2DKL(q, a) \u2212 Ex\u223cqr(x), which does not directly lie on C -this is because, as indicated in (Ziegler et al., 2019), this distribution is of the form pZ (x) \u221d a(x)e r(x)/\u03b2 , giving positive probability to all x's in the support of a, including to points not lying on C. Our own optimal p does lie on C by definition, while minimizing the KL divergence from a.\n\nD.2 COMPARISON AGAINST FURTHER BASELINES\nHere we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control. PPLM works by updating the hidden states of GPT-2 for a given prefix in order to derive the generation towards the desired attributes. Unlike GDC, PPLM needs a prefix to perform its hidden-state updates. Thus, our approach is more general in the sense that any prefix can be used on the trained model at test time, rather than requiring prefix-specifc fine-tuning. CTRL is a large-scale language model (1.63 billion parameters and 14x larger than GPT-2 small) based on control codes for steering text style and content. For the purpose of generating positive/negative sentiments using CTRL, we use its positive/negative reviews control codes as done in (Dathathri et al., 2020). The control codes used are \"Reviews Rating: 5.0\" and \"Reviews Rating: 1.0\" for positive and negative sentiment control, respectively. We use five different prefixes (or prompts) and generate 100 continuations given each prefix obtaining a total of 500 samples. It is worth noting that GDC is trained in the same way as described in the main text, i.e. without any knowledge of prefixes, and that we only use prefixes at test time with the saved checkpoint. The five prefixes used come from (Dathathri et al., 2020): \"The chicken \", \"The potato \", \"The lake \", \"The pizza \", and \"The horse \".\nWe use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019). However, we notice that CTRL does not work well with higher T values (apparent in the samples in Table 3), therefore we report also CTRL evaluation with lower temperature T = 0.5 and a repetition penalty \u03bb rep = 1.2 as reported in their paper.\nAs metrics, we use sentiment class expectation E\u03c6(x), the perplexity according to an external GPT-2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section \u00a73.1. We average all these metrics across the 500 continuations generated. Table 3 shows the results for positive and negative sentiment control experiments. As shown, GDC is able to achieve better positive/negative sentiment with lower perplexity than both PPLM and CTRL. As for diversity, GDC achieves comparable diversity to the other two approaches and even outperforms PPLM on the Distn metrics in the positive sentiment task. Table 4 shows sample continuations from all three approaches. Clearly, PPLM and CTRL exhibit some form of degeneration and repetition in many of the continuations (highlighted in light red), which is reflected in their very high perplexity score compared to GDC, which produces much more natural text with minimum repetitions without requiring a repetition penalty as CTRL.\nIt is also worth noting here that CTRL (and other control code methods) is very much limited in terms of its applications. For instance, to generate positive/negative sentiment text as we do in this experiment, we are required to use the ''Reviews Rating...'' control code, using control codes outside of those CTRL was fine-tuned on leads to very bad generations. This, in turn, restricts the generated text to positive/negative reviews although we may desire different types of positive/negative text (e.g. news reports). We can observe this effect 16 in some of the samples in Table 4 such as \"The chicken we just ordered from Amazon.com...\" and \"The pizza works no matter what settings you use it on.   (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) on positive and negative sentiment control. We generate 100 samples for each prefix obtaining a total of 500 samples. All metrics shown are averaged across the 500 samples obtained. CTRL refers to the shared setting across all approaches with temperature T = 1.0 and repetition penalty \u03bbrep = 1.0 and CTRL* refers to having T = 0.5 and \u03bbrep = 1.2. Here, we see a clear advantage of GDC in terms of constraint satisfaction and perplexity and a comparable performance in terms of diversity against PPLM and CTRL.\n\nPositive\nThe chicken is so tasty! This recipe has been in my freezer for about 2 months now. I have always had good quality chicken breasts, so I had a huge amount of chicken and The potato is my favourite part of the recipe, and it is also my biggest problem. It is a good addition to anything you try to add to your meal. I love potato soup The lake has been the most active place in the country for visitors since it opened last summer, and it's not hard to see why.\nThe lake has been a place to stay for years, The pizza place is great for a good time! They have all kinds of amazing sandwiches. My favorite sandwich is the \"Chicken Pizza\" which is the best I've ever had on my way to I don't think the pizza is better than any other pizza. It's not a The horse is very well balanced with the horse's head and ears. It is a great horse to have for the family. The horse is quite large and the tail is long enough to accommodate the\",\n\nNegative\nThe chicken is so bad it's literally going to die. It's like the dog is dying from a bad diet. I'm not even sure I want it in my kitchen\" The potato was really bad, but it's the worst potato that I've ever tasted. It was really hard for me to put my finger on. I was very disappointed with the flavor of the The lake was just too cold for the water and the wind to blow. I couldn't get out of it. I couldn't see anything. The wind blew through my windshield and my car windshield\" The pizza is so bad that I've had to buy it again, even if I was just going to order one at a time. I'm not going to be buying one for the next week The horse in the back row is not going to win the race. It is going to go up in flames, and the race will end in a heap of dust. The winner will be thrown\n\nPositive\nThe chicken and the egg story: the extraordinary rise of the powerful man as he transforms the lives of ordinary people in a world in crisis The story of the A man dressed in a white suit The potato. It is the world's most awesome people, and and and the that the the a the a the a the a the a the a the , \"and the the The lake is a great spot to enjoy the spectacular views of the Great Lakes. This is also a great place to take the children's swim. The lake is also a great place to hike in the beautiful mountains The pizza is a delight! I have never had this before. I am a fan of Italian, and I have not had it before in the States. I will be back! It was a great experience The horse is a powerful, beautiful, and extremely intelligent animal., (.C,:,.,.,../.../..',,' (., \".\n\nNegative\nThe chickenpox epidemic of 1918-1920 in Britain was an acute and deadly disease that killed about 100,000 people worldwide, most of them infants. The 1918-1919 epidemic was caused by the The potato is one of those things we all dream of. I think the most common thing that people come up with when I say I have the perfect one is the idea of a \"salt water\" version The lake is one one one. <endoftext>The United Nations (UN) and the European Union (EU) are among a number of the world's most in the state and,, on the House vote for The pizza crust is anvil, which is what the British have for a long time. The British Empire, the French, the the the the the a in the that is a a it is called and it The horse is in the saddle. That's how he's been for the last four years. The Tampa Bay Lightning won a series of three games in a row to begin the new year and into January we were\n\nPositive\nThe lake I am looking forward to seeing in September! The sea scene alone would have me watching again! Rating: 5.0 One of the best comedies I've seen. We will definitely watch it again. Smart and funny The horse for this ones lines is:&#34;The road to Hell is paved with good intentions. All roads to Hell end in Hell themselves.&#34; Rating: 5.0 I live in a small The potato were \"seeded\" during a European settlement. What the characters have gone through is inevitable, but extremely rare. (And the potato has the honor of being the world's oldest potato. For that honor, we have a nickname: \"@@ The chicken we just ordered from Amazon.com has not yet arrived and I am EXTREMELY EXCITED! The seller has the finest poultry in the market....plus, it is DELICIOUS!Thank you so The pizza has been around for decades. Now that time has been added to it, all of us can appreciate it better, and enjoy it the way we have always enjoyed.PERFECT Pie:(The second listen) And it\n\nNegative\nThe pizza works no matter what settings you use it on. The icecream maker always leaks out around the spout and onto the base (gross) -finally stopped working. I only wish I had spent more for a The horse can not be found. Characters whose names show up in the battle screen:EXE: SRMX&OY; SQX the knight \u00bfQWOKB SKOZY the warrior!A useful upgrade for a The lake has been made, but it's far from Earth 5. The ship has disappeared but they continue to radio.Ignoring the plot, which the Star Trek series never bothered with, Spock says that \"we should have followed up. There is The chicken died on me after 8 months. I don't think the unit is compatible with young chickens. Not recommended. Rating: 1.0 the plates didn't last long enough for me.I bought two of these plates and they The potato does not start from eggplants, it starts from the start of generation! How stupid is that! :( I bought this and many others to try with my toddler for his preschool class. I want him to get Published as a conference paper at ICLR 2021\n\nE RELATED WORK EXTENDED\nOptimizing global rewards for Text Generation There is a large reinforcement learning inspired literature about steering an autoregressive sequential model towards optimizing some global reward over the generated text. This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017). With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time. Some others use heuristic rewards as in (Li et al., 2016b;Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues. Other non-RL techniques for approximating the global sequence constraints \u03c6(x) by a biased estimator \u03c6(x t |x :t\u22121 ). These techniques usually referred to as weighted decoding Holtzman et al. (2018) KL Divergence penalty Another approach relied on penalizing too large deviations of the trained policy relative to the original policy. Jaques et al. (2017; propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model. This penalty acts as a regularizer to the optimization process that prevents the trained policy from deviating too much from the original policy. Ziegler et al. (2019) follow a similar approach for fine tuning a language model based on human preferences, in this case a proximal policy algorithm (Schulman et al., 2017) is used to maximize the combined reward. PPLM (Dathathri et al., 2020), this time in a plug-and-play rather than a fine-tuning context, also use KL divergence to penalize deviations from the initial policy.\nPointwise vs. Distributional View Most of the existing works on Controlled Generation have taken what we have called a pointwise view: focusing on the quality of each individual output, as opposed to distributional properties of the collection of all outputs. And in fact, the standard objective of RL is to optimize a pointwise reward. Even when policy-gradient methods do consider distributions over outputs, they only do as a tool towards producing maximal rewards; and in fact, it is a side effect of the limited capacity of the policy networks that such distributions do not peak on a single output, as would be the optimal outcome in cases of real-valued rewards with no ties. 17 By contrast to this usual optimization \"intent\", our own intent here is explicitly distributional, and the policies we are looking for are not simply tools towards maximizing scores, but actual objectives in their own right.\nSuch a change of perspective might be argued against in the case of conditional seq2seq problems, such as Machine Translation, where focusing on a single good output for a given input makes sense, but is clearly in-adapted when focusing on language models where sample diversity is a requirement.\nEnergy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002;LeCun et al., 2006;Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago. 18 There has been a recent surge of interest in these types of models across a variety of fields. Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016;Belanger & McCallum, 2016). Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data. Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models. A recent survey of EBMs for text is provided in Bakhtin et al. (2020).\n\nF HYPERPARAMETERS AND TRAINING DETAILS\nWe implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019). For all experiments we start from a pretrained GPT-2 small (117M parameters) obtained from the Hugging-Face library (Wolf et al., 2019) and fine-tune for 3K gradient-update steps. Each training required 2 Nvidia V100 GPUs, the longest model took \u223c 72 hours to train. A list of the hyperparameters used for GDC and baselines is given in table 5. K refers to the number of gradient steps per iteration in Algorithm 2.\nN refers to the number of samples required and \u00b5 tolerance to the minimum tolerated error ||\u03bc \u2212 \u00b5(\u03bb)|| 2 2 while optimizing \u03bb, and \u03bb learning is the SGD step size for updating \u03bb in Algorithm 1. During training of the policy \u03c0 \u03b8 , we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with top p = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples. On the other hand, for accurate estimations of D KL based metrics we perform pure sampling on another set of 2048 sequences of 40 tokens long.\nFor word-lists in the pointwise experiments in section 3.2, we used the 4 word lists from the Plug and Play (Dathathri et al., 2020) repository 19 . As for the sentiment and clickbait classifiers, we used their pre-trained classifier heads over GPT-2 medium 20 .\nFor distributional and hybrid experiments, we fine-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies (Lebret et al., 2016) which we refer to as GPT-2 bio . To detect if a given text is about a female gender, we construct \u03c6 f emale (x) as a simple rule-based discriminator that depends on the percentage of female personal pronouns (she, her, hers, herself) w.r.t. all mentioned pronouns. We define four types of professions \"Art\", \"Science\", \"Business and Politics\", and \"Sports\". To detect them, we define a wordlist for each type as shown in table 6.   Large pretrained Language Models are often trained on uncurated data from the internet, where several demographics are severely underrepresented. One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia's biographies (Graells-Garrido et al., 2015). It is expected that such bias is transferred if not amplified by Language Models. Previous work has suggested associations of certain demographics with certain professions, sentiments and stereotypes (Sheng et al., 2019b;Brown et al., 2020b;Nadeem et al., 2020). This shows thaat Bias in LMs also shows up in different forms than just under-representation, and the task of debiasing LMs could require more a complex control method. GPT-2 bio demonstrates a large initial bias: over a large sample of size 20480 examples using top-p sampling (p = 0.9), it generates only around 7% female biographies. and a large imbalance between profession types \"Science\" (1%), \"Art\" (10%), \"Business&Politics\" (10%) and \"Sports\" (20%).\nIn this set of experiments, we demonstrate the potential of GDC as flexible general framework that can control pretrained Language Models to impose pointwise, distributional constraints, or even a mix between them (hybrid constraints). We design a set of 6 experiments whose descriptions and results are displayed in the figures below. Generation examples are provided in Table 7.\n\nGDC Desired\nFigure 10: Exp2: Multiple Distributional Constraints This experiment demonstrates the flexibility of GDC in dealing with several distributional constraints at once, even when these constraints have different objectives (increase, decrease, or keep fixed). We challenge the flexibility of GDC by setting four distributional constraints with four arbitrary expectation values targeting E\u03c6science and E\u03c6art at 40% and E\u03c6sports and E\u03c6 business at 10%. In the figure, from left to right, we can note the increase of E\u03c6science and E\u03c6art from 1.5% to 20.3% and from 10% to 31.6% respectively. Interestingly, the initial E\u03c6 business of GPT-2 bio (10.9%) is already very close to the desired expectation (10%), and we can see that during the course of the training, GDC keeps this value fixed as it is already satisfying the corresponding target distributional constraint. E\u03c6sports initially starts higher than the target distributional constraint 10%, and we can note that GDC succeeds to reduce it from 19.6% to 11.9%. In this experiment, we specify two types of constraints: pointwise with E\u03c6science(x) = 1.0 and distributional with E\u03c6 f emale (x) = 0.5. GDC in a single training procedure is able to increase the expectation of biographies about females from 7.4% to 28.8% and Science professions from 1.2% to 74.7%.\n\nArt Professions Biographies F\noraci mart\u00ednez rubin ( born october 24, 1982 ) is a puerto rican actress, dancer and model. she was the first puerto ... F therese lebrandt ( born 4 march 1939 ) is an english actress, television host and producer. she is known for her roles as lily lenox... , better known by his stage name zac banezi, is an israeli singer and songwriter. the producer of many artists, as well as the keyboardist of heavy metal band the.. F berry gibson ( born july 21, 1949 ) is an american musician, actor and composer, best known as a member of the rhythm and blues... balkrishnan dev is an indian actor who is known for his roles in telugu movies. he began his career with a short supporting role in \" sapikaya \". later he played .. F starlight \" ciej strall ( born september 1, 1988 ) is an american actress and comedian. she is best known for her role as el ... quentin brantley ( born april 27, 1973 ) is a canadian actor, composer, director, writer and producer. he is best known for his work.. \"\u00c1lvaro olajerra \" is an argentine comedian and actor. in 1983, he won an episode of c\u00e9spedes justicialiste de bola\u00f1os.. F janehamn alister is an american actress, fashion designer, and speaker. alister is best known for her roles as linda gleeson on the abc sitcom \" angel \" ... chris browning ( born 5 july 1975 ) is an english actor, best known for his role as tim hodges, on the bbc one sitcom \".. andy papadelaspe ( born 9 july 1973 ) is a french actor and director. he is known for his performances in several feature films including \" bern .. she served as deputy... ashaun \" tom \" hicks ( born july 28, 1986 ) is an american actress, singer, and beauty pageant contestant. he is also a journalist and .. izhev, born \" yuri aleksandrovich isov \" ( ; ), was a writer, journalist and politician. isov first became active in..\n\nSports Professions Biographies F\nisaba aguirre ( born 10 february 1983 in\u00c9ixidat, france ) is a female volleyball player from spain. she is a...\n\nH.4 TOKEN FREQUENCY ANALYSIS\nTo analyse in depth the effect of deviating much from the original GPT-2, for policies obtained from our method and each baseline, we obtain a large sample and filter to 4000 sequences that satisfy the imposed pointwise constraints for each of the 17 pointwise experiments explained in \u00a73. Figures  35, 36 and 37 plot a token frequency analysis for each of the training methods.\nThe vanilla policy gradient baselines REINFORCE suffer from very low diversity of generations; in the examples shown in section H.5 we note strong degeneration, in which all generations are composed of a few repeated tokens.\nREINFORCE P(x) suffers from a token diversity issue. As noticed and confirmed by generated examples shown section H.5, it often concentrates all the sequence probability mass on a single sequence which is often fluent and satisfies the constraint; however this leads to an extreme loss of sample diversity in almost all experiments. This shows the usefulness of our proposed analysis -in addition to the self-BLEU metrics -for distinguishing diversity at the sequence level or at the distribution level. Similarly, ZIEGLER (Ziegler et al., 2019) often suffers from the same lack of sample diversity (5 out of the 17 experiments); GDC obtains the highest diversity amongst all baselines, as demonstrated by the long tail in the figures below. It is important to note here that low sample diversity is also captured by the KL deviation from the original GPT-2 model i.e. D KL (\u03c0 \u03b8 a); GDC identifies the target distribution as the one which minimally deviates from the original policy while satisfying the constraints (p = arg min q\u2208C D KL (q, a)) is thus expected to preserve the high sample diversity of the original GPT-2.    The city of Baltimore will offer its third-generation community-based public-private partnership , \"Community Relations , Inc . , \" to build more than 1 , 1 0 Greece . The eurozone-wide unemployment rate plunged to 1 . 3 percent in June and remains below the EU average of 2 . 4 percent 1 0 Winnipeg Jets Injury Update : RW RW Blake Wheeler Winnipeg Jets Injury Update : RW RW Blake Wheeler Tampa Bay Lightning In 1 0 \"We know that if there's a way out of these problems , it's not by having a single one of them , \" he says 1 0 1 Clean Episode #2 --Sledgehammer 5 : The Longest War in the World! In this special episode , the Sledgehammer 5 team discusses their 1 0 A man who took a photograph of a police officer wearing a bulletproof vest and said it was him was charged with assault causing bodily 1 0 In a very big way , I like this book . The only difference here is that I got an amazing story from Jack . 1 0 I think we should be building the same thing for everyone . A shared economy that creates jobs and a shared supply of energy . Ziegler 1 0 \"There is no way I can do that . And that's not a small thing , \" he told the Guardian . \"So I have 1 0 . The first person I ever spoke with about it is a big fan . \"I thought it was pretty cool . I love everything 1 0 This is an easy tutorial to get started with the Django application . Once you understand how the Django application is implemented , you can 1 0 When you're a student with one of the most popular online courses available , you may find it easy to fall in love with what 1 0 BRAINSTOCK The UK could be on the cusp of becoming the first in the world to have its own free market . Bobby Bould 1 0 \"We have a lot of good options that will enable our employees to compete better , improve our efficiency and create more value for the 1 0 \"That was like a lot of good times to me . \" He says . The group of five men in their late 30s went 1 0 You can view all posts of this blog here Table 8: Randomly selected generations from the single-word constraint task for the word \"Wikileaks\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 9: Randomly selected generations from the single-word constraint task for the word \"Vampire\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got an e-mail from a couple of folks that we found interesting and amusing . They asked if I could have an idea of 1 1 The \"Black Friday\" holiday has some amusing details about the price of goods on Thanksgiving weekend , and they are included in the holiday's list 1 1 \"It was amusing and very amusing for all of us to witness , \" he said . \"But it also was not a good time Korea's first president has said he will resign after he failed to reach agreement with North Korea on the group's nuclear programme and warned he 1 0 A group of students in the United States were arrested this week , on charges of criminal sexual misconduct , after they allegedly engaged in 1 0 Gigabyte has partnered with Intel to provide Linux developers with a full-text search engine , which can be used to find Linux-related documents . In 1 0 \"The real story is that , this time , it's really been about women's rights , \" Trump said . \"The real story is , 1 0 RICHMOND , Va . (WPRI) -Three people were killed and two others were injured when a bus was derailed Thursday morning at Union Station 1 0 U . S . Department of Energy's National Renewable Energy Laboratory (NREL) will begin pumping the first water from California reservoirs in a month in 1 0 . Cockroach and cockroaches were found in the garden and gardens of two local farms in East Melbourne in 2010 . A farmer who worked Ziegler 1 1 I really don't know why she was so excited about the \"I'm going to be in my own game . \" It was amusing to 1 1 You can see , the whole point of this post is to get back to the \"What is it all about ? \" point . 1 1 \"You know , it's all that has happened in a couple of weeks in the last two weeks , \" said Smith . \"It's amusing 1 1 Consequences of the War . I will not answer any questions . However it is amusing to see how many \"fancy\" books have been published 1 1 In fact , I'd say that this game is the closest thing I've ever seen to the real life story of the main characters . 1 1 The only thing more amusing , however , was to see how it went down . The last person who ever read this piece would 1 1 It may be an amusing fact that the American Society of Pediatricians and Surgeons does not endorse circumcision . However , it is actually the 1 0 Cannot be created with your username Cannot be created with your username Cannot be created with your username Cannot be created with your username Can't Table 10: Randomly selected generations from the single-word constraint task for the word \"amusing\" (with occurrence probability 1/10 4 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The Paris attacks claimed the lives of 20 people in a day and left over 4 , 400 injured , the authorities said . The 1 1 In Paris , a major tourist attraction in the Middle East with a long history of terrorist attacks , the Charlie Hebdo massacre and the 1 1 As the Paris attack unfolded , the European Union and the U . S . took to Twitter to describe the attack . A tweet 1 1 The Paris massacre in November 2012 was carried out under a pretext of preventing terrorism . But on this basis , the attackers knew nothing 1 1 In Paris on Monday , a delegation of 50 members of the European Commission was set to discuss the issue of the EU's plan to 1 1 In his Paris address , President Hollande pledged to work with France to fight \"the scourge of terrorism . \" On Sunday , in a 1 1 A man who allegedly attacked a girl in Paris was sentenced to 15 years to life in prison for killing three children in 2012 , 1 1 Cairo , July 18 -The Paris terrorist attacks , which killed 14 people , killed 16 , wounded 13 more and left a third Table 11: Randomly selected generations from the single-word constraint task for the word \"Paris\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 In 2014 , in an attempt to stop the restaurant industry from becoming a \"corporate welfare racket\" for the masses , the city of San 1 0 A New Jersey man was arrested early Thursday morning on suspicion of possessing a gun and was placed under investigation by the police department , 1 1 SINGAPORE -A sushi restaurant owner has been jailed for 10 years for allegedly stealing money from a customer during the summer . A witness 1 1 The restaurant 's owner , James Saito , was suspended without pay last month after he said he accidentally broke the glass in front of a 1 1 A local restaurant chain on Monday announced its intention to offer a variety of meals and snacks to customers in the form of ice cream 1 1 I've never been in a restaurant before , but the atmosphere at the restaurant was very different than I remembered . And with only a 1 1 Watchers was founded in 1993 by a restaurant co-owner who wanted a place that had a true Southern feel . The restaurant opened on June 1 1 A restaurant in the heart of the San Antonio area has been turned into an art gallery by a local entrepreneur . The restaurant in San Antonio , Texas is known for a \"Southern Texas food\" philosophy that has given it its name , according to the 1 1 We've had a lot of success with this , and a lot of great things . There's this restaurant . We were all over it 1 1 I'm really pleased with my purchase! The menu was the same with a lot of restaurant options and I couldn't say enough good things about 1 1 \"I wanted to bring this restaurant to town , \" said Jim Dorn , who manages the restaurant 's business department . \"I knew we were 1 1 The world's oldest restaurant chain , the Cinco de Mayo , offers a mix of comfort food and classic Southern hospitality with its iconic Italian 1 1 Saucer has been offering the restaurant the chance to offer a one-hour service for all its guests , but not necessarily at a premium . 1 1 SALT LAKE CITY -Three Utah restaurant owners have filed suit to force restaurant owner Jimmy Denny to close after his company failed to report 1 1 Fellow restaurant owners , remember that while every once in a while a friend invites you to his or her own restaurant , you never Table 12: Randomly selected generations from the single-word constraint task for the word \"restaurant\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nWe are doing this in collaboration with you! We've done amazing work to make Minecraft an amazing game . However , in the past , 1 1 This game is amazing ! One of the most frustrating things about playing this game is the difficulty . There is no leveling system , and 1 1 A team of Japanese scientists has found that the world's largest nuclear plant could be a disaster waiting to happen . \"This amazing discovery reveals 1 0 So there we were , looking at a gorgeous game . That was something I enjoyed when I played a bit of a Zelda , 1 1 I just found out about this and am super excited to get it for you guys! Its amazing how many great games I can find 1 1 Thanks to amazing support , you have had access to this content for years , but have it been delivered to you in the form 1 1 What an amazing time to be a professional football fan! WeWe're sure John and John 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't do our share of 11 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John would have 1 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're also pretty sure John 18 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We're sure John and John 2 1 Say thanks by giving John a tip and help them continue to share amazing Things with the Thingiverse community . We't get enough of the Ziegler 1 1 We need to make sure that this type of work will be shared . The amazing and talented team at Google has just announced a 1 1 I've been waiting for this amazing piece of artwork since I heard of it on the New York Times' \"The Art of Comic-Con 2012\" podcast 1 1 I love this site because I'm trying to find the right answers to every question I have as a designer . It's amazing how much 1 1 The New York Times is going to be out doing something amazing with its coverage of the presidential election . The paper is already releasing 1 1 You'll see a lot of amazing video games coming out of Sony's booth , all from Sony and Nintendo , in the coming months . 1 1 The New York City FC academy and its training group were both awarded two year contracts in 2014 with an amazing $2 . 5 million 1 1 My favorite part of this tutorial is when you watch it , the amazing detail of the line up . It's so fun to watch 1 1 You have amazing taste , can be enjoyed by yourself or others . Our wines are not for sale , so this is for you Table 13: Randomly selected generations from the single-word constraint task for the word \"amazing\" (with occurrence probability 1/10 3 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.   Table 15: Randomly selected generations from the single-word constraint task for the word \"China\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. The US has announced that it will launch a new drone war game for the game \"Call of Duty : Black Ops 2 , \" 1 1 A group of Chinese-Americans has sued US President Donald Trump in a bid to force him to pay their former student visa fees . Chinese 1 1 A U . S . Army soldier who was killed in Iraq is the second US soldier to be killed in the country since January 1 1 Haitian officials are trying to make sure the US forces who stormed Iraq will be held responsible for their actions . They want the US Table 16: Randomly selected generations from the single-word constraint task for the word \"US\" (with occurrence probability 1/10 2 ) highlighted in green. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.  Table 17: Randomly selected generations from the word-list constraint task for the kitchen word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I got these last year when they were $500 , but I didn't get a monster when they went out in 2012 , so this 1 0 A man who appeared in a video calling on supporters to be loyal to the Muslim faith is being attacked by an attacker who then 1 1 The ghost of her father is here , and it's time to get a ghost back . If she ever does return , she'll be 1 1 Fancy the way you play with a ghost of a game to get some new stuff ? Get it here! Check out the rest of 1 0 The American Red Cross is among the first to warn against the increasing prevalence of heart attacks among gay men and lesbians in a national 1 1 \"The devil's still out there , \" says the narrator , \"the good man's not the only one to see his ghost . 1 This is a great way to explore the life of this world . I was a very happy person , happy because I was the 1 1 I'll get into the beast of the bush in a bit , but in the last few minutes I've got a pretty good feel for 1 1 I am a big fan of the fantasy genre , but that is a topic for another time . I can tell you that I 1 1 In the years that followed , the Internet was transformed by the advent of the Internet in 1999 , with Facebook (FB) and Google (GOOGL) 1 1 A strange ghost is haunting the ruins of ancient Babylon . In one of those horror movies , a ghost is caught in a mysterious 1 0 \"We're seeing that now in the case of Syria , \" the judge said . \"That's why the State of Canada should not take it 1 0 \"The world should stop playing dead . The world should start playing alive . \" That was the line of the voice that emerged from 1 1 I just wanted to try it out . I'm so excited about it and just started a new game , and it works . It's In a major development in government's attempt to block further progress in the process of nationalisation of its commerce , the state government , in 1 1 The government may not prosecute a group of government-owned enterprises for its political , economic , or administrative purposes in its national economy . Article 1 1 The United States government has ordered a court order to enforce state laws or governmental power over the personal conduct of its political subdivision in 1 1 The government has ordered an order on its release of a dozen government ministers in attempts to block its operation in judicial proceedings in its 1 1 The state government's monopoly on its economic power over the political , economic , or administrative process in order of its citizens in order to 1 1 In its attempt to block access to the state government in its political action , government made an attempt to restrict economic activity in order 1 1 The government will invoke its powers against the government in court of India against its order seeking a order in its internal order in its 1 1 In its campaign against economic independence in its efforts to enforce an effective state monopoly on its political power in its state , the Government REINFORCE P( It has taken several years for the government to finally acknowledge the real issues facing the Australian population . This is because the most pressing 1 0 We had hoped that the election would be a simple one-sided affair between those who don't support the Republican Party and those who do . 1 1 The government of Saskatchewan has a long history of lobbying on behalf of business interests . The province recently passed an omnibus tax bill that 1 1 The NDP has taken the issue of whether the state has a \"fundamental right\" to free trade to the forefront in its annual platform , 1 1 By Steve Nelles More than two-thirds of Texans are expected to sign off on the state's future tax code in January , with a possible 1 1 An appeals court in Ohio ruled Monday that the state's refusal to allow a transgender employee to use the state bathroom of her choice violated 1 1 The government will set aside $2 . 4 billion to fund more than 800 schools in the South African state , including many in the  Table 20: Randomly selected generations from the word-list constraint task for the computers word-list. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\nreps \u03c6(x) Generation GDC 1 1 I have to say I was impressed with the way the writing and narration was done . The way they were presented , especially the 1 0 'I'm thrilled to say my team is on the way!' tweeted Sadiq Khan . The London Mayor is joining the \"Great London Olympics\" movement to 1 1 You are going to enjoy this book! It is a beautiful collection of beautifully detailed stories . It is a treasure trove of information for 1 0 It's a fascinating conversation that we have in the world of cryptocurrency . It's so much fun . The people who have been running the 1 0 Tired of waiting for the next best thing to happen , you know it . You want to know . We are dedicated to helping 1 1 We love your feedback , so we are pleased to bring you the most powerful and best-selling product that will satisfy your needs and your 1 1 \"Thank you all for the service this site gives me , \" he said . \"Thank you for the work I've been doing with the 1 1 \"The most amazing thing about this game is that there is no other games that have been released like this . It has such a REINFORCE 1 1 Enhanced performance with our world-renown world-renown exhibitions worldwide . We believe our clients with extraordinary audiences of our highest quality productions productions of outstanding international 1 1 Dramatic high quality performance quality products of leading global international audiences of the highest quality high quality high quality international leading worldwide markets leading global 1 1 Create beautiful stunning gifts of extraordinary quality gifts of beautiful high quality quality productions of the highest quality premier productions worldwide impact worldwide reach quality 1 1 Designed with the highest quality quality performance materials of our clients' top quality talent clients' top brands' leading global brands' leading worldwide attention-grab worldwide audiences 1 1 High quality artistry of the highest quality quality productions of worldwide worldwide world-renown audiences of world-renown worldwide audiences worldwide acclaim highest quality productions of our 1 1 Explore stunning quality productions of highest quality international premier excellence of top international premier quality international audiences' highest impact productions of the highest global highest 1 1 Highquality high quality productions with outstanding quality quality productions together the highest value clients' highest quality and highest level highest impact performance of our clients' 1 1 High quality quality artistry of quality high quality production value . The highest quality product highest quality productions of our customers' highest quality customers' highest REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1 1 I really have to say this about the two albums that I've been getting : \"Walking on Water\" and \"The Road . \" They're both 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 3560 1 Be the first to know . No one covers what is happening in our community better than we do . And with a digital subscription 4418 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Table 21: Randomly selected generations from the classifier-based constraint task for very positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations. 0 \"These are the kind of people we're going to have in our community for years to come , \" said Donny , the father of 1 1 \"A great book , \" said Mr . Moore , who has been writing an introduction to the work . \"But it is a wonderful 1 1 The great question of all time is \"who would have guessed that this was so different and fun ? \" This is the question I 1 1 \"I'm a big fan of all kinds of things and I can say that I've always been an avid fan of everything . The team 1 1 Today , it's nice to be back in the game! I want to offer some great games to show your support for your favourite artists 1 0 Categories Categories Select Category A Very Important Stuff A Very Important Thing You Need To Know A Very Important Thing You Should Know A Very REINFORCE 1 1 Our Mission is bringing together the best culinary adventure of this year's National Holiday is a wonderful celebration of true love , with which I 1 1 Our newest dish is Celebrate Our Harvest is bringing together a celebration of celebrating our unique culinary culinary journey and adventure has inspired us to 1 1 Our Mission is to Help Bring Together the best Korean Heritage and Celebration has inspired by our love and support for the Korean Heritage Tour 1 1 Our annual Taste and Taste brings together incredible culinary treats with wonderful ingredients to give us that we know we have , loved and enjoyed 1 1 Our special fundraiser to welcome our wonderful friend , The Red Queen is hosting a celebration and honor this wonderful gem is all deserves is 1 1 Our unique and eclectic evening celebrates our love for love has inspired us this year to share the joy and joy our little ones have 1 1 Our Mission at the Great Black History & Cultural Center celebrates the true story of our great African American has brought together a creative exploration 1 1 Our Mission is bringing together events and fun events that bring together a truly unique gift with this wonderful event brings together such amazing people REINFORCE P(x) 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 10000 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much Ziegler 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 Our team has long supported the idea of using your knowledge and talents to make a more efficient , effective and sustainable way of making 1238 1 Thank you for supporting the journalism that our community needs! For unlimited access to the best local , national , and international news and much 1 1 The 2017 Season is about to roll out a big , fun , and exciting new lineup with the addition of a very special guest 1 1 \"I'm happy that he took his time and let everyone know that I'm going to take the same steps as everyone else with the same 1 1 This is a great day for those who love art , poetry , and the world to get together and have a great time . 1 1 Gather up the best and best food at an affordable price . We offer a wide selection of vegan and vegetarian options and all our 1 1 The latest in our series of guides for working with digital artisans . We offer a number of free tools , including Photoshop and Illustrator Table 22: Randomly selected generations from the classifier-based constraint task for positive sentiment control. Tokens are highlighted with yellow with different intensities to indicate their overall frequencies in the generated corpus. \u03c6(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of its repetitions across all generations.\n</current_paper>\n\n<cited_paper>\nSequence Level Training with Recurrent Neural Networks\n\nMany natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.\n\n\nINTRODUCTION\nNatural language is the most natural form of communication for humans. It is therefore essential that interactive AI systems are capable of generating text (Reiter & Dale, 2000). A wide variety of applications rely on text generation, including machine translation, video/text summarization, question answering, among others. From a machine learning perspective, text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context. For instance, given an image, generate an appropriate caption or given a sentence in English language, translate it into French.\nPopular choices for text generation models are language models based on n-grams (Kneser & Ney, 1995), feed-forward neural networks (Morin & Bengio, 2005), and recurrent neural networks (RNNs; Mikolov et al., 2010). These models when used as is to generate text suffer from two major drawbacks. First, they are trained to predict the next word given the previous ground truth words as input. However, at test time, the resulting models are used to generate an entire sequence by predicting one word at a time, and by feeding the generated word back as input at the next time step. This process is very brittle because the model was trained on a different distribution of inputs, namely, words drawn from the data distribution, as opposed to words drawn from the model distribution. As a result the errors made along the way will quickly accumulate. We refer to this discrepancy as exposure bias which occurs when a model is only exposed to the training data distribution, instead of its own predictions. Second, the loss function used to train these models is at the word level. A popular choice is the cross-entropy loss used to maximize the probability of the next correct word. However, the performance of these models is typically evaluated using discrete metrics. One such metric is called BLEU (Papineni et al., 2002) for instance, which measures the n-gram overlap between the model generation and the reference text. Training these models to directly optimize metrics like BLEU is hard because a) these are not differentiable (Rosti et al., 2011), and b) combinatorial optimization is required to determine which sub-string maximizes them given some context. Prior attempts (McAllester et al., 2010;He & Deng, 2012) at optimizing test metrics were restricted to linear models, or required a large number of samples to work well (Auli & Gao, 2014). This paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm addresses the two issues discussed above as follows. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our final evaluation metric. Our proposed methodology bor-rows ideas from the reinforcement learning literature (Sutton & Barto, 1988). In particular, we build on the REINFORCE algorithm proposed by Williams (1992), to achieve the above two objectives. While sampling from the model during training is quite a natural step for the REINFORCE algorithm, optimizing directly for any test metric can also be achieved by it. REINFORCE side steps the issues associated with the discrete nature of the optimization by not requiring rewards (or losses) to be differentiable. While REINFORCE appears to be well suited to tackle the text generation problem, it suffers from a significant issue. The problem setting of text generation has a very large action space which makes it extremely difficult to learn with an initial random policy. Specifically, the search space for text generation is of size O(W T ), where W is the number of words in the vocabulary (typically around 10 4 or more) and T is the length of the sentence (typically around 10 to 30).\nTowards that end, we introduce Mixed Incremental Cross-Entropy Reinforce (MIXER), which is our first major contribution of this work. MIXER is an easy-to-implement recipe to make REINFORCE work well for text generation applications. It is based on two key ideas: incremental learning and the use of a hybrid loss function which combines both REINFORCE and cross-entropy (see Sec. 3.2.2 for details). Both ingredients are essential to training with large action spaces. In MIXER, the model starts from the optimal policy given by cross-entropy training (as opposed to a random one), from which it then slowly deviates, in order to make use of its own predictions, as is done at test time.\nOur second contribution is a thorough empirical evaluation on three different tasks, namely, Text Summarization, Machine Translation and Image Captioning. We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) Venkatraman et al., 2015). We also compare MIXER with another simple yet novel model that we propose in this paper. We call it the End-to-End BackProp model (see Sec. 3.1.3 for details). Our results show that MIXER with a simple greedy search achieves much better accuracy compared to the baselines on all the three tasks. In addition we show that MIXER with greedy search is even more accurate than the cross entropy model augmented with beam search at inference time as a post-processing step. This is particularly remarkable because MIXER with greedy search is at least 10 times faster than the cross entropy model with a beam of size 10. Lastly, we note that MIXER and beam search are complementary to each other and can be combined to further improve performance, although the extent of the improvement is task dependent. 1 2 RELATED WORK Sequence models are typically trained to predict the next word using the cross-entropy loss. At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014;Bahdanau et al., 2015;Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it makes the generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details).\nThe idea of improving generation by letting the model use its own predictions at training time (the key proposal of this work) was first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning. They then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework. Unfortunately, for text generation it is generally intractable to compute an oracle of the optimal target word given the words predicted so far. The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015) and applied for text generation by Bengio et al. (2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model's prediction. While DAD usually improves generation, it seems unsatisfactory to force the model to predict a certain word regardless of the preceding words (see sec. 3.1.2 for more details).  Table 1: Text generation models can be described across three dimensions: whether they suffer from exposure bias, whether they are trained in an end-to-end manner using back-propagation, and whether they are trained to predict one word ahead or the whole sequence.\nFinally, REINFORCE has already been used for other applications, such as in computer vision (Mnih et al., 2014;Xu et al., 2015;Ba et al.), and for speech recognition Graves & Jaitly (2014). While they simply pre-trained with cross-entropy loss, we found that the use of a mixed loss and a more gentle incremental learning scheduling to be important for all the tasks we considered.\n\nMODELS\nThe learning algorithms we describe in the following sections are agnostic to the choice of the underlying model, as long as it is parametric. In this work, we focus on Recurrent Neural Networks (RNNs) as they are a popular choice for text generation. In particular, we use standard Elman RNNs (Elman, 1990) and LSTMs (Hochreiter & Schmidhuber, 1997). For the sake of simplicity but without loss of generality, we discuss next Elman RNNs. This is a parametric model that at each time step t, takes as input a word w t \u2208 W as its input, together with an internal representation h t . W is the the vocabulary of input words. This internal representation h t is a real-valued vector which encodes the history of words the model has seen so far. Optionally, the RNN can also take as input an additional context vector c t , which encodes the context to be used while generating the output.\nIn our experiments c t is computed using an attentive decoder inspired by Bahdanau et al. (2015) and Rush et al. (2015), the details of which are given in Section 6.2 of the supplementary material. The RNN learns a recursive function to compute h t and outputs the distribution over the next word: The parametric expression for p \u03b8 and \u03c6 \u03b8 depends on the type of RNN. For Elman RNNs we have: where the parameters of the model \u03b8 are the set of matrices {M o , M i , M h , M c } and also the additional parameters used to compute c t . Softmax(x) is a vector whose components are e xj / k e x k , and 1(i) is an indicator vector with only the i-th component set to 1 and the rest to 0. We assume the first word of the sequence is a special token indicating the beginning of a sequence, denoted by w 1 = \u2205. All entries of the first hidden state h 1 are set to a constant value.\nNext, we are going to introduce both baselines and the model we propose. As we describe these models, it is useful to keep in mind the key characteristics of a text generation system, as outlined in Table 1. There are three dimensions which are important when training a model for text generation: the exposure bias which can adversely affect generation at test time, the ability to fully back-propagate gradients (including with respect to the chosen inputs at each time step), and a loss operating at the sequence level. We will start discussing models that do not possess any of these desirable features, and then move towards models that better satisfy our requirements. The last model we propose, dubbed MIXER, has all the desiderata.\n\nWORD-LEVEL TRAINING\nWe now review a collection of methodologies used for training text generation models which optimize the prediction of only one word ahead of time. We start with the simplest and the most popular method which optimizes the cross-entropy loss at every time step. We then discuss a recently proposed modification to it which explicitly uses the model predictions during training. We finish by Figure 1: RNN training using XENT (top), and how it is used at test time for generation (bottom). The RNN is unfolded for three time steps in this example. The red oval is a module computing a loss, while the rectangles represent the computation done by the RNN at one step. At the first step, all inputs are given. In the remaining steps, the input words are clamped to ground truth at training time, while they are clamped to model predictions (denoted by w g t ) at test time. Predictions are produced by either taking the argmax or by sampling from the distribution over words.\nproposing a simple yet novel baseline which uses its model prediction during training and also has the ability to back propagate the gradients through the entire sequence. While these extensions tend to make generation more robust, they still lack explicit supervision at the sequence level.\n\nCROSS ENTROPY TRAINING (XENT)\nCross-entropy loss (XENT) maximizes the probability of the observed sequence according to the model. If the target sequence is [w 1 , w 2 , . . . , w T ], then XENT training involves minimizing: When using an RNN, each term p(w t |w 1 , . . . , w t\u22121 ) is modeled as a parametric function as given in Equation (5). This loss function trains the model to be good at greedily predicting the next word at each time step without considering the whole sequence. Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986) with gradient clipping (Mikolov et al., 2010).\nOnce trained, one can use the model to generate an entire sequence as follows. Let w g t denote the word generated by the model at the t-th time step. Then the next word is generated by: Notice that, the model is trained to maximize p \u03b8 (w|w t , h t+1 ), where w t is the word in the ground truth sequence. However, during generation the model is used as p \u03b8 (w|w g t , h t+1 ). In other words, during training the model is only exposed to the ground truth words. However, at test time the model has only access to its own predictions, which may not be correct. As a result, during generation the model can potentially deviate quite far from the actual sequence to be generated. Figure 1 illustrates this discrepancy.\nThe generation described by Eq. (7) is a greedy left-to-right process which does not necessarily produce the most likely sequence according to the model, because: The most likely sequence [w 1 , w 2 , . . . , w T ] might contain a word w t which is sub-optimal at an intermediate time-step t. This phenomena is commonly known as a search error. One popular way Figure 2: Illustration of DAD Venkatraman et al., 2015). Training proceeds similar to XENT, except that at each time step we choose with a certain probability whether to take the previous model prediction or the ground truth word. Notice how a) gradients are not backpropagated through the eventual model predictions w g t , and b) the XENT loss always uses as target the next word in the reference sequence, even when the input is w g t .\nto reduce the effect of search error is to pursue not only one but k next word candidates at each point. While still approximate, this strategy can recover higher scoring sequences that are often also better in terms of our final evaluation metric. This process is commonly know as Beam Search.\nThe downside of using beam search is that it significantly slows down the generation process. The time complexity grows linearly in the number of beams k, because we need to perform k forward passes for our network, which is the most time intensive operation. The details of the Beam Search algorithm are described in Section 6.3.\n\nDATA AS DEMONSTRATOR (DAD)\nConventional training with XENT suffers from exposure bias since training uses ground truth words as opposed to model predictions. DAD, proposed in (Venkatraman et al., 2015) and also used in  for sequence generation, addresses this issue by mixing the ground truth training data with model predictions. At each time step and with a certain probability, DAD takes as input either the prediction from the model at the previous time step or the ground truth data. Bengio et al. (2015) proposed different annealing schedules for the probability of choosing the ground truth word. The annealing schedules are such that at the beginning, the algorithm always chooses the ground truth words. However, as the training progresses the model predictions are selected more often. This has the effect of making the model somewhat more aware of how it will be used at test time. Figure 2 illustrates the algorithm.\nA major limitation of DAD is that at every time step the target labels are always selected from the ground truth data, regardless of how the input was chosen. As a result, the targets may not be aligned with the generated sequence, forcing the model to predict a potentially incorrect sequence. For instance, if the ground truth sequence is \"I took a long walk\" and the model has so far predicted \"I took a walk\", DAD will force the model to predict the word \"walk\" a second time. Finally, gradients are not back-propagated through the samples drawn by the model and the XENT loss is still at the word level. It is not well understood how these problems affect generation.\n\nEND-TO-END BACKPROP (E2E)\nThe novel E2E algorithm is perhaps the most natural and na\u00efve approach approximating sequence level training, which can also be interpreted as a computationally efficient approximation to beam search. The key idea is that at time step t + 1 we propagate as input the top k words predicted at the previous time step instead of the ground truth word. Specifically, we take the output distribution over words from the previous time step t, and pass it through a k-max layer. This layer zeros all but the k largest values and re-normalizes them to sum to one. We thus have: where i t+1,j are indexes of the words with k largest probabilities and v t+1,j are their corresponding scores. At the time step t + 1, we take the k largest scoring previous words as input whose contributions is weighted by their scores v's. Smoothing the input this way makes the whole process differentiable and trainable using standard back-propagation. Compared to beam search, this can be interpreted as fusing the k possible next hypotheses together into a single path, as illustrated in Figure 3. In practice we also employ a schedule, whereby we use only the ground truth words at the beginning and gradually let the model use its own top-k predictions as training proceeds.\ntop-k Figure 3: Illustration of the End-to-End BackProp method. The first steps of the unrolled sequence (here just the first step) are exactly the same as in a regular RNN trained with cross-entropy. However, in the remaining steps the input to each module is a sparse vector whose non-zero entries are the k largest probabilities of the distribution predicted at the previous time step. Errors are backpropagated through these inputs as well.\nWhile this algorithm is a simple way to expose the model to its own predictions, the loss function optimized is still XENT at each time step. There is no explicit supervision at the sequence level while training the model.\n\nSEQUENCE LEVEL TRAINING\nWe now introduce a novel algorithm for sequence level training, which we call Mixed Incremental Cross-Entropy Reinforce (MIXER). The proposed method avoids the exposure bias problem, and also directly optimizes for the final evaluation metric. Since MIXER is an extension of the REIN-FORCE algorithm, we first describe REINFORCE from the perspective of sequence generation.\n\nREINFORCE\nIn order to apply the REINFORCE algorithm (Williams, 1992;Zaremba & Sutskever, 2015) to the problem of sequence generation we cast our problem in the reinforcement learning (RL) framework (Sutton & Barto, 1988). Our generative model (the RNN) can be viewed as an agent, which interacts with the external environment (the words and the context vector it sees as input at every time step). The parameters of this agent defines a policy, whose execution results in the agent picking an action. In the sequence generation setting, an action refers to predicting the next word in the sequence at each time step. After taking an action the agent updates its internal state (the hidden units of RNN). Once the agent has reached the end of a sequence, it observes a reward. We can choose any reward function. Here, we use BLEU (Papineni et al., 2002) and ROUGE-2 (Lin & Hovy, 2003) since these are the metrics we use at test time. BLEU is essentially a geometric mean over n-gram precision scores as well as a brevity penalty (Liang et al., 2006); in this work, we consider up to 4-grams. ROUGE-2 is instead recall over bi-grams. Like in imitation learning, we have a training set of optimal sequences of actions. During training we choose actions according to the current policy and only observe a reward at the end of the sequence (or after maximum sequence length), by comparing the sequence of actions from the current policy against the optimal action sequence. The goal of training is to find the parameters of the agent that maximize the expected reward. We define our loss as the negative expected reward: where w g n is the word chosen by our model at the n-th time step, and r is the reward associated with the generated sequence. In practice, we approximate this expectation with a single sample from the distribution of actions implemented by the RNN (right hand side of the equation above and Figure 9 of Supplementary Material). We refer the reader to prior work (Zaremba & Sutskever, 2015;Williams, 1992) for the full derivation of the gradients. Here, we directly report the partial derivatives and their interpretation. The derivatives w.r.t. parameters are: where o t is the input to the softmax. The gradient of the loss L \u03b8 with respect to o t is given by: wherer t+1 is the average reward at time t + 1.\nThe interpretation of this weight update rule is straightforward. While Equation 10 is standard backpropagation (a.k.a. chain rule), Equation 11 is almost exactly the same as the gradient of a multiclass logistic regression classifier. In logistic regression, the gradient is the difference between the prediction and the actual 1-of-N representation of the target word: Therefore, Equation 11 says that the chosen word w g t+1 acts like a surrogate target for our output distribution, p \u03b8 (w t+1 |w g t , h t+1 , c t ) at time t. REINFORCE first establishes a baseliner t+1 , and then either encourages a word choice w g t+1 if r >r t+1 , or discourages it if r <r t+1 . The actual derivation suggests that the choice of this average rewardr t is useful to decrease the variance of the gradient estimator since in Equation 9 we use a single sample from the distribution of actions.\nIn our implementation, the baseliner t is estimated by a linear regressor which takes as input the hidden states h t of the RNN. The regressor is an unbiased estimator of future rewards since it only uses past information. The parameters of the regressor are trained by minimizing the mean squared loss: ||r t \u2212 r|| 2 . In order to prevent feedback loops, we do not backpropagate this error through the recurrent network (Zaremba & Sutskever, 2015).\nREINFORCE is an elegant algorithm to train at the sequence level using any user-defined reward. In this work, we use BLEU and ROUGE-2 as reward, however one could just as easily use any other metric. When presented as is, one major drawback associated with the algorithm is that it assumes a random policy to start with. This assumption can make the learning for large action spaces very challenging. Unfortunately, text generation is such a setting where the cardinality of the action set is in the order of 10 4 (the number of words in the vocabulary). This leads to a very high branching factor where it is extremely hard for a random policy to improve in any reasonable amount of time.\nIn the next section we describe the MIXER algorithm which addresses these issues, better targeting text generation applications.\n\nMIXED INCREMENTAL CROSS-ENTROPY REINFORCE (MIXER)\nThe MIXER algorithm borrows ideas both from DAGGER (Ross et al., 2011) and DAD (Venkatraman et al., 2015;Bengio et al., 2015) and modifies the REINFORCE appropriately. The first key idea is to change the initial policy of REINFORCE to make sure the model can effectively deal with the large action space of text generation. Instead of starting from a poor random policy and training the model to converge towards the optimal policy, we do the exact opposite. We start from the optimal policy and then slowly deviate from it to let the model explore and make use of its own predictions. We first train the RNN with the cross-entropy loss for N XENT epochs using the ground truth sequences. This ensures that we start off with a much better policy than random because now the model can focus on a good part of the search space. This can be better understood by comparing the perplexity of a language model that is randomly initialized versus one that is trained. Perplexity is a measure of uncertainty of the prediction and, roughly speaking, it corresponds to the average number of words the model is 'hesitating' about when making a prediction. A good language model trained on one of our data sets has perplexity of 50, whereas a random model is likely to have perplexity close to the size of the vocabulary, which is about 10, 000.\nThe second idea is to introduce model predictions during training with an annealing schedule in order to gradually teach the model to produce stable sequences. Let T be the length of the sequence. After the initial N XENT epochs, we continue training the model for N XE+R epochs, such that, for every sequence we use the XENT loss for the first (T \u2212 \u2206) steps, and the REINFORCE algorithm for the remaining \u2206 steps. In our experiments \u2206 is typically set to two or three. Next we anneal the number of steps for which we use the XENT loss for every sequence to (T \u2212 2\u2206) and repeat the training for another N XE+R epochs. We repeat this process until only REINFORCE is used to train the whole sequence. See Algorithm 1 for the pseudo-code. Figure 4: Illustration of MIXER. In the first s unrolling steps (here s = 1), the network resembles a standard RNN trained by XENT. In the remaining steps, the input to each module is a sample from the distribution over words produced at the previous time step. Once the end of sentence is reached (or the maximum sequence length), a reward is computed, e.g., BLEU. REINFORCE is then used to back-propagate the gradients through the sequence of samplers. We employ an annealing schedule on s, starting with s equal to the maximum sequence length T and finishing with s = 1. We call this algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) because we combine both XENT and REINFORCE, and we use incremental learning (a.k.a. curriculum learning). The overall algorithm is illustrated in Figure 4. By the end of training, the model can make effective use of its own predictions in-line with its use at test time.\n\nEXPERIMENTS\nIn all our experiments, we train conditional RNNs by unfolding them up to a certain maximum length. We chose this length to cover about 95% of the target sentences in the data sets we consider. The remaining sentences are cropped to the chosen maximum length. For training, we use stochastic gradient descent with mini-batches of size 32 and we reset the hidden states at the beginning of each sequence. Before updating the parameters we re-scale the gradients if their norm is above 10 (Mikolov et al., 2010). We search over the values of hyper-parameter, such as the initial learning rate, the various scheduling parameters, number of epochs, etc., using a held-out validation set. We then take the model that performed best on the validation set and compute BLEU or ROUGE score on the test set. In the following sections we report results on the test set only. Greedy generation is performed by taking the most likely word at each time step. 2\n\nTEXT SUMMARIZATION\nWe consider the problem of abstractive summarization where, given a piece of \"source\" text, we aim at generating its summary (the \"target\" text) such that its meaning is intact. The data set we use to train and evaluate our models consists of a subset of the Gigaword corpus (Graff et al., 2003) as described in Rush et al. (2015). This is a collection of news articles taken from different sources over the past two decades. Our version is organized as a set of example pairs, where each pair is composed of the first sentence of a news article (the source sentence) and its corresponding headline (the target sentence). We pre-process the data in the same way as in (Rush et al., 2015), which consists of lower-casing and replacing the infrequent words with a special token denoted by \"<unk>\". After pre-processing there are 12321 unique words in the source dictionary and 6828 words in the target dictionary. The number of sample pairs in the training, validation and test set are 179414, 22568, and 22259 respectively. The average sequence length of the target headline is about 10 words. We considered sequences up to 15 words to comply with our initial constraint of covering at least 95% of the data.\nOur generative model is a conditional Elman RNN (Equation 3) with 128 hidden units, where the conditioning vector c t is provided by a convolutional attentive encoder, similar to the one described in Section 3.2 of Rush et al. (2015) and inspired by Bahdanau et al. (2015). The details of our attentive encoder are mentioned in Section 6.2 of the Supplementary Material. We also tried LSTMs as our generative model for this task, however it did not improve performance. We conjecture this is due to the fact that the target sentences in this data set are rather short.\n\nMACHINE TRANSLATION\nFor the translation task, our generative model is an LSTM with 256 hidden units and it uses the same attentive encoder architecture as the one used for summarization. We use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). The corpus consists of sentence-aligned subtitles of TED and TEDx talks. We pre-process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as casing. The training data comprises of about 153000 sentences where the average English sentence is 17.5 words long and the average German sentence is 18.5 words long. In order to retain at least 95% of this data, we unrolled our RNN for 25 steps. Our validation set comprises of 6969 sentence pairs which was taken from the training data. The test set is a concatenation of dev2010, dev2012, tst2010, tst2011 and tst2012 which results in 6750 sentence pairs. The English dictionary has 22822 words while the German has 32009 words.\n\nIMAGE CAPTIONING\nFor the image captioning task, we use the MSCOCO dataset (Lin et al., 2014). We use the entire training set provided by the authors, which consists of around 80k images. We then took the original validation set (consisting of around 40k images) and randomly sampled (without replacement) 5000 images for validation and another 5000 for test. There are 5 different captions for each image. At training time we sample one of these captions, while at test time we report the maximum BLEU score across the five captions. The context is represented by 1024 features extracted by a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not back-propagate through these features. We use a similar experimental set up as described in Bengio et al. (2015). The RNN is a single layer LSTM with 512 hidden units and the image features are provided to the generative model as the first word in the sequence. We pre-process the captions by lower-casing all words and replacing all the words which appear less than 3 times with a special token \"<unk>\". As a result the total number of unique words in our dataset is 10012. Keeping in mind the 95% rule, we unroll the RNN for 15 steps.\n\nRESULTS\nIn order to validate MIXER, we compute BLEU score on the machine translation and image captioning task, and ROUGE on the summarization task. The input provided to the system is only the context and the beginning of sentence token. We apply the same protocol to the baseline methods as well. The scores on the test set are reported in Figure 5.\nWe observe that MIXER produces the best generations and improves generation over XENT by 1 to 3 points across all the tasks. Unfortunately the E2E approach did not prove to be very effective. Training at the sequence level and directly optimizing for testing score yields better generations than turning a sequence of discrete decisions into a differentiable process amenable to standard back-propagation of the error. DAD is usually better than the XENT, but not as good as MIXER.\nOverall, these experiments demonstrate the importance of optimizing for the metric used at test time. In summarization for instance, XENT and MIXER trained with ROUGE achieve a poor performance in terms of BLEU (8.16 and 5.80    Next, we experimented with beam search. The results in Figure 6 suggest that all methods, including MIXER, improve the quality of their generation by using beam search. However, the extent of the improvement is very much task dependent. We observe that the greedy performance of MIXER (i.e., without beam search) cannot be matched by baselines using beam search in two out of the three tasks. Moreover, MIXER is several times faster since it relies only on greedy search.\nIt is worth mentioning that the REINFORCE baseline did not work for these applications. Exploration from a random policy has little chance of success. We do not report it since we were never able to make it converge within a reasonable amount of time. Using the hybrid XENT-REINFORCE loss without incremental learning is also insufficient to make training take off from random chance. In order to gain some insight on what kind of schedule works, we report in Table 2\n\nCONCLUSIONS\nOur work is motivated by two major deficiencies in training the current generative models for text generation: exposure bias and a loss which does not operate at the sequence level. While Reinforcement learning can potentially address these issues, it struggles in settings when there are very large action spaces, such as in text generation. Towards that end, we propose the MIXER algorithm, which deals with these issues and enables successful training of reinforcement learning models for text generation. We achieve this by replacing the initial random policy with the optimal policy of a cross-entropy trained model and by gradually exposing the model more and more to its own predictions in an incremental learning framework.\nOur results show that MIXER outperforms three strong baselines for greedy generation and it is very competitive with beam search. The approach we propose is agnostic to the underlying model or the form of the reward function. In future work we would like to design better estimation techniques for the average rewardr t , because poor estimates can lead to slow convergence of both REINFORCE and MIXER. Finally, our training algorithm relies on a single sample while it would be interesting to investigate the effect of more comprehensive search methods at training time.\n\nQUALITATIVE COMPARISON\nCONTEXT: a chinese government official on sunday dismissed reports that the government was delaying the issuing of third generation -lrb-#g -rrb-mobile phone licenses in order to give a developing <unk> system an advantage GROUND TRUTH: foreign phone operators to get equal access to china 's #g market XENT: china dismisses report of #g mobile phone phone DAD: china denies <unk> <unk> mobile phone licenses E2E: china 's mobile phone licenses delayed MIXER: china official dismisses reports of #g mobile licenses CONTEXT: greece risks bankruptcy if it does not take radical extra measures to fix its finances , prime minister george papandreou warned on tuesday , saying the country was in a '' wartime situation GROUND TRUTH: greece risks bankruptcy without radical action XENT: greece warns <unk> measures to <unk> finances DAD: greece says no measures to <unk> <unk> E2E: greece threatens to <unk> measures to <unk> finances MIXER: greece does not take radical measures to <unk> deficit CONTEXT: the indonesian police were close to identify the body parts resulted from the deadly explosion in front of the australian embassy by the dna test , police chief general <unk> <unk> said on wednesday GROUND TRUTH: indonesian police close to <unk> australian embassy bomber XENT: indonesian police close to <unk> DAD: indonesian police close to <unk> E2E: indonesian police close to monitor deadly australia MIXER: indonesian police close to <unk> parts of australian embassy CONTEXT: hundreds of catholic and protestant youths attacked security forces with <unk> bombs in a flashpoint area of north belfast late thursday as violence erupted for the second night in a row , police said GROUND TRUTH: second night of violence erupts in north belfast XENT: urgent hundreds of catholic and <unk> <unk> in <unk> DAD: hundreds of belfast <unk> <unk> in n. belfast E2E: hundreds of catholic protestant , <unk> clash with <unk> MIXER: hundreds of catholic <unk> attacked in north belfast CONTEXT: uganda 's lord 's resistance army -lrb-lra -rrb-rebel leader joseph <unk> is planning to join his commanders in the ceasefire area ahead of talks with the government , ugandan army has said GROUND TRUTH: rebel leader to move to ceasefire area XENT: uganda 's <unk> rebel leader to join ceasefire DAD: ugandan rebel leader to join ceasefire talks E2E: ugandan rebels <unk> rebel leader MIXER: ugandan rebels to join ceasefire in <unk> CONTEXT: a russian veterinary official reported a fourth outbreak of dead domestic poultry in a suburban moscow district sunday as experts tightened <unk> following confirmation of the presence of the deadly h#n# bird flu strain GROUND TRUTH: tests confirm h#n# bird flu strain in # <unk> moscow <unk> XENT: russian official reports fourth flu in <unk> DAD: bird flu outbreak in central china E2E: russian official official says outbreak outbreak outbreak in <unk> MIXER: russian official reports fourth bird flu CONTEXT: a jewish human rights group announced monday that it will offer <unk> a dlrs ##,### reward for information that helps them track down those suspected of participating in nazi atrocities during world war ii GROUND TRUTH: jewish human rights group offers reward for information on nazi suspects in lithuania XENT: jewish rights group announces <unk> to reward for war during world war DAD: rights group announces <unk> dlrs dlrs dlrs reward E2E: jewish rights group offers reward for <unk> MIXER: jewish human rights group to offer reward for <unk> CONTEXT: a senior u.s. envoy reassured australia 's opposition labor party on saturday that no decision had been made to take military action against iraq and so no military assistance had been sought from australia GROUND TRUTH: u.s. envoy meets opposition labor party to discuss iraq XENT: australian opposition party makes progress on military action against iraq DAD: australian opposition party says no military action against iraq E2E: us envoy says no decision to take australia 's labor MIXER: u.s. envoy says no decision to military action against iraq CONTEXT: republican u.s. presidential candidate rudy giuliani met privately wednesday with iraqi president jalal talabani and indicated that he would keep a u.s. presence in iraq for as long as necessary , campaign aides said GROUND TRUTH: giuliani meets with iraqi president , discusses war XENT: <unk> meets with president of iraqi president DAD: republican presidential candidate meets iraqi president E2E: u.s. president meets with iraqi president MIXER: u.s. presidential candidate giuliani meets with iraqi president Figure 7: Examples of greedy generations after conditioning on sentences from the test summarization dataset. The \"<unk>\" token is produced by our tokenizer and it replaces rare words.  Training in expectation (Reinforce) Greedy training such as XENT optimizes only the probability of the next word. The model may consider choices indicated by the green arrows, but it starts off from words taken from the ground truth sequence. The model experiences exposure bias, since it sees only words branching off the ground truth path; (Right) REINFORCE and MIXER optimize over all possible sequences, using the predictions made by the model itself. In practice, the model samples only a single path indicated by the blue solid line. The model does not suffer from exposure bias; the model is trained as it is tested.\n</cited_paper>\n\n<citance>\nPrior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.\n</citance>\n\nAdditional instructions: Each question should be about the papers rather than a generic question. Each question should be about the papers rather than a generic question. Each question should be about the papers rather than a generic question. Also make sure that the answer is not just a short phrase that can be extracted from the paper by searching. The answer should avoid idiosyncratic terminology as much as possible, but should instead use language where a reader unfamiliar with the paper but is genearlly well-read in the field should be able to understand."
    },
    "response_format": {
        "type": "json_object"
    },
    "temperature": 0.0
}
