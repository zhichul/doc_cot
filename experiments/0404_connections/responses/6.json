{
    "prompt": [
        {
            "content": "Your are an expert in machine learning and natural language processing. You need to analyze papers.\n",
            "role": "system"
        },
        {
            "content": "Here's a paper Controlling Conditional Language Models with Distributional Policy Gradients\n\nMachine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g. hallucination in abstractive summarization or wrong format in automatic code generation). This raises an important question on how to adapt pre-trained generative models to a new task without destroying its capabilities. Recent work has suggested to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Unfortunately, this approach is limited to unconditional distributions, represented by unconditional EBMs. In this paper, we extend this approach to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on three different control objectives across two tasks: summarization with T5 and code generation with GPT-Neo. Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.\n\n\nIntroduction\nPretrained generative models are shifting the landscape of machine learning research and practice. General purpose models such as the GPT family (Radford et al., 2019;Brown et al., 2020;Black et al., 2021), T5 (Raffel et al., 2020), CLIP (Radford et al., 2021) and Codex (Chen et al., 2021) are trained in a self-supervised manner on large amounts of uncurated data and can then be adapted to specific downstream tasks (e.g. generating Python code) or control objectives (e.g. controlling the style of generated code). Frequently, control objectives are motivated by the desire to address the shortcomings of certain pretrained models. These can be due to the uncurated nature of the original training data (e.g. a large portion of Python source code on the Internet violates PEP8 (van Rossum et al., 2001), the Python Style Guide) or the difficulty of learning a desired behaviour by purely selfsupervised traning (e.g. there is not enough training signal to ensure that a model trained on source code always generates compilable code or that a summarization model always produces factually correct summaries). The practice of adapting and controlling pretrained generative models poses two open problems. The first is that control objectives frequently lack ground truth data that could be used for supervised finetuning; in general, we are only given an indicator b(x) of whether a given sample from the model x satisfies a given control objective. Thus, approaches such as reinforcement learning (RL) (Pasunuru & Bansal, 2018;Ziegler et al., 2019), weighted decoding (Ghazvininejad et al., 2017;Holtzman et al., 2018;See et al., 2019) or decoding with perturbed activations (Dathathri et al., 2020) need to be employed. The second problem is catastrophic forgetting: most approaches to enforcing a control objective result in a dramatic loss of capabilities of the original model beyond the scope of the control objective. Notably, there exists one approach largely avoiding both of these problems (Parshakova et al., 2019a;Khalifa et al., 2021): representing the control objective as an energy-based models (EBM) and approximating that EBM using distributional policy gradients (DPG).\nAlthough this approach shows great improvements in controlling pretrained language models while avoiding catastrophic forgetting (Khalifa et al., 2021), it is limited to unconditional generation tasks and is not able to fine-tune conditional models that are behind the most impactful NLP tasks such as machine translation, summarization or dialogue systems.\nIn this paper, we present Conditional DPG (CDPG), an extension of DPG that can approximate conditional EBMs. A conditional EBM P defines an unnormalized distribution for each context c (e.g. a source document). Extending the approach of Khalifa et al. (2021) we build a conditional EBM that represents the ideal behaviour of a generative model given context c as the distribution that incorporates the control objectives while remaining as close as possible to the original distribution. Then, to approximate conditionals EBMs we build unconditional EBMs P c on the fly for any given context and estimate their normalizing constants Z c . = x P c (x) by importance sampling.\nWe demonstrate the effectiveness of CDPG in addressing shortcomings of pretrained generative models by considering two tasks, summarization and code generation, and two corresponding pretrained generative models: T5 (Raffel et al., 2020) and GPT-Neo (Black et al., 2021). An open problem in summarization is ensuring that summaries are factually faithful to source documents given that summarization models are prone to hallucinating named entities never mentioned in the source (Maynez et al., 2020). 2 We show that a preference for factually faithful summaries (operationalized as entity-level factual consistency (Nan et al., 2021)) can be represented by a conditional EBM. Then, we show that using CDPG to fine-tune T5 to approximate this EBM increases the number of correct and relevant named entities in summaries and improves T5's Rouge score. In contrast with RL approaches, CDPG does not degrade the diversity and quality of summaries. For code generation, we consider the task of generating a Python function given its signature (name and arguments). While general-purpose language models can generate idiomatic Python functions (Chen et al., 2021;Austin et al., 2021), they still may struggle to learn some desirable properties of generated code. For instance, a Python function generated by GPT-Neo will compile only 40% of the time and will contain on average 4 violations of PEP8. We show that using CDPG to approximate a conditional EBM expressing corresponding constraints improves both compilability and PEP8compliance without hurting the diversity of summaries or leading to degeneration (Holtzman et al., 2020).\nTo summarize, the contributions of this paper are as follows: 1. We introduce a formal framework for representing control objectives for conditional generative models as conditional EBMs, 2. We design CDPG, an extension of DPG suitable for approximate conditional EBMs, 3. We evaluate CDPG on three control objectives across two tasks: summarization constrained to be factually correct, code generation constrained to generate compilable Python functions and code generation constrained to respect PEP8.\n\nMethod\nConditional EBMs A standard, \"unconditional\" EBM is a function P from a (discrete, i.e. finite or countable) space X to the non-negative reals, such that the partition function Z . = x\u2208X P (x) is strictly positive and finite. We will denote by lowercase p the normalized distribution over X associated with P , namely p(x) . = P (x)/Z. Let us now consider a discrete, potentially infinite set C of conditions c. Formally, P, a conditional EBM over C, is defined as a function from C to the set of unconditional EBMs over X, in other words, a function that maps each c \u2208 C to an unconditional EBM P c over X: (1) P c : x \u2192 R + . ( We denote by Z c the partition function of P c (x), namely: Z c . = x\u2208X P c (x), and by p c (x) the normalized version of P c (x), namely: p c (x|c) . = P c (x)/Z c .We will use the shortcut notation P(x|c) to denote P(c)(x): a conditional EBM P evaluated first at a given context c (resulting in an unconditional EBM P c ) and then at given x (resulting in a non-negative EBM score).\nRepresenting constraints as conditional EBMs The problem of fine-tuning a pretrained model a(x|c) to satisfy a control objective (e.g. generating factually correct summaries) can be seen as a constraint satisfaction problem: finding a model p(x|c) that meets the demands of the control objective but at the same time stays as close as possible to the original pretrained model a(x|c). We represent such an optimal model as a conditional EBM P (x, c). A control objective can be defined in terms of a binary scorer b(x, c) such that b(x, c) = 1 if a sample (c, x) satisfies a constraint given by a control objective (e.g. x is factually correct with respect to c) and b(x, c) = 0 otherwise. Let us consider a set of contexts c \u2208 C. For each c, there is a unique model p c (x) such that (i) b(x, c) = 1 for all samples x \u223c p c (x), and (ii) p c (x) has minimal KL divergence from a(\u00b7|c). Following our example, p c could be a distribution over factually correct summaries of c as similar as possible to a distribution over summaries the original model a would produce for a document c. In information-geometric terms, p c is an I-projection of a onto the manifold of all distributions satisfying a constraint given by b (Csisz\u00e1r & Shields, 2004). As shown by (Khalifa et al., 2021), p c can be represented as an unconditional EBM P c (x) of the following form: 3 Approximating conditional EBMs While P represents the target condtional model optimally reconciling distance from a(x|c) and the control objective, sampling or MAP decoding for p(x|c) is intractable for two reasons. First, p(x|c) actually represents a potentially infinite collection of unconditional models of the form p c (x). Second, each of these conditional models still cannot be easily sampled from because they do not admit an autoregressive factorisation: b(x, c) is only defined for the whole sequence x.\nThe second problem was addressed by Parshakova et al. (2019b) and Khalifa et al. (2021) who used the distributional policy gradients (DPG) algorithm to approximate unconditional EBMs p using a new unconditional model \u03c0 \u03b8 trained to minimize the cross-entropy CE(p, \u03c0 \u03b8 ). Unfortunately, DPG is no longer usable for conditional models that should satisfy infinitely many constraints represented by the set of conditional EBMs defined by P.\nTo address the second problem, we instead try to find a single model \u03c0 \u03b8 approximating p on average across contexts. Concretely, we minimize the expected cross-entropy between \u03c0 \u03b8 and multiple p c 's: where the expectation is over \u03c4 (c), a distribution over c \u2208 C. The gradient of this objective takes the following form: where in (7) we applied importance sampling from \u03c0 \u03b8 and (8) expresses p c in terms of unconditional EBM P c and its partition function Z c . 4 We approximate both expectations in (8) by sampling. Intuitively, this corresponds to building unconditional EBMs P c (\u00b7) on the fly for each c \u223c \u03c4 (c), computing the EBM \"score\" P c (x) for each sample from the conditional model x \u223c \u03c0 \u03b8 (\u00b7|x) and then using this score as a \"pseudoreward\" term P c (x)/Z c \u03c0 \u03b8 (x|c) in the policy gradient estimate.\n\nP(x|c)\n\u03c0 \u03b8 (x|c) \u2207 \u03b8 log \u03c0 \u03b8 (x|c) Output: \u03c0 \u03b8 Estimating Z c The one term in (8) that remains difficult to evaluate is the partition function Z c . For a single unconditional EBM, Khalifa et al. (2021) had no need to evaluate its partition function Z: it just scaled gradient estimates and therefore could be absorbed into the learning rate. In the conditional case, Z c varies with c. Therefore for each c, we compute Z c using a batch of M samples {x 1 , . . . , x j , . . . x M } from \u03c0 \u03b8 (x|c i ). Then, Z c is estimated using importance sampling by reweighting samples x j by their likelihood according to \u03c0 \u03b8 (\u00b7|c i ).\nTraining loop We train \u03c0 \u03b8 by stochastic gradient descent using the gradient estimate (8). At each epoch, we first sample N contexts c and then, for each c, M samples x from \u03c0 \u03b8 (x|c). We maintain a buffer B storing each (c i , x j ) pair along with its corresponding partition function Z ci . Finally, we shuffle B and iterate over it to perform gradient steps using (8) with learning rate \u03b1 (\u03b8) . The procedure is summarized in Algorithm 1. \u03b1 (\u03b8) , N and M are hyperparameters. Values used in experiments are reported in Tables 1-2 in the Appendix.\n\nExperiments\nWe evaluate CDPG as well as three baselines on three control objectives across two tasks: summarization and code generation. Each task is associated with C train , a set of contexts c used for prompting the model: these are Python function signatures in case of code generation and source documents in case of summarization. When computing evaluation metrics, we sample contexts from a held out set C test not used for training. In addition to that, for each experiment, we measure E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), the expected forward KL divergence from the optimal distribution p c , as well as E c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a), the expected reverse KL divergence from the original pretrained model. 5 .\n\nBaselines\nUnconditional DPG We compare our algorithm against the original DPG for unconditional EBMs developed by Parshakova et al. (2019a) and extended by Khalifa et al. (2021). This algorithm differs from conditional DPG: it ignores the partition function Z c in the denominator of (8) assuming it to be constant (which it is for unconditional EBMs but not for conditional ones). For a fair comparison, we use the on-policy variant (called \"DPG-on\" in (Parshakova et al., 2019b)) where we sample from \u03c0 \u03b8 as opposed to a different proposal distribution q.\nReinforcement learning The problem of fine-tuning a pretrained model to satisfy a pointwise constraint b(x, c) can be posed as maximising tbe expected reward E c\u223c\u03c4 (c) E x\u223c\u03c0 \u03b8 (x|c) R(x, c). We consider two instances of this approach: Reinforce (Williams, 1992) and Ziegler (Ziegler et al., 2019). For Reinforce, we simply define R(x, c) = b(x, c). Ziegler prevents too large departures from a by adding a KL penalty term and defining R(x, c) = b(x, c) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), where \u03b2 is a hyperparameter updated using an adaptive schedule.\n\nExperimental setup\nDataset To conduct our summarization experiments, we use the CNN/DailyMail dataset (Nallapati et al., 2016): we sampled 5k source documents from the train and test subsets of CNN/DailyMail to use for fine-tuning and evaluation, respectively. \u03c4 (c) from Algorithm 1 will be a uniform distribution over a given subset of source documents. Note that neither CDPG nor the baselines utilise ground truth summaries; we compute b(x, c) based on source documents and generated summaries. However, we use ground truth summaries for computing reference-based evaluation metrics such as Rouge score or recall-target.\nModel We conduct our experiments on the T5 architecture (Raffel et al., 2020), using the pretrained model t5-small (60m parameters) as \u03c0 \u03b8 . During fine-tuning, we generate summaries x conditioned on a source document c by pure ancestral sampling from \u03c0 \u03b8 ; for evaluation, we follow the setup described by Raffel et al. (2020) and use beam search.\nConstraints Following Nan et al. (2021), we define an entity-level factual consistency constraint as a product of two constraints: there must be at least four named entities in the summary x and all the named entities x must have occurred in the source c. More formally, let NER(\u00b7) denote the set of named entities found in a text. Then, b(x, c) Metrics In addition to measuring expected D KL (p c , \u03c0 \u03b8 ) and D KL (\u03c0 \u03b8 , a), we evaluate the quality and factual consistency of generated summaries using the following metrics: 1. Precision-source (Nan et al., 2021), defined as |NER(x \u2229 c)|/|NER(x)| is the percentage of named entities in the summary that can be found in the source. Low precision-source indicates severe hallucination, 2. Recall-target (Nan et al., 2021), defined as |NER(x \u2229 t)|/|NER(t)|, is the percentage of named entities in the target summary t that can be found in the generated summary x. Results We present the evolution of our 7 metrics through time on Figure 2. CDPG is the only method stably decreasing expected D KL (p c , \u03c0 \u03b8 ) and thus approaching (as opposed to drifting away from) optimal distributions p c . This is reflected in moderate divergence from a and translates into downstream metrics. Summaries generated by the fine-tuned model contain, on average, more named entities. Moreover, name entities in summaries are both more factually consistent with source : Summarization with factual consistency constraint. Evaluation metrics: expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), precision-source (\u2191 better), recall-target (\u2191 better), number of named entities (\u2191 better), Distinct-2 (\u2191 better), Rouge-1 (\u2191 better) for models obtained from fine-tuning with conditional DPG, DPG, Ziegler and Reinforce.\n(an increase in precision-source) and more relevant (an increase in recall-target). The tendency towards mentioning more factually consistent named entities increases the bigram diversity within summaries (Distinct-2) and the overall quality of generated summaries compared to ground truth (Rouge-1). This last results might seem surprising given that CDPG did not have access to ground truth summaries. A plausible explanation is that the original pretrained model was biased towards mentioning too few factually correct entities, at least compared to ground truth summaries. Satisfying the factual consistency constraint reduced this bias.\nBaseline approaches fall short of achieving similar results. Standard DPG, the closest contender, still leaves a significant gap in terms of all metrics and is far less stable than CDPG (e.g. its D KL (p c , \u03c0 \u03b8 ) starts to diverge again after around 500 epochs). Ziegler stays extremely close to the original model a, failing to improve its shortcomings. In contrast, Reinforce heavily departs from a pushing it to mention a large number of named entities. This results in artificially inflated recall-target but no increase in precision-source and a decrease in Rouge-1. The additional named entities frequently are frequently irrelevant (i.e. not mentioned in ground truth summaries) or simply hallucinated. See Tables 3-7 in the Appendix for randomly chosen summaries of documents in the test set.\n\nCode generation\nDataset For code generation experiments, we condition a language model on Python function signatures (both of methods and standalone functions) extracted from the Python150 dataset which consists of Python source code obtained from GitHub . We use the code provided by Roziere et al. (2020) for function extraction and randomly choose 5k functions for C train and 5k for C test . \u03c4 (c) is a uniform distribution over these signatures. Note that neither in fine-tuning nor in evaluation do we use ground truth function bodies.\n\nModel\nWe conduct experiments using GPT-Neo (Black et al., 2021): an off-the-shelf, freely available autoregressive language model mirroring the GPT-3 architecture (Brown et al., 2020). GPT-Neo's training set included 85 GiB of source code from GitHub which endowed it with some code completion abilities (Gao et al., 2020). We use the gpt-neo-125 variant available on Huggingface Transformers (Wolf et al., 2019). During both fine-tuning and evaluation we generate function bodies by conditioning on signatures using pure ancestral sampling.   : Code generation with compilability (a) and PEP8 (b) constraint. Evaluation metrics: compilability (\u2191 better), number of PEP8 errors (\u2193 better), expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), number of characters, AST node count (\u2191 better) for models obtained from fine-tuning with CDPG, DPG, Ziegler and Reinforce.\n\nConstraints\npycodestyle, an off-the-shelf linter (static code analysis tool) Metrics We evaluate the quality of generated Python functions using the following metrics: 1. PEP8 error count, the average number of violations of PEP8, 2. Compilability, the fraction of samples [c, x] that compile, 3. The average number of characters in [c, x] (after detokenization), 4. The average number of nodes in an abstract syntax tree (AST) of sequences that compile.\nIntuitively, this metric indicates the logical (as opposed to surface) complexity of generated programs.\nFor more details on how scorers b and metrics are implemented, see Appendix B.3.\n\nResults\nWe present the evolution of metrics through time on Figure 3. CDPG was able to increase the fraction of compilable functions from around 40% to around 65% and decrease the average number of PEP8 violations. Incidentally, the PEP8 control objective also leads to an increase in compilability because many PEP8 violations are also compilation errors. Similarly to the summarization experiment, CDPG and DPG are the only methods actually approaching optimal distributions p c and diverging moderately from a. This allows them to maintain the original statistics of a: length and the number of nodes in AST trees of generated functions. In contrast, Reinforce learns to generate shorter functions (having less opportunity for mistakes) and Ziegler produces heavily degenerated samples (Holtzman et al., 2020): syntactically simple functions with severe repetitions. This is reflected in an increase in length and a decrease in AST nodes count. See Tables 8-10 and Tables  11-13 for randomly chosen samples from the compilability and PEP8 experiments, respectively.\nNote that the gap between CDPG and DPG is much closer for code generation (especially with compilability control objective) than for summarization. This can be accounted for by the normalized standard deviation of partition functions Z c for EBMs P c in the range of conditional EBMs P for each control objective. For code generation, this standard deviation is lower meaning that Z c in (8) is better approximated by a constant which can be absorb into the learning rate \u03b1 (\u03b8) . For summarization, this variance is higher, therefore ignoring the Z c term incurs higher bias. See Appendix C.1 for a comparison.\n\nQualitative analysis\nIn the previous sections, we showed how CDPG is able to fine-tune a pretrained model a to satisfy certain constraints without destroying a's capabilities. Here we attempt to gain a better understanding of how different fine-tuning approaches affect the distributions of final models. On Figure 4 we present frequencies of errors and named entities obtained from fine-tuned models. While errors and named entities differ significantly in their frequency, CDPG consistently decreases frequencies of these errors and consistently increases the frequencies of all kinds of named entities, including the long tail of rare ones.\nTo compare lexical diversity of samples obtained from fine-tuned models, we plot the frequency of each token (the number of times it occurs) and its rank (its index in a sorted list of tokens) in Figure 5. CDPG and DPG are able to closely match original token frequencies while Ziegler and Reinforce tend to have shorter tails of rare tokens.\n\nConclusion\nWe presented CDPG, a principled approach to fine-tuning conditional language models to satisfy arbitrary constraints. In contrast with other methods, CDPG does not require ground truth training data and is able to shift model distribution in a minimally invasive way. In consequence, models fine-tuned with CDPG share desired characteristics, such as improved factual consistency or compilability, with the fluency and diversity of the original model.\nFuture work could evaluate CDPG on other tasks -such as machine translation or dialogue response generation -as well as explore other control objectives such as constraining the semantics (as opposed to syntax) of generated Python functions. Another future direction consists in extending CDPG to approximate conditional analogues of the more general, exponential-form (\n\nA Extended related work\nReducing hallucinations in summarization Neural abstractive summarization is highly prone to hallucinate content in the summary that is unfaithful to the source document. Maynez et al. (2020) found that hallucinations occur in more than 70% of single-sentence summaries and most of these are extrinsic hallucinations: adding information not directly inferable from the input document. Therefore, a substantial effort was devoted to to improving factual consistency of abstractive summarization. Some notable attempts include reranking summaries based on their correctness predicted by entailment classifiers (Falke et al., 2019) or fine-tuning using RL with a reward derived from an entailment classifier (Pasunuru & Bansal, 2018). The notion of entity-level factual consistency -a property such that that all named entities in the summary are actually mentioned in the source document -was introduced (Nan et al., 2021) as is one way of operationalizing the notion of extrinsic hallucinations.\nControllable code generation Generating source code is an established application of language models (Nguyen et al., 2013;Raychev et al., 2014;Karpathy et al., 2015;) that recently enjoys renewed interest (Lu et al., 2021;Chen et al., 2021;Austin et al., 2021). The task is formulated both as unconditional generation (with applications in code completion, e.g. Codex (Lu et al., 2021) or GitHub Copilot 7 ) and as conditional generation (e.g. program synthesis or generating a program satisfying a given input-output specification, e.g. (Austin et al., 2021)). Our task of function generation can be seen as a simplified program synthesis when the specification is given by a function signature (a name of a function and a list of arguments). Previous work found compilability errors to be a signification failure mode of neural code generation (Roziere et al., 2020). Previous attempts at improving compilability of generated code include Maddison & Tarlow (2014), who augment neural probabilistic context free grammars with semantic constraints and use them for unconditional generation or Zhong et al. (2017), who used policy gradients to train a model translating natural language questions to corresponding SQL queries and -in addition for rewarding for query execution results -added a penalty for syntactically invalid queries. Most in line with our work, Korbak et al. (2021) were using DPG to improve compilability of unconditional language models for code.\nB Details of metric and score calculation B.1 KL divergences Calculation of metrics relative to p c 's, such as E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), requires estimating Z c 's. This is done using importance sampling from \u03c0 \u03b8 in a manner analogous to the training loop (Algorithm 1). Then, expected KL can be simplified to the following form: A small is added to Z c for stability. We approximate both expectations (over \u03c4 and \u03c0 \u03b8 ) using importance sampling. For a complete procedure, see Algorithm 2.\nE c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a) is computed in a simpler manner as it doesn't require estimating Z c 's and we can directly sample from \u03c0 \u03b8 . It boils down to sampling a batch of N contexts c i , a batch of M samples x j from \u03c0 \u03b8 (x|c i ) for each c i and evaluating: To avoid bias, when computing KL divergences we always sample from \u03c0 \u03b8 using pure ancestral sampling (as opposed to top p sampling or beam search decoding).\n\nB.2 Summarization\nFollowing Nan et al. (2021), we implement NER(\u00b7) as using a pretrained SpaCy (Honnibal et al., 2020) named entity recognizer. We use the en_core_web_sm model and restrict the named entities we extract to the following categories: PERSON, FAC (buildings, airports, highways, bridges, etc.), GPE (geopolitical entities: countries, cities, etc.), ORG (companies, agencies, institutions, etc.), NORP (nationalities or religious or political groups), LOC (Non-GPE locations: mountain ranges, bodies of water, etc.), EVENT (named hurricanes, battles, wars, sports events, etc.). Also following Nan et al. (2021), we ignore entities such as date, time and numerals due to large variation in their representation in documents.\n\nB.3 Code generation\nCompilability To check for compilability, we call the compile_command function from the codeop module of Python Standard Library 8 with a sequence obtained by string concatenation [c, x] as argument. We then check if compile_command returns a code object. The only postprocessing we apply is removing any characters from x after the end of function declaration (with function end defined in terms of indentation) as we are concerned specifically with function generation. codeop.compile_command is the implementation that Python interactive interpreters use in read-eval-print loop (REPL) to determine whether a string is a valid Python code. The method tries to compile a string of Python code and raise and exception if compilation fails, for instance a SyntaxError for invalid Python syntax and ValueError or OverflowError if there is an invalid literal. Note that our notion of compilability is concerned only with syntactic correctness as Python interpreter does not execute the body of a function at function declaration time.\nPEP8 To compute the number of PEP8 violations triggered by a sequence [c, x], we run pycodestyle, 9 a Python linter (static code analysis tool) and report the number of violations it reports.\nAST node count Finally, to compute AST node count, the average number of nodes in an abstract syntax trees (ASTs) of generated functions, we consider only samples [c, x] that compile. They are parsed to their corresponding ASTs using the ast module from Python Standard Library. 10 Lower normalized standard deviation for a task explains closer performance gap between CDPG and DPG for that task on Code generation with PEP8 normalized std of Zc Figure 6: Normalized standard deviations for Zc's associated with unconditional EBMs Pc in the codomain of conditional EBMs P defined for three control objectives: summarization with factual consistency constraint, code generation with compilability constraint and code generation with PEP8 constraint.\ninvalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation error original model a Conditional DPG 10 0 10 1 10 2 10 3 error count DPG 10 0 10 1 10 2 10 3 error count invalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation\n\nD Hyperparmeters and implementation details\nWe implemented all models using PyTorch (Paszke et al., 2019) and HuggingFace Transformers (Wolf et al., 2019). Each training run took approximately 5 days on 2 Nvidia V100 GPUs. For a detailed list of hyperparameter values, see Table 1 and 2. For a description of hyperparameters specific to Ziegle see (Ziegler et al., 2019).  Tables 3-7 for summaries of 5 randomly chosen documents from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. Summaries were generated with beam search.\n\nE.2 Code generation with compilability constraint\nSee Tables 8-10 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\n\nE.3 Code generation with PEP8 constraint\nSee Tables 11-13 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\nSource document c (CNN)\"Success Kid\" is likely the Internet's most famous baby. You've seen him in dozens of memes, fist clenched in a determined look of persevering despite the odds. Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. His dad, Justin, needs a kidney transplant. About a week ago, Laney Griner, Justin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000 to help cover the medical expenses that go along with a kidney transplant. The campaign is already a success. By Wednesday it had topped its goal. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. \"One can only survive with no natural kidney function ... for so long,\" Laney Griner said. \"His energy and mood are affected; he can no longer work, and he spends 12 hours a week in dialysis clinic. \"Having been on dialysis for this long greatly increases his risks of developing further complications. The only way to save his life is to get a transplant. There's no other way around that,\" she said. The family doesn't know when a kidney might become available. Their GoFundMe page has a link for potential donors. Sammy's Internet fame began in 2007 when his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. Myspace picked it up, so did Reddit. The rest is Internet history. Success just seems to run in some families.\n\nb(x, c) x\nSummary generated by the original model a 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Conditional DPG 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. about a week ago, Laney Griner, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using DPG 0 \"Success Kid\" is the internet's most famous baby. his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. a goFundMe campaign has a goal of $75,000 to help cover medical expenses.\nSummary generated by a model fine-tuned using Ziegler 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Reinforce 1 Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. Laney Griner, textcolorredJustin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. The family doesn't know when a kidney might become available. Sammy's Table 3: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c A picture, believed to be the only image of the Civil War ironclad, the CSS Georgia has been revealed to be a fake created in a teenage hoax using a 2ft model. John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. Scroll down for video . John Potter (pictured), from Savannah, has admitted forging the picture with his brother in the 1980s and placing the picture in a frame which now holds a picture of his dead dog . In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted . The picture Mr Potter handed over to the society was actually a picture of a picture. In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame. Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted, so he took a photo of it and then mailed it to historical groups in Savannah. Potter has now admitted the sham and explained how he falsified the image. When he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about the buildingand destruction -of the CSS Georgia in a fictional battle with Union troops. For the movie, they built an 18-foot long boat of plywood and Styrofoam, as well as a smaller 2-foot model. They based the design, in part, on his grandfather's recollections of details passed down by word of mouth through generations of their family. Potter also used an illustration of the ironclad he found on a postcard. To create the fake image Potter's younger brother put on a coat and straw hat went out to a marsh with a cane fishing pole and Potter took a photo. He took another photo of the 2-foot model and cut out the boat's image, glued it onto the photo of his brother, then used dirt and glue to create the illusion of a photo faded by age and stained by water or chemicals. Evidence of the hoax: All elements of the fakery were snapped by Potter at one time. The small model boat here appears alongside the false picture, polaroids of it and a 1984 copy of Mad Magazine . He bought an old picture frame and beat it up even further. He put the photo in it. Then he drove 120 miles to a yard sale in Waycross, Georgia, put the picture down and took a Polaroid of it. He laughs now, when he remembers that it had seemed so important that he actually do this at a yard sale, so at least that part would be true. 'Who knows what goes through the mind of a kid,' he said. The US Army Corps of Engineers appealed for information about the picture as it embarked on a project to raise the ironclad . Potter sent out the photo to historical groups, setting off the sporadic, and fruitless, search for a CSS Georgia photo that he now says never existed. As the Army Corps of Engineers embarked this year on a $14 million project to raise the wreckage of the real CSS Georgia from the river, archaeologists publicised the image online and in news stories hoping to track down the original photo. 'Wanted: A Photo Of This Confederate Battleship,' read the headline above the image on the Army Corps website. 'There are a lot of characteristics in the photograph that lead us to believe it is the CSS Georgia,' Julie Morgan, the Army Corps archaeologist in charge of raising the ironclad's remains, said in a February interview. 'On the other side, there are some skeptics who believe it's a complete fake.' Robert Holcombe, former curator of the National Civil War Naval Museum in Columbus, Georgia, said in February that while the original photograph would be needed to confirm if the image was authentic, he believed it was real. 'Most people seem to think so,' he said. 'Or else it's an awfully good fake'. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug. He never became a successful special effects artist with the exception of the one faked photo. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug .\n\nb(x, c) x\nSummary generated by the original model a 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Conditional DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, Summary generated by a model fine-tuned using DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. the picture became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, was 'CSS Georgia' Summary generated by a model fine-tuned using Ziegler 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Reinforce 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He told historians that Mr Potter handed over to the Georgia Historical Society and the photo became an unofficial part of the ship's history.\nPotter has now admitted the sham and explained how he falsified the image when he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about Table 4: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienaar has been dogged by groin and knee injuries this season limiting him to just 11 appearances. He returned to action from his latest setback in the 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Everton midfielder Steven Pienaar is held back by Swansea's Ki Sung-Yueng at the Liberty Stadium . Pienaar said: 'At one stage, I thought I had better just hang my boots up and call it a day but on the other side I was just thinking that I enjoy going in and seeing the guys so I just had to stay strong and that kept me going. 'When you are at home not coming in for training you feel very down but as soon as I walk through the door, there's always the camaraderie in the group, there is always fun. 'Even if you are injured, you can always laugh and it keeps you going. Just to be among the players, it's kept me going.' Pienaar has made just 11 appearances for the Toffees this season due to groin and knee injuries .\n\nb(x, c) x\nSummary generated by the original model a 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Conditional DPG 0 Steven Pienaar has been dogged by groin and knee injuries this season. the 33-year-old midfielder has made just 11 appearances for the toffees. he returned to action from his latest setback in the 1-1 draw with Swansea city earlier this month.\nSummary generated by a model fine-tuned using DPG 0 the 33-year-old midfielder has been dogged by groin and knee injuries this season. he returned to action from his latest setback in the 1-1 draw with Swansea city. pienaar has made just 11 appearances for the toffees this season due to injury.\nSummary generated by a model fine-tuned using Ziegler 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Reinforce 0 Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienanar has been dogged by groin and knee injuries this season for the Toffees. He returned to action from his latest setback in a 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Pienar said Pien Table 5: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency. Named entities in summaries are highlighted in red. constraint Source document c Expat: Detectives are investigating the killing of David King (pictured), a 70-year-old pensioner from Newham, east London, who retired to Normandy . A fight over stolen vegetables may have led to a British expat being murdered and dumped at the bottom of a well, French police fear. The macabre theory has been outlined by detectives investigating the killing of David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed 28-year-old Frenchman has been charged with his murder. The alleged killer was living rough in the area, and is thought to have been behind a number of thefts in the area. Now prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over stolen vegetables. Ms Etienne, who is leading the police enquiry, said: 'Among the vague attempts at an explanation given by the accused, we are looking at the possibility of a fight over the theft of foodstuffs. 'In one night, a whole plot of leeks might disappear. One person saw four rabbits vanishing overnight.' Ms Etienne added: 'People were getting more and more angry, and some were even threatening to defend their plots with shotguns'. Neighbours of Mr King, who was hugely proud of his vegetable garden, told Le Parisien newspaper that a description of the thief corresponds to the alleged murderer. Discovery: In February, Mr King's car, a Renault Scenic (above) was found parked in Vire, a nearby town, but there was no sign of any body . Family and neighbours of Mr King have attacked detectives for allowing his suspected murderer to remain at large for six months. Interpol, the international police organisation, had initially refused to open a missing person's enquiry. Instead they believed Mr King had travelled to Australia to see his daughter, Sandie Ray. Mr King had been living in France for 15 years . Ms Ray, who lives in Perth, Australia, said her father's passport details had been mixed up with another David King, who had indeed travelled to Australia from France. She said this was known by November last year, but 'the French authorities still hadn't been formally notified of this via Interpol until approximately three months later'. Ms Ray said a quicker enquiry would have avoided 'lots of anxiety and frustration for our family and dads friends.' John King, Mr King's son, who lives in Brighton, East Sussex, said the botched investigation had been a 'bureaucratic nightmare' for all concerned. Other expats living in the area said it was 'hugely frightening' to have a suspected murderer living in their midst while the operation went on. 'This is an isolated part of the world, and everyone is potentially vulnerable to attack,' said one. 'Detectives should have worked out what was going on far quicker. The slow speed of the enquiry was unacceptable.' In February, Mr King's car, a Renault Scenic was found parked in Vire, a nearby town, but there was no sign of any body.\n\nb(x, c) x\nSummary generated by the original model a 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Conditional DPG 1 the macabre theory has been outlined by detectives investigating the killing of David King, 70, from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year-old frenchman has been charged with his murder. prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over over stolen vegetables Summary generated by a model fine-tuned using DPG 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of caen, and an unnamed 28-year old frenchman has been charged with his murder. prosecutor Carole Etienne has said that the alleged killer may well have confronted the man over stolen vegetables Summary generated by a model fine-tuned using Ziegler 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Reinforce 1 David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed Frenchman has been charged with his murder. prosecutor Carole Etienne has said Mr King, keen gardener, may well have confronted the man over stolen vegetables, Neighbours of Mr King. Interpol, international police organisation Table 6: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c German legend Franz Beckenbauer has accused Bayern Munich stars of playing as if they had taken 'sleeping pills' in their midweek defeat by Porto. The Bundesliga champions conceded twice in the opening 10minutes before losing 3-1 to the Portuguese in the first leg of their Champions League quarter-final on Wednesday. Der Kaiser may be a brand ambassador for the club but he couldn't hide his feelings after the game, when he criticised defender Dante for playing as if he were wearing 'ski boots' before turning on the entire team. Franz Beckenbauer was speaking in New York as part of Bayern Munich's media agreement with MSN . New York Cosmos legends Beckenbauer and Pele pose together after launching the team's spring season . Ricardo Quaresma scored the opening goal from the spot in Porto's 3-1 defeat of the Germans . Beckenbauer accused Brazilian defender Dante (left) of playing as if he were wearing 'ski boots' Speaking to reporters in New York to mark the club's new media agreement with MSN, Beckenbauer said: 'It was one of those days, all the players didn't show their real performance. 'After 10 minutes you are 2-0 down in the quarter-final in the Champions League, so many mistakes. I never saw this before. I thought they took sleeping pills! Porto were much faster, real power. It made me angry. Quaresma celebrates with Porto team-mates after netting his second goal inside the first 10minutes . Bayern Munich head coach Pep Guardiola goes crazy at his team's performance . Bayern players trudge off after defeat knowing they will have to vastly improve to reach the semi-finals . 'If they played like that again, no chance [they can go through]. But they can win 2-0 or go to extra-time. They have a chance. Last year we had the same problem, playing excellent in the Bundesliga and then we had two bad days and were out of the competition.' The 69-year-old did attempt to play down his comments over Dante's performance, he said: 'At half-time, I have to give a comment and that was my reaction. And I'm sorry to give him the ski boots... but he's Brazilian, and Brazilian to me means for me, technique and Pele and... Brazil. But not this.' 'If you want to win the Champions League you need a strong league with a lot of players. Internationally, Bayern Munich belongs to the top teams but to win the Champions League. . . you saw the game probably two days ago, we are far, far away. With this performance, you don't win anything,' he added.\n\nb(x, c) x\nSummary generated by the original model a 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.\nSummary generated by a model fine-tuned using Conditional DPG 1 german legend Franz Beckenbauer was speaking in new york as part of the club's media agreement with MSN. he accused defender Dante of playing as if he were wearing'ski boots' the 69-year-old said: 'if they played like that again, no chance [they can go through] but they can win 2-0 or go to extra-time' Summary generated by a model fine-tuned using DPG 0 the germans conceded twice in the opening 10 minutes before losing 3-1 to porto. the 69-year-old criticised defender defender Dante for playing as if he were wearing'ski boots' he said: 'if they played like that again, no chance [they can go through]' Summary generated by a model fine-tuned using Ziegler 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.\n\nSuppose someone is trying to remember this paper but only remembered parts of it. What parts do you think would be salient, and what queries would the user put into the search engine? What about asking a colleague? Would they ask different queries to a human than a search engine? \n\nList your answer in two sections:\n<search_engine></search_engine> for search engine queries\n<human></human> for queries to a human colleague\n",
            "role": "user"
        }
    ],
    "responses": [
        "## Search Engine\nThe user might enter queries such as:\n- \"Controlling Conditional Language Models with Distributional Policy Gradients\"\n- \"Conditional DPG for language models\"\n- \"Fine-tuning pre-trained language models for specific tasks\"\n- \"Distributional policy gradients for conditional language models\"\n- \"Adapting pre-trained generative models to new tasks without catastrophic forgetting\"\n\n## Human\nWhen asking a colleague, the user might ask more open-ended or descriptive questions, such as:\n- \"Do you remember that paper about fine-tuning language models to meet specific requirements without losing their original capabilities?\"\n- \"Have you come across any research on using distributional policy gradients for controlling conditional language models?\"\n- \"I'm trying to recall a method for adapting pre-trained generative models to new tasks. Does Conditional DPG sound familiar to you?\"\n- \"Can you point me towards any papers or research on minimizing catastrophic forgetting when fine-tuning large language models for specific tasks?\"\n- \"Do you know of any techniques similar to Conditional DPG for controlling the output of language models?\""
    ],
    "example": {
        "prompt": [
            {
                "content": "Your are an expert in machine learning and natural language processing. You need to analyze papers.\n",
                "role": "system"
            },
            {
                "content": "Here's a paper Controlling Conditional Language Models with Distributional Policy Gradients\n\nMachine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g. hallucination in abstractive summarization or wrong format in automatic code generation). This raises an important question on how to adapt pre-trained generative models to a new task without destroying its capabilities. Recent work has suggested to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Unfortunately, this approach is limited to unconditional distributions, represented by unconditional EBMs. In this paper, we extend this approach to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on three different control objectives across two tasks: summarization with T5 and code generation with GPT-Neo. Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.\n\n\nIntroduction\nPretrained generative models are shifting the landscape of machine learning research and practice. General purpose models such as the GPT family (Radford et al., 2019;Brown et al., 2020;Black et al., 2021), T5 (Raffel et al., 2020), CLIP (Radford et al., 2021) and Codex (Chen et al., 2021) are trained in a self-supervised manner on large amounts of uncurated data and can then be adapted to specific downstream tasks (e.g. generating Python code) or control objectives (e.g. controlling the style of generated code). Frequently, control objectives are motivated by the desire to address the shortcomings of certain pretrained models. These can be due to the uncurated nature of the original training data (e.g. a large portion of Python source code on the Internet violates PEP8 (van Rossum et al., 2001), the Python Style Guide) or the difficulty of learning a desired behaviour by purely selfsupervised traning (e.g. there is not enough training signal to ensure that a model trained on source code always generates compilable code or that a summarization model always produces factually correct summaries). The practice of adapting and controlling pretrained generative models poses two open problems. The first is that control objectives frequently lack ground truth data that could be used for supervised finetuning; in general, we are only given an indicator b(x) of whether a given sample from the model x satisfies a given control objective. Thus, approaches such as reinforcement learning (RL) (Pasunuru & Bansal, 2018;Ziegler et al., 2019), weighted decoding (Ghazvininejad et al., 2017;Holtzman et al., 2018;See et al., 2019) or decoding with perturbed activations (Dathathri et al., 2020) need to be employed. The second problem is catastrophic forgetting: most approaches to enforcing a control objective result in a dramatic loss of capabilities of the original model beyond the scope of the control objective. Notably, there exists one approach largely avoiding both of these problems (Parshakova et al., 2019a;Khalifa et al., 2021): representing the control objective as an energy-based models (EBM) and approximating that EBM using distributional policy gradients (DPG).\nAlthough this approach shows great improvements in controlling pretrained language models while avoiding catastrophic forgetting (Khalifa et al., 2021), it is limited to unconditional generation tasks and is not able to fine-tune conditional models that are behind the most impactful NLP tasks such as machine translation, summarization or dialogue systems.\nIn this paper, we present Conditional DPG (CDPG), an extension of DPG that can approximate conditional EBMs. A conditional EBM P defines an unnormalized distribution for each context c (e.g. a source document). Extending the approach of Khalifa et al. (2021) we build a conditional EBM that represents the ideal behaviour of a generative model given context c as the distribution that incorporates the control objectives while remaining as close as possible to the original distribution. Then, to approximate conditionals EBMs we build unconditional EBMs P c on the fly for any given context and estimate their normalizing constants Z c . = x P c (x) by importance sampling.\nWe demonstrate the effectiveness of CDPG in addressing shortcomings of pretrained generative models by considering two tasks, summarization and code generation, and two corresponding pretrained generative models: T5 (Raffel et al., 2020) and GPT-Neo (Black et al., 2021). An open problem in summarization is ensuring that summaries are factually faithful to source documents given that summarization models are prone to hallucinating named entities never mentioned in the source (Maynez et al., 2020). 2 We show that a preference for factually faithful summaries (operationalized as entity-level factual consistency (Nan et al., 2021)) can be represented by a conditional EBM. Then, we show that using CDPG to fine-tune T5 to approximate this EBM increases the number of correct and relevant named entities in summaries and improves T5's Rouge score. In contrast with RL approaches, CDPG does not degrade the diversity and quality of summaries. For code generation, we consider the task of generating a Python function given its signature (name and arguments). While general-purpose language models can generate idiomatic Python functions (Chen et al., 2021;Austin et al., 2021), they still may struggle to learn some desirable properties of generated code. For instance, a Python function generated by GPT-Neo will compile only 40% of the time and will contain on average 4 violations of PEP8. We show that using CDPG to approximate a conditional EBM expressing corresponding constraints improves both compilability and PEP8compliance without hurting the diversity of summaries or leading to degeneration (Holtzman et al., 2020).\nTo summarize, the contributions of this paper are as follows: 1. We introduce a formal framework for representing control objectives for conditional generative models as conditional EBMs, 2. We design CDPG, an extension of DPG suitable for approximate conditional EBMs, 3. We evaluate CDPG on three control objectives across two tasks: summarization constrained to be factually correct, code generation constrained to generate compilable Python functions and code generation constrained to respect PEP8.\n\nMethod\nConditional EBMs A standard, \"unconditional\" EBM is a function P from a (discrete, i.e. finite or countable) space X to the non-negative reals, such that the partition function Z . = x\u2208X P (x) is strictly positive and finite. We will denote by lowercase p the normalized distribution over X associated with P , namely p(x) . = P (x)/Z. Let us now consider a discrete, potentially infinite set C of conditions c. Formally, P, a conditional EBM over C, is defined as a function from C to the set of unconditional EBMs over X, in other words, a function that maps each c \u2208 C to an unconditional EBM P c over X: (1) P c : x \u2192 R + . ( We denote by Z c the partition function of P c (x), namely: Z c . = x\u2208X P c (x), and by p c (x) the normalized version of P c (x), namely: p c (x|c) . = P c (x)/Z c .We will use the shortcut notation P(x|c) to denote P(c)(x): a conditional EBM P evaluated first at a given context c (resulting in an unconditional EBM P c ) and then at given x (resulting in a non-negative EBM score).\nRepresenting constraints as conditional EBMs The problem of fine-tuning a pretrained model a(x|c) to satisfy a control objective (e.g. generating factually correct summaries) can be seen as a constraint satisfaction problem: finding a model p(x|c) that meets the demands of the control objective but at the same time stays as close as possible to the original pretrained model a(x|c). We represent such an optimal model as a conditional EBM P (x, c). A control objective can be defined in terms of a binary scorer b(x, c) such that b(x, c) = 1 if a sample (c, x) satisfies a constraint given by a control objective (e.g. x is factually correct with respect to c) and b(x, c) = 0 otherwise. Let us consider a set of contexts c \u2208 C. For each c, there is a unique model p c (x) such that (i) b(x, c) = 1 for all samples x \u223c p c (x), and (ii) p c (x) has minimal KL divergence from a(\u00b7|c). Following our example, p c could be a distribution over factually correct summaries of c as similar as possible to a distribution over summaries the original model a would produce for a document c. In information-geometric terms, p c is an I-projection of a onto the manifold of all distributions satisfying a constraint given by b (Csisz\u00e1r & Shields, 2004). As shown by (Khalifa et al., 2021), p c can be represented as an unconditional EBM P c (x) of the following form: 3 Approximating conditional EBMs While P represents the target condtional model optimally reconciling distance from a(x|c) and the control objective, sampling or MAP decoding for p(x|c) is intractable for two reasons. First, p(x|c) actually represents a potentially infinite collection of unconditional models of the form p c (x). Second, each of these conditional models still cannot be easily sampled from because they do not admit an autoregressive factorisation: b(x, c) is only defined for the whole sequence x.\nThe second problem was addressed by Parshakova et al. (2019b) and Khalifa et al. (2021) who used the distributional policy gradients (DPG) algorithm to approximate unconditional EBMs p using a new unconditional model \u03c0 \u03b8 trained to minimize the cross-entropy CE(p, \u03c0 \u03b8 ). Unfortunately, DPG is no longer usable for conditional models that should satisfy infinitely many constraints represented by the set of conditional EBMs defined by P.\nTo address the second problem, we instead try to find a single model \u03c0 \u03b8 approximating p on average across contexts. Concretely, we minimize the expected cross-entropy between \u03c0 \u03b8 and multiple p c 's: where the expectation is over \u03c4 (c), a distribution over c \u2208 C. The gradient of this objective takes the following form: where in (7) we applied importance sampling from \u03c0 \u03b8 and (8) expresses p c in terms of unconditional EBM P c and its partition function Z c . 4 We approximate both expectations in (8) by sampling. Intuitively, this corresponds to building unconditional EBMs P c (\u00b7) on the fly for each c \u223c \u03c4 (c), computing the EBM \"score\" P c (x) for each sample from the conditional model x \u223c \u03c0 \u03b8 (\u00b7|x) and then using this score as a \"pseudoreward\" term P c (x)/Z c \u03c0 \u03b8 (x|c) in the policy gradient estimate.\n\nP(x|c)\n\u03c0 \u03b8 (x|c) \u2207 \u03b8 log \u03c0 \u03b8 (x|c) Output: \u03c0 \u03b8 Estimating Z c The one term in (8) that remains difficult to evaluate is the partition function Z c . For a single unconditional EBM, Khalifa et al. (2021) had no need to evaluate its partition function Z: it just scaled gradient estimates and therefore could be absorbed into the learning rate. In the conditional case, Z c varies with c. Therefore for each c, we compute Z c using a batch of M samples {x 1 , . . . , x j , . . . x M } from \u03c0 \u03b8 (x|c i ). Then, Z c is estimated using importance sampling by reweighting samples x j by their likelihood according to \u03c0 \u03b8 (\u00b7|c i ).\nTraining loop We train \u03c0 \u03b8 by stochastic gradient descent using the gradient estimate (8). At each epoch, we first sample N contexts c and then, for each c, M samples x from \u03c0 \u03b8 (x|c). We maintain a buffer B storing each (c i , x j ) pair along with its corresponding partition function Z ci . Finally, we shuffle B and iterate over it to perform gradient steps using (8) with learning rate \u03b1 (\u03b8) . The procedure is summarized in Algorithm 1. \u03b1 (\u03b8) , N and M are hyperparameters. Values used in experiments are reported in Tables 1-2 in the Appendix.\n\nExperiments\nWe evaluate CDPG as well as three baselines on three control objectives across two tasks: summarization and code generation. Each task is associated with C train , a set of contexts c used for prompting the model: these are Python function signatures in case of code generation and source documents in case of summarization. When computing evaluation metrics, we sample contexts from a held out set C test not used for training. In addition to that, for each experiment, we measure E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), the expected forward KL divergence from the optimal distribution p c , as well as E c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a), the expected reverse KL divergence from the original pretrained model. 5 .\n\nBaselines\nUnconditional DPG We compare our algorithm against the original DPG for unconditional EBMs developed by Parshakova et al. (2019a) and extended by Khalifa et al. (2021). This algorithm differs from conditional DPG: it ignores the partition function Z c in the denominator of (8) assuming it to be constant (which it is for unconditional EBMs but not for conditional ones). For a fair comparison, we use the on-policy variant (called \"DPG-on\" in (Parshakova et al., 2019b)) where we sample from \u03c0 \u03b8 as opposed to a different proposal distribution q.\nReinforcement learning The problem of fine-tuning a pretrained model to satisfy a pointwise constraint b(x, c) can be posed as maximising tbe expected reward E c\u223c\u03c4 (c) E x\u223c\u03c0 \u03b8 (x|c) R(x, c). We consider two instances of this approach: Reinforce (Williams, 1992) and Ziegler (Ziegler et al., 2019). For Reinforce, we simply define R(x, c) = b(x, c). Ziegler prevents too large departures from a by adding a KL penalty term and defining R(x, c) = b(x, c) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), where \u03b2 is a hyperparameter updated using an adaptive schedule.\n\nExperimental setup\nDataset To conduct our summarization experiments, we use the CNN/DailyMail dataset (Nallapati et al., 2016): we sampled 5k source documents from the train and test subsets of CNN/DailyMail to use for fine-tuning and evaluation, respectively. \u03c4 (c) from Algorithm 1 will be a uniform distribution over a given subset of source documents. Note that neither CDPG nor the baselines utilise ground truth summaries; we compute b(x, c) based on source documents and generated summaries. However, we use ground truth summaries for computing reference-based evaluation metrics such as Rouge score or recall-target.\nModel We conduct our experiments on the T5 architecture (Raffel et al., 2020), using the pretrained model t5-small (60m parameters) as \u03c0 \u03b8 . During fine-tuning, we generate summaries x conditioned on a source document c by pure ancestral sampling from \u03c0 \u03b8 ; for evaluation, we follow the setup described by Raffel et al. (2020) and use beam search.\nConstraints Following Nan et al. (2021), we define an entity-level factual consistency constraint as a product of two constraints: there must be at least four named entities in the summary x and all the named entities x must have occurred in the source c. More formally, let NER(\u00b7) denote the set of named entities found in a text. Then, b(x, c) Metrics In addition to measuring expected D KL (p c , \u03c0 \u03b8 ) and D KL (\u03c0 \u03b8 , a), we evaluate the quality and factual consistency of generated summaries using the following metrics: 1. Precision-source (Nan et al., 2021), defined as |NER(x \u2229 c)|/|NER(x)| is the percentage of named entities in the summary that can be found in the source. Low precision-source indicates severe hallucination, 2. Recall-target (Nan et al., 2021), defined as |NER(x \u2229 t)|/|NER(t)|, is the percentage of named entities in the target summary t that can be found in the generated summary x. Results We present the evolution of our 7 metrics through time on Figure 2. CDPG is the only method stably decreasing expected D KL (p c , \u03c0 \u03b8 ) and thus approaching (as opposed to drifting away from) optimal distributions p c . This is reflected in moderate divergence from a and translates into downstream metrics. Summaries generated by the fine-tuned model contain, on average, more named entities. Moreover, name entities in summaries are both more factually consistent with source : Summarization with factual consistency constraint. Evaluation metrics: expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), precision-source (\u2191 better), recall-target (\u2191 better), number of named entities (\u2191 better), Distinct-2 (\u2191 better), Rouge-1 (\u2191 better) for models obtained from fine-tuning with conditional DPG, DPG, Ziegler and Reinforce.\n(an increase in precision-source) and more relevant (an increase in recall-target). The tendency towards mentioning more factually consistent named entities increases the bigram diversity within summaries (Distinct-2) and the overall quality of generated summaries compared to ground truth (Rouge-1). This last results might seem surprising given that CDPG did not have access to ground truth summaries. A plausible explanation is that the original pretrained model was biased towards mentioning too few factually correct entities, at least compared to ground truth summaries. Satisfying the factual consistency constraint reduced this bias.\nBaseline approaches fall short of achieving similar results. Standard DPG, the closest contender, still leaves a significant gap in terms of all metrics and is far less stable than CDPG (e.g. its D KL (p c , \u03c0 \u03b8 ) starts to diverge again after around 500 epochs). Ziegler stays extremely close to the original model a, failing to improve its shortcomings. In contrast, Reinforce heavily departs from a pushing it to mention a large number of named entities. This results in artificially inflated recall-target but no increase in precision-source and a decrease in Rouge-1. The additional named entities frequently are frequently irrelevant (i.e. not mentioned in ground truth summaries) or simply hallucinated. See Tables 3-7 in the Appendix for randomly chosen summaries of documents in the test set.\n\nCode generation\nDataset For code generation experiments, we condition a language model on Python function signatures (both of methods and standalone functions) extracted from the Python150 dataset which consists of Python source code obtained from GitHub . We use the code provided by Roziere et al. (2020) for function extraction and randomly choose 5k functions for C train and 5k for C test . \u03c4 (c) is a uniform distribution over these signatures. Note that neither in fine-tuning nor in evaluation do we use ground truth function bodies.\n\nModel\nWe conduct experiments using GPT-Neo (Black et al., 2021): an off-the-shelf, freely available autoregressive language model mirroring the GPT-3 architecture (Brown et al., 2020). GPT-Neo's training set included 85 GiB of source code from GitHub which endowed it with some code completion abilities (Gao et al., 2020). We use the gpt-neo-125 variant available on Huggingface Transformers (Wolf et al., 2019). During both fine-tuning and evaluation we generate function bodies by conditioning on signatures using pure ancestral sampling.   : Code generation with compilability (a) and PEP8 (b) constraint. Evaluation metrics: compilability (\u2191 better), number of PEP8 errors (\u2193 better), expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), number of characters, AST node count (\u2191 better) for models obtained from fine-tuning with CDPG, DPG, Ziegler and Reinforce.\n\nConstraints\npycodestyle, an off-the-shelf linter (static code analysis tool) Metrics We evaluate the quality of generated Python functions using the following metrics: 1. PEP8 error count, the average number of violations of PEP8, 2. Compilability, the fraction of samples [c, x] that compile, 3. The average number of characters in [c, x] (after detokenization), 4. The average number of nodes in an abstract syntax tree (AST) of sequences that compile.\nIntuitively, this metric indicates the logical (as opposed to surface) complexity of generated programs.\nFor more details on how scorers b and metrics are implemented, see Appendix B.3.\n\nResults\nWe present the evolution of metrics through time on Figure 3. CDPG was able to increase the fraction of compilable functions from around 40% to around 65% and decrease the average number of PEP8 violations. Incidentally, the PEP8 control objective also leads to an increase in compilability because many PEP8 violations are also compilation errors. Similarly to the summarization experiment, CDPG and DPG are the only methods actually approaching optimal distributions p c and diverging moderately from a. This allows them to maintain the original statistics of a: length and the number of nodes in AST trees of generated functions. In contrast, Reinforce learns to generate shorter functions (having less opportunity for mistakes) and Ziegler produces heavily degenerated samples (Holtzman et al., 2020): syntactically simple functions with severe repetitions. This is reflected in an increase in length and a decrease in AST nodes count. See Tables 8-10 and Tables  11-13 for randomly chosen samples from the compilability and PEP8 experiments, respectively.\nNote that the gap between CDPG and DPG is much closer for code generation (especially with compilability control objective) than for summarization. This can be accounted for by the normalized standard deviation of partition functions Z c for EBMs P c in the range of conditional EBMs P for each control objective. For code generation, this standard deviation is lower meaning that Z c in (8) is better approximated by a constant which can be absorb into the learning rate \u03b1 (\u03b8) . For summarization, this variance is higher, therefore ignoring the Z c term incurs higher bias. See Appendix C.1 for a comparison.\n\nQualitative analysis\nIn the previous sections, we showed how CDPG is able to fine-tune a pretrained model a to satisfy certain constraints without destroying a's capabilities. Here we attempt to gain a better understanding of how different fine-tuning approaches affect the distributions of final models. On Figure 4 we present frequencies of errors and named entities obtained from fine-tuned models. While errors and named entities differ significantly in their frequency, CDPG consistently decreases frequencies of these errors and consistently increases the frequencies of all kinds of named entities, including the long tail of rare ones.\nTo compare lexical diversity of samples obtained from fine-tuned models, we plot the frequency of each token (the number of times it occurs) and its rank (its index in a sorted list of tokens) in Figure 5. CDPG and DPG are able to closely match original token frequencies while Ziegler and Reinforce tend to have shorter tails of rare tokens.\n\nConclusion\nWe presented CDPG, a principled approach to fine-tuning conditional language models to satisfy arbitrary constraints. In contrast with other methods, CDPG does not require ground truth training data and is able to shift model distribution in a minimally invasive way. In consequence, models fine-tuned with CDPG share desired characteristics, such as improved factual consistency or compilability, with the fluency and diversity of the original model.\nFuture work could evaluate CDPG on other tasks -such as machine translation or dialogue response generation -as well as explore other control objectives such as constraining the semantics (as opposed to syntax) of generated Python functions. Another future direction consists in extending CDPG to approximate conditional analogues of the more general, exponential-form (\n\nA Extended related work\nReducing hallucinations in summarization Neural abstractive summarization is highly prone to hallucinate content in the summary that is unfaithful to the source document. Maynez et al. (2020) found that hallucinations occur in more than 70% of single-sentence summaries and most of these are extrinsic hallucinations: adding information not directly inferable from the input document. Therefore, a substantial effort was devoted to to improving factual consistency of abstractive summarization. Some notable attempts include reranking summaries based on their correctness predicted by entailment classifiers (Falke et al., 2019) or fine-tuning using RL with a reward derived from an entailment classifier (Pasunuru & Bansal, 2018). The notion of entity-level factual consistency -a property such that that all named entities in the summary are actually mentioned in the source document -was introduced (Nan et al., 2021) as is one way of operationalizing the notion of extrinsic hallucinations.\nControllable code generation Generating source code is an established application of language models (Nguyen et al., 2013;Raychev et al., 2014;Karpathy et al., 2015;) that recently enjoys renewed interest (Lu et al., 2021;Chen et al., 2021;Austin et al., 2021). The task is formulated both as unconditional generation (with applications in code completion, e.g. Codex (Lu et al., 2021) or GitHub Copilot 7 ) and as conditional generation (e.g. program synthesis or generating a program satisfying a given input-output specification, e.g. (Austin et al., 2021)). Our task of function generation can be seen as a simplified program synthesis when the specification is given by a function signature (a name of a function and a list of arguments). Previous work found compilability errors to be a signification failure mode of neural code generation (Roziere et al., 2020). Previous attempts at improving compilability of generated code include Maddison & Tarlow (2014), who augment neural probabilistic context free grammars with semantic constraints and use them for unconditional generation or Zhong et al. (2017), who used policy gradients to train a model translating natural language questions to corresponding SQL queries and -in addition for rewarding for query execution results -added a penalty for syntactically invalid queries. Most in line with our work, Korbak et al. (2021) were using DPG to improve compilability of unconditional language models for code.\nB Details of metric and score calculation B.1 KL divergences Calculation of metrics relative to p c 's, such as E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), requires estimating Z c 's. This is done using importance sampling from \u03c0 \u03b8 in a manner analogous to the training loop (Algorithm 1). Then, expected KL can be simplified to the following form: A small is added to Z c for stability. We approximate both expectations (over \u03c4 and \u03c0 \u03b8 ) using importance sampling. For a complete procedure, see Algorithm 2.\nE c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a) is computed in a simpler manner as it doesn't require estimating Z c 's and we can directly sample from \u03c0 \u03b8 . It boils down to sampling a batch of N contexts c i , a batch of M samples x j from \u03c0 \u03b8 (x|c i ) for each c i and evaluating: To avoid bias, when computing KL divergences we always sample from \u03c0 \u03b8 using pure ancestral sampling (as opposed to top p sampling or beam search decoding).\n\nB.2 Summarization\nFollowing Nan et al. (2021), we implement NER(\u00b7) as using a pretrained SpaCy (Honnibal et al., 2020) named entity recognizer. We use the en_core_web_sm model and restrict the named entities we extract to the following categories: PERSON, FAC (buildings, airports, highways, bridges, etc.), GPE (geopolitical entities: countries, cities, etc.), ORG (companies, agencies, institutions, etc.), NORP (nationalities or religious or political groups), LOC (Non-GPE locations: mountain ranges, bodies of water, etc.), EVENT (named hurricanes, battles, wars, sports events, etc.). Also following Nan et al. (2021), we ignore entities such as date, time and numerals due to large variation in their representation in documents.\n\nB.3 Code generation\nCompilability To check for compilability, we call the compile_command function from the codeop module of Python Standard Library 8 with a sequence obtained by string concatenation [c, x] as argument. We then check if compile_command returns a code object. The only postprocessing we apply is removing any characters from x after the end of function declaration (with function end defined in terms of indentation) as we are concerned specifically with function generation. codeop.compile_command is the implementation that Python interactive interpreters use in read-eval-print loop (REPL) to determine whether a string is a valid Python code. The method tries to compile a string of Python code and raise and exception if compilation fails, for instance a SyntaxError for invalid Python syntax and ValueError or OverflowError if there is an invalid literal. Note that our notion of compilability is concerned only with syntactic correctness as Python interpreter does not execute the body of a function at function declaration time.\nPEP8 To compute the number of PEP8 violations triggered by a sequence [c, x], we run pycodestyle, 9 a Python linter (static code analysis tool) and report the number of violations it reports.\nAST node count Finally, to compute AST node count, the average number of nodes in an abstract syntax trees (ASTs) of generated functions, we consider only samples [c, x] that compile. They are parsed to their corresponding ASTs using the ast module from Python Standard Library. 10 Lower normalized standard deviation for a task explains closer performance gap between CDPG and DPG for that task on Code generation with PEP8 normalized std of Zc Figure 6: Normalized standard deviations for Zc's associated with unconditional EBMs Pc in the codomain of conditional EBMs P defined for three control objectives: summarization with factual consistency constraint, code generation with compilability constraint and code generation with PEP8 constraint.\ninvalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation error original model a Conditional DPG 10 0 10 1 10 2 10 3 error count DPG 10 0 10 1 10 2 10 3 error count invalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation\n\nD Hyperparmeters and implementation details\nWe implemented all models using PyTorch (Paszke et al., 2019) and HuggingFace Transformers (Wolf et al., 2019). Each training run took approximately 5 days on 2 Nvidia V100 GPUs. For a detailed list of hyperparameter values, see Table 1 and 2. For a description of hyperparameters specific to Ziegle see (Ziegler et al., 2019).  Tables 3-7 for summaries of 5 randomly chosen documents from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. Summaries were generated with beam search.\n\nE.2 Code generation with compilability constraint\nSee Tables 8-10 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\n\nE.3 Code generation with PEP8 constraint\nSee Tables 11-13 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\nSource document c (CNN)\"Success Kid\" is likely the Internet's most famous baby. You've seen him in dozens of memes, fist clenched in a determined look of persevering despite the odds. Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. His dad, Justin, needs a kidney transplant. About a week ago, Laney Griner, Justin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000 to help cover the medical expenses that go along with a kidney transplant. The campaign is already a success. By Wednesday it had topped its goal. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. \"One can only survive with no natural kidney function ... for so long,\" Laney Griner said. \"His energy and mood are affected; he can no longer work, and he spends 12 hours a week in dialysis clinic. \"Having been on dialysis for this long greatly increases his risks of developing further complications. The only way to save his life is to get a transplant. There's no other way around that,\" she said. The family doesn't know when a kidney might become available. Their GoFundMe page has a link for potential donors. Sammy's Internet fame began in 2007 when his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. Myspace picked it up, so did Reddit. The rest is Internet history. Success just seems to run in some families.\n\nb(x, c) x\nSummary generated by the original model a 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Conditional DPG 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. about a week ago, Laney Griner, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using DPG 0 \"Success Kid\" is the internet's most famous baby. his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. a goFundMe campaign has a goal of $75,000 to help cover medical expenses.\nSummary generated by a model fine-tuned using Ziegler 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Reinforce 1 Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. Laney Griner, textcolorredJustin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. The family doesn't know when a kidney might become available. Sammy's Table 3: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c A picture, believed to be the only image of the Civil War ironclad, the CSS Georgia has been revealed to be a fake created in a teenage hoax using a 2ft model. John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. Scroll down for video . John Potter (pictured), from Savannah, has admitted forging the picture with his brother in the 1980s and placing the picture in a frame which now holds a picture of his dead dog . In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted . The picture Mr Potter handed over to the society was actually a picture of a picture. In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame. Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted, so he took a photo of it and then mailed it to historical groups in Savannah. Potter has now admitted the sham and explained how he falsified the image. When he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about the buildingand destruction -of the CSS Georgia in a fictional battle with Union troops. For the movie, they built an 18-foot long boat of plywood and Styrofoam, as well as a smaller 2-foot model. They based the design, in part, on his grandfather's recollections of details passed down by word of mouth through generations of their family. Potter also used an illustration of the ironclad he found on a postcard. To create the fake image Potter's younger brother put on a coat and straw hat went out to a marsh with a cane fishing pole and Potter took a photo. He took another photo of the 2-foot model and cut out the boat's image, glued it onto the photo of his brother, then used dirt and glue to create the illusion of a photo faded by age and stained by water or chemicals. Evidence of the hoax: All elements of the fakery were snapped by Potter at one time. The small model boat here appears alongside the false picture, polaroids of it and a 1984 copy of Mad Magazine . He bought an old picture frame and beat it up even further. He put the photo in it. Then he drove 120 miles to a yard sale in Waycross, Georgia, put the picture down and took a Polaroid of it. He laughs now, when he remembers that it had seemed so important that he actually do this at a yard sale, so at least that part would be true. 'Who knows what goes through the mind of a kid,' he said. The US Army Corps of Engineers appealed for information about the picture as it embarked on a project to raise the ironclad . Potter sent out the photo to historical groups, setting off the sporadic, and fruitless, search for a CSS Georgia photo that he now says never existed. As the Army Corps of Engineers embarked this year on a $14 million project to raise the wreckage of the real CSS Georgia from the river, archaeologists publicised the image online and in news stories hoping to track down the original photo. 'Wanted: A Photo Of This Confederate Battleship,' read the headline above the image on the Army Corps website. 'There are a lot of characteristics in the photograph that lead us to believe it is the CSS Georgia,' Julie Morgan, the Army Corps archaeologist in charge of raising the ironclad's remains, said in a February interview. 'On the other side, there are some skeptics who believe it's a complete fake.' Robert Holcombe, former curator of the National Civil War Naval Museum in Columbus, Georgia, said in February that while the original photograph would be needed to confirm if the image was authentic, he believed it was real. 'Most people seem to think so,' he said. 'Or else it's an awfully good fake'. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug. He never became a successful special effects artist with the exception of the one faked photo. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug .\n\nb(x, c) x\nSummary generated by the original model a 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Conditional DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, Summary generated by a model fine-tuned using DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. the picture became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, was 'CSS Georgia' Summary generated by a model fine-tuned using Ziegler 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Reinforce 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He told historians that Mr Potter handed over to the Georgia Historical Society and the photo became an unofficial part of the ship's history.\nPotter has now admitted the sham and explained how he falsified the image when he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about Table 4: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienaar has been dogged by groin and knee injuries this season limiting him to just 11 appearances. He returned to action from his latest setback in the 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Everton midfielder Steven Pienaar is held back by Swansea's Ki Sung-Yueng at the Liberty Stadium . Pienaar said: 'At one stage, I thought I had better just hang my boots up and call it a day but on the other side I was just thinking that I enjoy going in and seeing the guys so I just had to stay strong and that kept me going. 'When you are at home not coming in for training you feel very down but as soon as I walk through the door, there's always the camaraderie in the group, there is always fun. 'Even if you are injured, you can always laugh and it keeps you going. Just to be among the players, it's kept me going.' Pienaar has made just 11 appearances for the Toffees this season due to groin and knee injuries .\n\nb(x, c) x\nSummary generated by the original model a 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Conditional DPG 0 Steven Pienaar has been dogged by groin and knee injuries this season. the 33-year-old midfielder has made just 11 appearances for the toffees. he returned to action from his latest setback in the 1-1 draw with Swansea city earlier this month.\nSummary generated by a model fine-tuned using DPG 0 the 33-year-old midfielder has been dogged by groin and knee injuries this season. he returned to action from his latest setback in the 1-1 draw with Swansea city. pienaar has made just 11 appearances for the toffees this season due to injury.\nSummary generated by a model fine-tuned using Ziegler 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Reinforce 0 Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienanar has been dogged by groin and knee injuries this season for the Toffees. He returned to action from his latest setback in a 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Pienar said Pien Table 5: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency. Named entities in summaries are highlighted in red. constraint Source document c Expat: Detectives are investigating the killing of David King (pictured), a 70-year-old pensioner from Newham, east London, who retired to Normandy . A fight over stolen vegetables may have led to a British expat being murdered and dumped at the bottom of a well, French police fear. The macabre theory has been outlined by detectives investigating the killing of David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed 28-year-old Frenchman has been charged with his murder. The alleged killer was living rough in the area, and is thought to have been behind a number of thefts in the area. Now prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over stolen vegetables. Ms Etienne, who is leading the police enquiry, said: 'Among the vague attempts at an explanation given by the accused, we are looking at the possibility of a fight over the theft of foodstuffs. 'In one night, a whole plot of leeks might disappear. One person saw four rabbits vanishing overnight.' Ms Etienne added: 'People were getting more and more angry, and some were even threatening to defend their plots with shotguns'. Neighbours of Mr King, who was hugely proud of his vegetable garden, told Le Parisien newspaper that a description of the thief corresponds to the alleged murderer. Discovery: In February, Mr King's car, a Renault Scenic (above) was found parked in Vire, a nearby town, but there was no sign of any body . Family and neighbours of Mr King have attacked detectives for allowing his suspected murderer to remain at large for six months. Interpol, the international police organisation, had initially refused to open a missing person's enquiry. Instead they believed Mr King had travelled to Australia to see his daughter, Sandie Ray. Mr King had been living in France for 15 years . Ms Ray, who lives in Perth, Australia, said her father's passport details had been mixed up with another David King, who had indeed travelled to Australia from France. She said this was known by November last year, but 'the French authorities still hadn't been formally notified of this via Interpol until approximately three months later'. Ms Ray said a quicker enquiry would have avoided 'lots of anxiety and frustration for our family and dads friends.' John King, Mr King's son, who lives in Brighton, East Sussex, said the botched investigation had been a 'bureaucratic nightmare' for all concerned. Other expats living in the area said it was 'hugely frightening' to have a suspected murderer living in their midst while the operation went on. 'This is an isolated part of the world, and everyone is potentially vulnerable to attack,' said one. 'Detectives should have worked out what was going on far quicker. The slow speed of the enquiry was unacceptable.' In February, Mr King's car, a Renault Scenic was found parked in Vire, a nearby town, but there was no sign of any body.\n\nb(x, c) x\nSummary generated by the original model a 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Conditional DPG 1 the macabre theory has been outlined by detectives investigating the killing of David King, 70, from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year-old frenchman has been charged with his murder. prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over over stolen vegetables Summary generated by a model fine-tuned using DPG 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of caen, and an unnamed 28-year old frenchman has been charged with his murder. prosecutor Carole Etienne has said that the alleged killer may well have confronted the man over stolen vegetables Summary generated by a model fine-tuned using Ziegler 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Reinforce 1 David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed Frenchman has been charged with his murder. prosecutor Carole Etienne has said Mr King, keen gardener, may well have confronted the man over stolen vegetables, Neighbours of Mr King. Interpol, international police organisation Table 6: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c German legend Franz Beckenbauer has accused Bayern Munich stars of playing as if they had taken 'sleeping pills' in their midweek defeat by Porto. The Bundesliga champions conceded twice in the opening 10minutes before losing 3-1 to the Portuguese in the first leg of their Champions League quarter-final on Wednesday. Der Kaiser may be a brand ambassador for the club but he couldn't hide his feelings after the game, when he criticised defender Dante for playing as if he were wearing 'ski boots' before turning on the entire team. Franz Beckenbauer was speaking in New York as part of Bayern Munich's media agreement with MSN . New York Cosmos legends Beckenbauer and Pele pose together after launching the team's spring season . Ricardo Quaresma scored the opening goal from the spot in Porto's 3-1 defeat of the Germans . Beckenbauer accused Brazilian defender Dante (left) of playing as if he were wearing 'ski boots' Speaking to reporters in New York to mark the club's new media agreement with MSN, Beckenbauer said: 'It was one of those days, all the players didn't show their real performance. 'After 10 minutes you are 2-0 down in the quarter-final in the Champions League, so many mistakes. I never saw this before. I thought they took sleeping pills! Porto were much faster, real power. It made me angry. Quaresma celebrates with Porto team-mates after netting his second goal inside the first 10minutes . Bayern Munich head coach Pep Guardiola goes crazy at his team's performance . Bayern players trudge off after defeat knowing they will have to vastly improve to reach the semi-finals . 'If they played like that again, no chance [they can go through]. But they can win 2-0 or go to extra-time. They have a chance. Last year we had the same problem, playing excellent in the Bundesliga and then we had two bad days and were out of the competition.' The 69-year-old did attempt to play down his comments over Dante's performance, he said: 'At half-time, I have to give a comment and that was my reaction. And I'm sorry to give him the ski boots... but he's Brazilian, and Brazilian to me means for me, technique and Pele and... Brazil. But not this.' 'If you want to win the Champions League you need a strong league with a lot of players. Internationally, Bayern Munich belongs to the top teams but to win the Champions League. . . you saw the game probably two days ago, we are far, far away. With this performance, you don't win anything,' he added.\n\nb(x, c) x\nSummary generated by the original model a 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.\nSummary generated by a model fine-tuned using Conditional DPG 1 german legend Franz Beckenbauer was speaking in new york as part of the club's media agreement with MSN. he accused defender Dante of playing as if he were wearing'ski boots' the 69-year-old said: 'if they played like that again, no chance [they can go through] but they can win 2-0 or go to extra-time' Summary generated by a model fine-tuned using DPG 0 the germans conceded twice in the opening 10 minutes before losing 3-1 to porto. the 69-year-old criticised defender defender Dante for playing as if he were wearing'ski boots' he said: 'if they played like that again, no chance [they can go through]' Summary generated by a model fine-tuned using Ziegler 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.\n\nSuppose someone is trying to remember this paper but only remembered parts of it. What parts do you think would be salient, and what queries would the user put into the search engine? What about asking a colleague? Would they ask different queries to a human than a search engine? \n\nList your answer in two sections:\n<search_engine></search_engine> for search engine queries\n<human></human> for queries to a human colleague\n",
                "role": "user"
            }
        ],
        "paper": {
            "added": "2021-12-03T02:15:30.823Z",
            "created": "2021-01-01T00:00:00.000Z",
            "id": "244798746",
            "metadata": {
                "abstract": null,
                "abstract_count": null,
                "abstract_language": null,
                "abstract_perplexity": null,
                "extfieldsofstudy": [
                    "Computer Science"
                ],
                "provenance": "pes2o_v2-0018.json.gz:169596",
                "s2fieldsofstudy": [
                    "Computer Science"
                ],
                "sha1": "4b70f356f50d3e8e1f72c4a10f0ce2a26da95b5a",
                "sources": null,
                "title": null,
                "title_count": null,
                "title_language": null,
                "title_perplexity": null,
                "top_frequencies": null,
                "year": 2021
            },
            "source": "s2",
            "text": "Controlling Conditional Language Models with Distributional Policy Gradients\n\nMachine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g. hallucination in abstractive summarization or wrong format in automatic code generation). This raises an important question on how to adapt pre-trained generative models to a new task without destroying its capabilities. Recent work has suggested to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Unfortunately, this approach is limited to unconditional distributions, represented by unconditional EBMs. In this paper, we extend this approach to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on three different control objectives across two tasks: summarization with T5 and code generation with GPT-Neo. Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.\n\n\nIntroduction\nPretrained generative models are shifting the landscape of machine learning research and practice. General purpose models such as the GPT family (Radford et al., 2019;Brown et al., 2020;Black et al., 2021), T5 (Raffel et al., 2020), CLIP (Radford et al., 2021) and Codex (Chen et al., 2021) are trained in a self-supervised manner on large amounts of uncurated data and can then be adapted to specific downstream tasks (e.g. generating Python code) or control objectives (e.g. controlling the style of generated code). Frequently, control objectives are motivated by the desire to address the shortcomings of certain pretrained models. These can be due to the uncurated nature of the original training data (e.g. a large portion of Python source code on the Internet violates PEP8 (van Rossum et al., 2001), the Python Style Guide) or the difficulty of learning a desired behaviour by purely selfsupervised traning (e.g. there is not enough training signal to ensure that a model trained on source code always generates compilable code or that a summarization model always produces factually correct summaries). The practice of adapting and controlling pretrained generative models poses two open problems. The first is that control objectives frequently lack ground truth data that could be used for supervised finetuning; in general, we are only given an indicator b(x) of whether a given sample from the model x satisfies a given control objective. Thus, approaches such as reinforcement learning (RL) (Pasunuru & Bansal, 2018;Ziegler et al., 2019), weighted decoding (Ghazvininejad et al., 2017;Holtzman et al., 2018;See et al., 2019) or decoding with perturbed activations (Dathathri et al., 2020) need to be employed. The second problem is catastrophic forgetting: most approaches to enforcing a control objective result in a dramatic loss of capabilities of the original model beyond the scope of the control objective. Notably, there exists one approach largely avoiding both of these problems (Parshakova et al., 2019a;Khalifa et al., 2021): representing the control objective as an energy-based models (EBM) and approximating that EBM using distributional policy gradients (DPG).\nAlthough this approach shows great improvements in controlling pretrained language models while avoiding catastrophic forgetting (Khalifa et al., 2021), it is limited to unconditional generation tasks and is not able to fine-tune conditional models that are behind the most impactful NLP tasks such as machine translation, summarization or dialogue systems.\nIn this paper, we present Conditional DPG (CDPG), an extension of DPG that can approximate conditional EBMs. A conditional EBM P defines an unnormalized distribution for each context c (e.g. a source document). Extending the approach of Khalifa et al. (2021) we build a conditional EBM that represents the ideal behaviour of a generative model given context c as the distribution that incorporates the control objectives while remaining as close as possible to the original distribution. Then, to approximate conditionals EBMs we build unconditional EBMs P c on the fly for any given context and estimate their normalizing constants Z c . = x P c (x) by importance sampling.\nWe demonstrate the effectiveness of CDPG in addressing shortcomings of pretrained generative models by considering two tasks, summarization and code generation, and two corresponding pretrained generative models: T5 (Raffel et al., 2020) and GPT-Neo (Black et al., 2021). An open problem in summarization is ensuring that summaries are factually faithful to source documents given that summarization models are prone to hallucinating named entities never mentioned in the source (Maynez et al., 2020). 2 We show that a preference for factually faithful summaries (operationalized as entity-level factual consistency (Nan et al., 2021)) can be represented by a conditional EBM. Then, we show that using CDPG to fine-tune T5 to approximate this EBM increases the number of correct and relevant named entities in summaries and improves T5's Rouge score. In contrast with RL approaches, CDPG does not degrade the diversity and quality of summaries. For code generation, we consider the task of generating a Python function given its signature (name and arguments). While general-purpose language models can generate idiomatic Python functions (Chen et al., 2021;Austin et al., 2021), they still may struggle to learn some desirable properties of generated code. For instance, a Python function generated by GPT-Neo will compile only 40% of the time and will contain on average 4 violations of PEP8. We show that using CDPG to approximate a conditional EBM expressing corresponding constraints improves both compilability and PEP8compliance without hurting the diversity of summaries or leading to degeneration (Holtzman et al., 2020).\nTo summarize, the contributions of this paper are as follows: 1. We introduce a formal framework for representing control objectives for conditional generative models as conditional EBMs, 2. We design CDPG, an extension of DPG suitable for approximate conditional EBMs, 3. We evaluate CDPG on three control objectives across two tasks: summarization constrained to be factually correct, code generation constrained to generate compilable Python functions and code generation constrained to respect PEP8.\n\nMethod\nConditional EBMs A standard, \"unconditional\" EBM is a function P from a (discrete, i.e. finite or countable) space X to the non-negative reals, such that the partition function Z . = x\u2208X P (x) is strictly positive and finite. We will denote by lowercase p the normalized distribution over X associated with P , namely p(x) . = P (x)/Z. Let us now consider a discrete, potentially infinite set C of conditions c. Formally, P, a conditional EBM over C, is defined as a function from C to the set of unconditional EBMs over X, in other words, a function that maps each c \u2208 C to an unconditional EBM P c over X: (1) P c : x \u2192 R + . ( We denote by Z c the partition function of P c (x), namely: Z c . = x\u2208X P c (x), and by p c (x) the normalized version of P c (x), namely: p c (x|c) . = P c (x)/Z c .We will use the shortcut notation P(x|c) to denote P(c)(x): a conditional EBM P evaluated first at a given context c (resulting in an unconditional EBM P c ) and then at given x (resulting in a non-negative EBM score).\nRepresenting constraints as conditional EBMs The problem of fine-tuning a pretrained model a(x|c) to satisfy a control objective (e.g. generating factually correct summaries) can be seen as a constraint satisfaction problem: finding a model p(x|c) that meets the demands of the control objective but at the same time stays as close as possible to the original pretrained model a(x|c). We represent such an optimal model as a conditional EBM P (x, c). A control objective can be defined in terms of a binary scorer b(x, c) such that b(x, c) = 1 if a sample (c, x) satisfies a constraint given by a control objective (e.g. x is factually correct with respect to c) and b(x, c) = 0 otherwise. Let us consider a set of contexts c \u2208 C. For each c, there is a unique model p c (x) such that (i) b(x, c) = 1 for all samples x \u223c p c (x), and (ii) p c (x) has minimal KL divergence from a(\u00b7|c). Following our example, p c could be a distribution over factually correct summaries of c as similar as possible to a distribution over summaries the original model a would produce for a document c. In information-geometric terms, p c is an I-projection of a onto the manifold of all distributions satisfying a constraint given by b (Csisz\u00e1r & Shields, 2004). As shown by (Khalifa et al., 2021), p c can be represented as an unconditional EBM P c (x) of the following form: 3 Approximating conditional EBMs While P represents the target condtional model optimally reconciling distance from a(x|c) and the control objective, sampling or MAP decoding for p(x|c) is intractable for two reasons. First, p(x|c) actually represents a potentially infinite collection of unconditional models of the form p c (x). Second, each of these conditional models still cannot be easily sampled from because they do not admit an autoregressive factorisation: b(x, c) is only defined for the whole sequence x.\nThe second problem was addressed by Parshakova et al. (2019b) and Khalifa et al. (2021) who used the distributional policy gradients (DPG) algorithm to approximate unconditional EBMs p using a new unconditional model \u03c0 \u03b8 trained to minimize the cross-entropy CE(p, \u03c0 \u03b8 ). Unfortunately, DPG is no longer usable for conditional models that should satisfy infinitely many constraints represented by the set of conditional EBMs defined by P.\nTo address the second problem, we instead try to find a single model \u03c0 \u03b8 approximating p on average across contexts. Concretely, we minimize the expected cross-entropy between \u03c0 \u03b8 and multiple p c 's: where the expectation is over \u03c4 (c), a distribution over c \u2208 C. The gradient of this objective takes the following form: where in (7) we applied importance sampling from \u03c0 \u03b8 and (8) expresses p c in terms of unconditional EBM P c and its partition function Z c . 4 We approximate both expectations in (8) by sampling. Intuitively, this corresponds to building unconditional EBMs P c (\u00b7) on the fly for each c \u223c \u03c4 (c), computing the EBM \"score\" P c (x) for each sample from the conditional model x \u223c \u03c0 \u03b8 (\u00b7|x) and then using this score as a \"pseudoreward\" term P c (x)/Z c \u03c0 \u03b8 (x|c) in the policy gradient estimate.\n\nP(x|c)\n\u03c0 \u03b8 (x|c) \u2207 \u03b8 log \u03c0 \u03b8 (x|c) Output: \u03c0 \u03b8 Estimating Z c The one term in (8) that remains difficult to evaluate is the partition function Z c . For a single unconditional EBM, Khalifa et al. (2021) had no need to evaluate its partition function Z: it just scaled gradient estimates and therefore could be absorbed into the learning rate. In the conditional case, Z c varies with c. Therefore for each c, we compute Z c using a batch of M samples {x 1 , . . . , x j , . . . x M } from \u03c0 \u03b8 (x|c i ). Then, Z c is estimated using importance sampling by reweighting samples x j by their likelihood according to \u03c0 \u03b8 (\u00b7|c i ).\nTraining loop We train \u03c0 \u03b8 by stochastic gradient descent using the gradient estimate (8). At each epoch, we first sample N contexts c and then, for each c, M samples x from \u03c0 \u03b8 (x|c). We maintain a buffer B storing each (c i , x j ) pair along with its corresponding partition function Z ci . Finally, we shuffle B and iterate over it to perform gradient steps using (8) with learning rate \u03b1 (\u03b8) . The procedure is summarized in Algorithm 1. \u03b1 (\u03b8) , N and M are hyperparameters. Values used in experiments are reported in Tables 1-2 in the Appendix.\n\nExperiments\nWe evaluate CDPG as well as three baselines on three control objectives across two tasks: summarization and code generation. Each task is associated with C train , a set of contexts c used for prompting the model: these are Python function signatures in case of code generation and source documents in case of summarization. When computing evaluation metrics, we sample contexts from a held out set C test not used for training. In addition to that, for each experiment, we measure E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), the expected forward KL divergence from the optimal distribution p c , as well as E c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a), the expected reverse KL divergence from the original pretrained model. 5 .\n\nBaselines\nUnconditional DPG We compare our algorithm against the original DPG for unconditional EBMs developed by Parshakova et al. (2019a) and extended by Khalifa et al. (2021). This algorithm differs from conditional DPG: it ignores the partition function Z c in the denominator of (8) assuming it to be constant (which it is for unconditional EBMs but not for conditional ones). For a fair comparison, we use the on-policy variant (called \"DPG-on\" in (Parshakova et al., 2019b)) where we sample from \u03c0 \u03b8 as opposed to a different proposal distribution q.\nReinforcement learning The problem of fine-tuning a pretrained model to satisfy a pointwise constraint b(x, c) can be posed as maximising tbe expected reward E c\u223c\u03c4 (c) E x\u223c\u03c0 \u03b8 (x|c) R(x, c). We consider two instances of this approach: Reinforce (Williams, 1992) and Ziegler (Ziegler et al., 2019). For Reinforce, we simply define R(x, c) = b(x, c). Ziegler prevents too large departures from a by adding a KL penalty term and defining R(x, c) = b(x, c) \u2212 \u03b2D KL (\u03c0 \u03b8 , a), where \u03b2 is a hyperparameter updated using an adaptive schedule.\n\nExperimental setup\nDataset To conduct our summarization experiments, we use the CNN/DailyMail dataset (Nallapati et al., 2016): we sampled 5k source documents from the train and test subsets of CNN/DailyMail to use for fine-tuning and evaluation, respectively. \u03c4 (c) from Algorithm 1 will be a uniform distribution over a given subset of source documents. Note that neither CDPG nor the baselines utilise ground truth summaries; we compute b(x, c) based on source documents and generated summaries. However, we use ground truth summaries for computing reference-based evaluation metrics such as Rouge score or recall-target.\nModel We conduct our experiments on the T5 architecture (Raffel et al., 2020), using the pretrained model t5-small (60m parameters) as \u03c0 \u03b8 . During fine-tuning, we generate summaries x conditioned on a source document c by pure ancestral sampling from \u03c0 \u03b8 ; for evaluation, we follow the setup described by Raffel et al. (2020) and use beam search.\nConstraints Following Nan et al. (2021), we define an entity-level factual consistency constraint as a product of two constraints: there must be at least four named entities in the summary x and all the named entities x must have occurred in the source c. More formally, let NER(\u00b7) denote the set of named entities found in a text. Then, b(x, c) Metrics In addition to measuring expected D KL (p c , \u03c0 \u03b8 ) and D KL (\u03c0 \u03b8 , a), we evaluate the quality and factual consistency of generated summaries using the following metrics: 1. Precision-source (Nan et al., 2021), defined as |NER(x \u2229 c)|/|NER(x)| is the percentage of named entities in the summary that can be found in the source. Low precision-source indicates severe hallucination, 2. Recall-target (Nan et al., 2021), defined as |NER(x \u2229 t)|/|NER(t)|, is the percentage of named entities in the target summary t that can be found in the generated summary x. Results We present the evolution of our 7 metrics through time on Figure 2. CDPG is the only method stably decreasing expected D KL (p c , \u03c0 \u03b8 ) and thus approaching (as opposed to drifting away from) optimal distributions p c . This is reflected in moderate divergence from a and translates into downstream metrics. Summaries generated by the fine-tuned model contain, on average, more named entities. Moreover, name entities in summaries are both more factually consistent with source : Summarization with factual consistency constraint. Evaluation metrics: expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), precision-source (\u2191 better), recall-target (\u2191 better), number of named entities (\u2191 better), Distinct-2 (\u2191 better), Rouge-1 (\u2191 better) for models obtained from fine-tuning with conditional DPG, DPG, Ziegler and Reinforce.\n(an increase in precision-source) and more relevant (an increase in recall-target). The tendency towards mentioning more factually consistent named entities increases the bigram diversity within summaries (Distinct-2) and the overall quality of generated summaries compared to ground truth (Rouge-1). This last results might seem surprising given that CDPG did not have access to ground truth summaries. A plausible explanation is that the original pretrained model was biased towards mentioning too few factually correct entities, at least compared to ground truth summaries. Satisfying the factual consistency constraint reduced this bias.\nBaseline approaches fall short of achieving similar results. Standard DPG, the closest contender, still leaves a significant gap in terms of all metrics and is far less stable than CDPG (e.g. its D KL (p c , \u03c0 \u03b8 ) starts to diverge again after around 500 epochs). Ziegler stays extremely close to the original model a, failing to improve its shortcomings. In contrast, Reinforce heavily departs from a pushing it to mention a large number of named entities. This results in artificially inflated recall-target but no increase in precision-source and a decrease in Rouge-1. The additional named entities frequently are frequently irrelevant (i.e. not mentioned in ground truth summaries) or simply hallucinated. See Tables 3-7 in the Appendix for randomly chosen summaries of documents in the test set.\n\nCode generation\nDataset For code generation experiments, we condition a language model on Python function signatures (both of methods and standalone functions) extracted from the Python150 dataset which consists of Python source code obtained from GitHub . We use the code provided by Roziere et al. (2020) for function extraction and randomly choose 5k functions for C train and 5k for C test . \u03c4 (c) is a uniform distribution over these signatures. Note that neither in fine-tuning nor in evaluation do we use ground truth function bodies.\n\nModel\nWe conduct experiments using GPT-Neo (Black et al., 2021): an off-the-shelf, freely available autoregressive language model mirroring the GPT-3 architecture (Brown et al., 2020). GPT-Neo's training set included 85 GiB of source code from GitHub which endowed it with some code completion abilities (Gao et al., 2020). We use the gpt-neo-125 variant available on Huggingface Transformers (Wolf et al., 2019). During both fine-tuning and evaluation we generate function bodies by conditioning on signatures using pure ancestral sampling.   : Code generation with compilability (a) and PEP8 (b) constraint. Evaluation metrics: compilability (\u2191 better), number of PEP8 errors (\u2193 better), expected DKL(pc, \u03c0 \u03b8 ) (\u2193 better) and DKL(\u03c0 \u03b8 , a) (\u2193 better), number of characters, AST node count (\u2191 better) for models obtained from fine-tuning with CDPG, DPG, Ziegler and Reinforce.\n\nConstraints\npycodestyle, an off-the-shelf linter (static code analysis tool) Metrics We evaluate the quality of generated Python functions using the following metrics: 1. PEP8 error count, the average number of violations of PEP8, 2. Compilability, the fraction of samples [c, x] that compile, 3. The average number of characters in [c, x] (after detokenization), 4. The average number of nodes in an abstract syntax tree (AST) of sequences that compile.\nIntuitively, this metric indicates the logical (as opposed to surface) complexity of generated programs.\nFor more details on how scorers b and metrics are implemented, see Appendix B.3.\n\nResults\nWe present the evolution of metrics through time on Figure 3. CDPG was able to increase the fraction of compilable functions from around 40% to around 65% and decrease the average number of PEP8 violations. Incidentally, the PEP8 control objective also leads to an increase in compilability because many PEP8 violations are also compilation errors. Similarly to the summarization experiment, CDPG and DPG are the only methods actually approaching optimal distributions p c and diverging moderately from a. This allows them to maintain the original statistics of a: length and the number of nodes in AST trees of generated functions. In contrast, Reinforce learns to generate shorter functions (having less opportunity for mistakes) and Ziegler produces heavily degenerated samples (Holtzman et al., 2020): syntactically simple functions with severe repetitions. This is reflected in an increase in length and a decrease in AST nodes count. See Tables 8-10 and Tables  11-13 for randomly chosen samples from the compilability and PEP8 experiments, respectively.\nNote that the gap between CDPG and DPG is much closer for code generation (especially with compilability control objective) than for summarization. This can be accounted for by the normalized standard deviation of partition functions Z c for EBMs P c in the range of conditional EBMs P for each control objective. For code generation, this standard deviation is lower meaning that Z c in (8) is better approximated by a constant which can be absorb into the learning rate \u03b1 (\u03b8) . For summarization, this variance is higher, therefore ignoring the Z c term incurs higher bias. See Appendix C.1 for a comparison.\n\nQualitative analysis\nIn the previous sections, we showed how CDPG is able to fine-tune a pretrained model a to satisfy certain constraints without destroying a's capabilities. Here we attempt to gain a better understanding of how different fine-tuning approaches affect the distributions of final models. On Figure 4 we present frequencies of errors and named entities obtained from fine-tuned models. While errors and named entities differ significantly in their frequency, CDPG consistently decreases frequencies of these errors and consistently increases the frequencies of all kinds of named entities, including the long tail of rare ones.\nTo compare lexical diversity of samples obtained from fine-tuned models, we plot the frequency of each token (the number of times it occurs) and its rank (its index in a sorted list of tokens) in Figure 5. CDPG and DPG are able to closely match original token frequencies while Ziegler and Reinforce tend to have shorter tails of rare tokens.\n\nConclusion\nWe presented CDPG, a principled approach to fine-tuning conditional language models to satisfy arbitrary constraints. In contrast with other methods, CDPG does not require ground truth training data and is able to shift model distribution in a minimally invasive way. In consequence, models fine-tuned with CDPG share desired characteristics, such as improved factual consistency or compilability, with the fluency and diversity of the original model.\nFuture work could evaluate CDPG on other tasks -such as machine translation or dialogue response generation -as well as explore other control objectives such as constraining the semantics (as opposed to syntax) of generated Python functions. Another future direction consists in extending CDPG to approximate conditional analogues of the more general, exponential-form (\n\nA Extended related work\nReducing hallucinations in summarization Neural abstractive summarization is highly prone to hallucinate content in the summary that is unfaithful to the source document. Maynez et al. (2020) found that hallucinations occur in more than 70% of single-sentence summaries and most of these are extrinsic hallucinations: adding information not directly inferable from the input document. Therefore, a substantial effort was devoted to to improving factual consistency of abstractive summarization. Some notable attempts include reranking summaries based on their correctness predicted by entailment classifiers (Falke et al., 2019) or fine-tuning using RL with a reward derived from an entailment classifier (Pasunuru & Bansal, 2018). The notion of entity-level factual consistency -a property such that that all named entities in the summary are actually mentioned in the source document -was introduced (Nan et al., 2021) as is one way of operationalizing the notion of extrinsic hallucinations.\nControllable code generation Generating source code is an established application of language models (Nguyen et al., 2013;Raychev et al., 2014;Karpathy et al., 2015;) that recently enjoys renewed interest (Lu et al., 2021;Chen et al., 2021;Austin et al., 2021). The task is formulated both as unconditional generation (with applications in code completion, e.g. Codex (Lu et al., 2021) or GitHub Copilot 7 ) and as conditional generation (e.g. program synthesis or generating a program satisfying a given input-output specification, e.g. (Austin et al., 2021)). Our task of function generation can be seen as a simplified program synthesis when the specification is given by a function signature (a name of a function and a list of arguments). Previous work found compilability errors to be a signification failure mode of neural code generation (Roziere et al., 2020). Previous attempts at improving compilability of generated code include Maddison & Tarlow (2014), who augment neural probabilistic context free grammars with semantic constraints and use them for unconditional generation or Zhong et al. (2017), who used policy gradients to train a model translating natural language questions to corresponding SQL queries and -in addition for rewarding for query execution results -added a penalty for syntactically invalid queries. Most in line with our work, Korbak et al. (2021) were using DPG to improve compilability of unconditional language models for code.\nB Details of metric and score calculation B.1 KL divergences Calculation of metrics relative to p c 's, such as E c\u223c\u03c4 (c) D KL (p c , \u03c0 \u03b8 ), requires estimating Z c 's. This is done using importance sampling from \u03c0 \u03b8 in a manner analogous to the training loop (Algorithm 1). Then, expected KL can be simplified to the following form: A small is added to Z c for stability. We approximate both expectations (over \u03c4 and \u03c0 \u03b8 ) using importance sampling. For a complete procedure, see Algorithm 2.\nE c\u223c\u03c4 (c) D KL (\u03c0 \u03b8 , a) is computed in a simpler manner as it doesn't require estimating Z c 's and we can directly sample from \u03c0 \u03b8 . It boils down to sampling a batch of N contexts c i , a batch of M samples x j from \u03c0 \u03b8 (x|c i ) for each c i and evaluating: To avoid bias, when computing KL divergences we always sample from \u03c0 \u03b8 using pure ancestral sampling (as opposed to top p sampling or beam search decoding).\n\nB.2 Summarization\nFollowing Nan et al. (2021), we implement NER(\u00b7) as using a pretrained SpaCy (Honnibal et al., 2020) named entity recognizer. We use the en_core_web_sm model and restrict the named entities we extract to the following categories: PERSON, FAC (buildings, airports, highways, bridges, etc.), GPE (geopolitical entities: countries, cities, etc.), ORG (companies, agencies, institutions, etc.), NORP (nationalities or religious or political groups), LOC (Non-GPE locations: mountain ranges, bodies of water, etc.), EVENT (named hurricanes, battles, wars, sports events, etc.). Also following Nan et al. (2021), we ignore entities such as date, time and numerals due to large variation in their representation in documents.\n\nB.3 Code generation\nCompilability To check for compilability, we call the compile_command function from the codeop module of Python Standard Library 8 with a sequence obtained by string concatenation [c, x] as argument. We then check if compile_command returns a code object. The only postprocessing we apply is removing any characters from x after the end of function declaration (with function end defined in terms of indentation) as we are concerned specifically with function generation. codeop.compile_command is the implementation that Python interactive interpreters use in read-eval-print loop (REPL) to determine whether a string is a valid Python code. The method tries to compile a string of Python code and raise and exception if compilation fails, for instance a SyntaxError for invalid Python syntax and ValueError or OverflowError if there is an invalid literal. Note that our notion of compilability is concerned only with syntactic correctness as Python interpreter does not execute the body of a function at function declaration time.\nPEP8 To compute the number of PEP8 violations triggered by a sequence [c, x], we run pycodestyle, 9 a Python linter (static code analysis tool) and report the number of violations it reports.\nAST node count Finally, to compute AST node count, the average number of nodes in an abstract syntax trees (ASTs) of generated functions, we consider only samples [c, x] that compile. They are parsed to their corresponding ASTs using the ast module from Python Standard Library. 10 Lower normalized standard deviation for a task explains closer performance gap between CDPG and DPG for that task on Code generation with PEP8 normalized std of Zc Figure 6: Normalized standard deviations for Zc's associated with unconditional EBMs Pc in the codomain of conditional EBMs P defined for three control objectives: summarization with factual consistency constraint, code generation with compilability constraint and code generation with PEP8 constraint.\ninvalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation error original model a Conditional DPG 10 0 10 1 10 2 10 3 error count DPG 10 0 10 1 10 2 10 3 error count invalid syntax unexpected EOF while parsing EOF while scanning triple-quoted string literal unindent does not match any outer indentation level EOL while scanning string literal unexpected indent Missing parentheses in call to 'print' invalid character in identifier keyword can't be an expression unexpected character after line continuation character positional argument follows keyword argument can't assign to operator can't assign to function call invalid token can't assign to literal expected an indented block from __future__ imports must occur at the beginning of the file 'break' outside loop keyword argument repeated illegal target for annotation\n\nD Hyperparmeters and implementation details\nWe implemented all models using PyTorch (Paszke et al., 2019) and HuggingFace Transformers (Wolf et al., 2019). Each training run took approximately 5 days on 2 Nvidia V100 GPUs. For a detailed list of hyperparameter values, see Table 1 and 2. For a description of hyperparameters specific to Ziegle see (Ziegler et al., 2019).  Tables 3-7 for summaries of 5 randomly chosen documents from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. Summaries were generated with beam search.\n\nE.2 Code generation with compilability constraint\nSee Tables 8-10 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\n\nE.3 Code generation with PEP8 constraint\nSee Tables 11-13 for functions obtained by sampling conditioned on 3 randomly chosen signatures from our evaluation set C test generated by models fine-tuned using Conditional DPG, 3 baselines and a. We used pure ancestral sampling.\nSource document c (CNN)\"Success Kid\" is likely the Internet's most famous baby. You've seen him in dozens of memes, fist clenched in a determined look of persevering despite the odds. Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. His dad, Justin, needs a kidney transplant. About a week ago, Laney Griner, Justin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000 to help cover the medical expenses that go along with a kidney transplant. The campaign is already a success. By Wednesday it had topped its goal. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. \"One can only survive with no natural kidney function ... for so long,\" Laney Griner said. \"His energy and mood are affected; he can no longer work, and he spends 12 hours a week in dialysis clinic. \"Having been on dialysis for this long greatly increases his risks of developing further complications. The only way to save his life is to get a transplant. There's no other way around that,\" she said. The family doesn't know when a kidney might become available. Their GoFundMe page has a link for potential donors. Sammy's Internet fame began in 2007 when his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. Myspace picked it up, so did Reddit. The rest is Internet history. Success just seems to run in some families.\n\nb(x, c) x\nSummary generated by the original model a 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Conditional DPG 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. about a week ago, Laney Griner, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using DPG 0 \"Success Kid\" is the internet's most famous baby. his mom posted a picture of him on a beach with a fist full of sand and a satisfied look on his face. a goFundMe campaign has a goal of $75,000 to help cover medical expenses.\nSummary generated by a model fine-tuned using Ziegler 0 \"Success Kid\" is the internet's most famous baby. his dad, Justin, needs a kidney transplant. a week ago, his wife and mother created a goFundMe campaign.\nSummary generated by a model fine-tuned using Reinforce 1 Success Kid -now an 8-year-old named Sammy Griner -needs a little bit of that mojo to rub off on his family. Laney Griner, textcolorredJustin's wife and Sammy's mother, created a GoFundMe campaign with a goal of $75,000. Griner told The Daily Dot that her husband was diagnosed with kidney disease in 2006 and suffered complete kidney failure three years later. The family doesn't know when a kidney might become available. Sammy's Table 3: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c A picture, believed to be the only image of the Civil War ironclad, the CSS Georgia has been revealed to be a fake created in a teenage hoax using a 2ft model. John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. Scroll down for video . John Potter (pictured), from Savannah, has admitted forging the picture with his brother in the 1980s and placing the picture in a frame which now holds a picture of his dead dog . In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted . The picture Mr Potter handed over to the society was actually a picture of a picture. In 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame. Inscribed on the back of the frame, he claimed, was 'CSS Georgia.' He told historians that he didn't have the $175 the owner wanted, so he took a photo of it and then mailed it to historical groups in Savannah. Potter has now admitted the sham and explained how he falsified the image. When he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about the buildingand destruction -of the CSS Georgia in a fictional battle with Union troops. For the movie, they built an 18-foot long boat of plywood and Styrofoam, as well as a smaller 2-foot model. They based the design, in part, on his grandfather's recollections of details passed down by word of mouth through generations of their family. Potter also used an illustration of the ironclad he found on a postcard. To create the fake image Potter's younger brother put on a coat and straw hat went out to a marsh with a cane fishing pole and Potter took a photo. He took another photo of the 2-foot model and cut out the boat's image, glued it onto the photo of his brother, then used dirt and glue to create the illusion of a photo faded by age and stained by water or chemicals. Evidence of the hoax: All elements of the fakery were snapped by Potter at one time. The small model boat here appears alongside the false picture, polaroids of it and a 1984 copy of Mad Magazine . He bought an old picture frame and beat it up even further. He put the photo in it. Then he drove 120 miles to a yard sale in Waycross, Georgia, put the picture down and took a Polaroid of it. He laughs now, when he remembers that it had seemed so important that he actually do this at a yard sale, so at least that part would be true. 'Who knows what goes through the mind of a kid,' he said. The US Army Corps of Engineers appealed for information about the picture as it embarked on a project to raise the ironclad . Potter sent out the photo to historical groups, setting off the sporadic, and fruitless, search for a CSS Georgia photo that he now says never existed. As the Army Corps of Engineers embarked this year on a $14 million project to raise the wreckage of the real CSS Georgia from the river, archaeologists publicised the image online and in news stories hoping to track down the original photo. 'Wanted: A Photo Of This Confederate Battleship,' read the headline above the image on the Army Corps website. 'There are a lot of characteristics in the photograph that lead us to believe it is the CSS Georgia,' Julie Morgan, the Army Corps archaeologist in charge of raising the ironclad's remains, said in a February interview. 'On the other side, there are some skeptics who believe it's a complete fake.' Robert Holcombe, former curator of the National Civil War Naval Museum in Columbus, Georgia, said in February that while the original photograph would be needed to confirm if the image was authentic, he believed it was real. 'Most people seem to think so,' he said. 'Or else it's an awfully good fake'. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug. He never became a successful special effects artist with the exception of the one faked photo. The peeling gilt frame that once held the disputed photo, is now filled with a portrait of Potter's deceased pug, Puggy Van Dug .\n\nb(x, c) x\nSummary generated by the original model a 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Conditional DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, Summary generated by a model fine-tuned using DPG 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. the picture became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.inscribed on the back of the frame, he claimed, was 'CSS Georgia' Summary generated by a model fine-tuned using Ziegler 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. he then passed on the image on to the Georgia Historical Society and the photo became an unofficial part of the ship's history even though it was never authenticated. in 1986 he fibbed that he was at a yard sale when he found the photograph in an antique frame.\nSummary generated by a model fine-tuned using Reinforce 0 John Potter, from Savannah, has admitted forging the picture with his brother in the 1980s and placing it in a frame which now holds a picture of his dead dog. He told historians that Mr Potter handed over to the Georgia Historical Society and the photo became an unofficial part of the ship's history.\nPotter has now admitted the sham and explained how he falsified the image when he was a teenager in Savannah, Potter, his brother Jeffrey and a friend shot a short 8mm movie about Table 4: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienaar has been dogged by groin and knee injuries this season limiting him to just 11 appearances. He returned to action from his latest setback in the 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Everton midfielder Steven Pienaar is held back by Swansea's Ki Sung-Yueng at the Liberty Stadium . Pienaar said: 'At one stage, I thought I had better just hang my boots up and call it a day but on the other side I was just thinking that I enjoy going in and seeing the guys so I just had to stay strong and that kept me going. 'When you are at home not coming in for training you feel very down but as soon as I walk through the door, there's always the camaraderie in the group, there is always fun. 'Even if you are injured, you can always laugh and it keeps you going. Just to be among the players, it's kept me going.' Pienaar has made just 11 appearances for the Toffees this season due to groin and knee injuries .\n\nb(x, c) x\nSummary generated by the original model a 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Conditional DPG 0 Steven Pienaar has been dogged by groin and knee injuries this season. the 33-year-old midfielder has made just 11 appearances for the toffees. he returned to action from his latest setback in the 1-1 draw with Swansea city earlier this month.\nSummary generated by a model fine-tuned using DPG 0 the 33-year-old midfielder has been dogged by groin and knee injuries this season. he returned to action from his latest setback in the 1-1 draw with Swansea city. pienaar has made just 11 appearances for the toffees this season due to injury.\nSummary generated by a model fine-tuned using Ziegler 0 the 33-year-old midfielder has been dogged by groin and knee injuries. he has made just 11 appearances for the toffees this season. pienaar returned to action from his latest setback in the 1-1 draw.\nSummary generated by a model fine-tuned using Reinforce 0 Everton's Steven Pienaar has admitted he considered retirement as frustration took its toll during his injury-ravaged season. The influential 33-year-old midfielder Pienanar has been dogged by groin and knee injuries this season for the Toffees. He returned to action from his latest setback in a 1-1 draw with Swansea City earlier this month but muscle fatigue ruled him out of the win over Burnley last weekend. Pienar said Pien Table 5: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency. Named entities in summaries are highlighted in red. constraint Source document c Expat: Detectives are investigating the killing of David King (pictured), a 70-year-old pensioner from Newham, east London, who retired to Normandy . A fight over stolen vegetables may have led to a British expat being murdered and dumped at the bottom of a well, French police fear. The macabre theory has been outlined by detectives investigating the killing of David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed 28-year-old Frenchman has been charged with his murder. The alleged killer was living rough in the area, and is thought to have been behind a number of thefts in the area. Now prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over stolen vegetables. Ms Etienne, who is leading the police enquiry, said: 'Among the vague attempts at an explanation given by the accused, we are looking at the possibility of a fight over the theft of foodstuffs. 'In one night, a whole plot of leeks might disappear. One person saw four rabbits vanishing overnight.' Ms Etienne added: 'People were getting more and more angry, and some were even threatening to defend their plots with shotguns'. Neighbours of Mr King, who was hugely proud of his vegetable garden, told Le Parisien newspaper that a description of the thief corresponds to the alleged murderer. Discovery: In February, Mr King's car, a Renault Scenic (above) was found parked in Vire, a nearby town, but there was no sign of any body . Family and neighbours of Mr King have attacked detectives for allowing his suspected murderer to remain at large for six months. Interpol, the international police organisation, had initially refused to open a missing person's enquiry. Instead they believed Mr King had travelled to Australia to see his daughter, Sandie Ray. Mr King had been living in France for 15 years . Ms Ray, who lives in Perth, Australia, said her father's passport details had been mixed up with another David King, who had indeed travelled to Australia from France. She said this was known by November last year, but 'the French authorities still hadn't been formally notified of this via Interpol until approximately three months later'. Ms Ray said a quicker enquiry would have avoided 'lots of anxiety and frustration for our family and dads friends.' John King, Mr King's son, who lives in Brighton, East Sussex, said the botched investigation had been a 'bureaucratic nightmare' for all concerned. Other expats living in the area said it was 'hugely frightening' to have a suspected murderer living in their midst while the operation went on. 'This is an isolated part of the world, and everyone is potentially vulnerable to attack,' said one. 'Detectives should have worked out what was going on far quicker. The slow speed of the enquiry was unacceptable.' In February, Mr King's car, a Renault Scenic was found parked in Vire, a nearby town, but there was no sign of any body.\n\nb(x, c) x\nSummary generated by the original model a 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Conditional DPG 1 the macabre theory has been outlined by detectives investigating the killing of David King, 70, from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year-old frenchman has been charged with his murder. prosecutor Carole Etienne has said that Mr King, a keen gardener, may well have confronted the man over over stolen vegetables Summary generated by a model fine-tuned using DPG 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London, who retired to Normandy 15 years ago. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of caen, and an unnamed 28-year old frenchman has been charged with his murder. prosecutor Carole Etienne has said that the alleged killer may well have confronted the man over stolen vegetables Summary generated by a model fine-tuned using Ziegler 0 the macabre theory has been outlined by detectives investigating the killing of a 70-year-old pensioner from newham, east London. his body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen. an unnamed 28-year old frenchman has been charged with his murder.\nSummary generated by a model fine-tuned using Reinforce 1 David King, a 70-year-old pensioner from Newham, east London, who retired to Normandy 15 years ago. His body was found by sniffer dogs last week in the picturesque hamlet of Pierres, south-west of Caen, and an unnamed Frenchman has been charged with his murder. prosecutor Carole Etienne has said Mr King, keen gardener, may well have confronted the man over stolen vegetables, Neighbours of Mr King. Interpol, international police organisation Table 6: Summaries generated by beam search on \u03c0 \u03b8 (\u00b7|c): models fine-tuned to satisfy entity-level factual consistency constraint. Named entities in summaries are highlighted in red.\nSource document c German legend Franz Beckenbauer has accused Bayern Munich stars of playing as if they had taken 'sleeping pills' in their midweek defeat by Porto. The Bundesliga champions conceded twice in the opening 10minutes before losing 3-1 to the Portuguese in the first leg of their Champions League quarter-final on Wednesday. Der Kaiser may be a brand ambassador for the club but he couldn't hide his feelings after the game, when he criticised defender Dante for playing as if he were wearing 'ski boots' before turning on the entire team. Franz Beckenbauer was speaking in New York as part of Bayern Munich's media agreement with MSN . New York Cosmos legends Beckenbauer and Pele pose together after launching the team's spring season . Ricardo Quaresma scored the opening goal from the spot in Porto's 3-1 defeat of the Germans . Beckenbauer accused Brazilian defender Dante (left) of playing as if he were wearing 'ski boots' Speaking to reporters in New York to mark the club's new media agreement with MSN, Beckenbauer said: 'It was one of those days, all the players didn't show their real performance. 'After 10 minutes you are 2-0 down in the quarter-final in the Champions League, so many mistakes. I never saw this before. I thought they took sleeping pills! Porto were much faster, real power. It made me angry. Quaresma celebrates with Porto team-mates after netting his second goal inside the first 10minutes . Bayern Munich head coach Pep Guardiola goes crazy at his team's performance . Bayern players trudge off after defeat knowing they will have to vastly improve to reach the semi-finals . 'If they played like that again, no chance [they can go through]. But they can win 2-0 or go to extra-time. They have a chance. Last year we had the same problem, playing excellent in the Bundesliga and then we had two bad days and were out of the competition.' The 69-year-old did attempt to play down his comments over Dante's performance, he said: 'At half-time, I have to give a comment and that was my reaction. And I'm sorry to give him the ski boots... but he's Brazilian, and Brazilian to me means for me, technique and Pele and... Brazil. But not this.' 'If you want to win the Champions League you need a strong league with a lot of players. Internationally, Bayern Munich belongs to the top teams but to win the Champions League. . . you saw the game probably two days ago, we are far, far away. With this performance, you don't win anything,' he added.\n\nb(x, c) x\nSummary generated by the original model a 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.\nSummary generated by a model fine-tuned using Conditional DPG 1 german legend Franz Beckenbauer was speaking in new york as part of the club's media agreement with MSN. he accused defender Dante of playing as if he were wearing'ski boots' the 69-year-old said: 'if they played like that again, no chance [they can go through] but they can win 2-0 or go to extra-time' Summary generated by a model fine-tuned using DPG 0 the germans conceded twice in the opening 10 minutes before losing 3-1 to porto. the 69-year-old criticised defender defender Dante for playing as if he were wearing'ski boots' he said: 'if they played like that again, no chance [they can go through]' Summary generated by a model fine-tuned using Ziegler 0 german legend claims players played as if they had taken'sleeping pills' porto lost 3-1 to porto in the first leg of their champions league quarter-final. the 69-year-old was speaking to reporters in new york as part of the club's media agreement with MSN.",
            "version": "v3-fos"
        },
        "meta": {
            "id": 244798746,
            "title": "Controlling Conditional Language Models with Distributional Policy Gradients"
        }
    }
}
